{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The goal of this project is to predict wildfire ignition risk in California from using a neural network trained on historical weather and wildfire data. Two approaches were attempted and scaled. The most effective approach was a parallel long short term memory network (pLSTM). The neural network portion of this project had six major phases. See below for more detail on each part.\n",
    "\n",
    "1. [Feature addition and smoothing](https://github.com/gperdrizet/wildfire/blob/master/notebooks/add_features.ipynb) - Added min, mean and max features for each weather variable and smooth the whole dataset with a daily average.\n",
    "2. [Stratified sampling](https://github.com/gperdrizet/wildfire/blob/master/notebooks/recursive_sampling.ipynb) - Dataset was split into fully stratified blocks of ~500,000 observations.\n",
    "3. [Small scale MLP](https://github.com/gperdrizet/wildfire/blob/master/notebooks/keras_MLP_skopt.ipynb) - The stratified samples were used to optimize and test a 'deep' neural network architecture using a binary 'fire/no fire' paradigm.\n",
    "4. [Scaled MLP]() - Insights gained from the small scale MLP were applied to the entire dataset.\n",
    "5. [Single LSTM](https://github.com/gperdrizet/wildfire/blob/master/notebooks/keras_LSTM_skopt_one_spatial_bin.ipynb) - A simple long short term memory based network architecture was optimized and tested on the full 22 years of data in just one geospatial bin.\n",
    "6. [Parallel LSTM](https://github.com/gperdrizet/wildfire/blob/master/notebooks/keras_parallel_LSTM_410input.ipynb) A parallel LSTM network consisting of 410 inputs, on for each geospatial bin, was trained on all 22 years of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature addition and smoothing\n",
    "The motivation to add min, max and mean features and then smooth by daily averaging was to 1) reduce the size of the data set and 2) make the resolution of the weather data more closely match that of the fire data (i.e. day of fire discovery)\n",
    "\n",
    "The first step was to add the min, max and mean features using a sliding window of 24 hr. The calculated values were added at the right edge of the window. This was done to reflect the temporal nature of the data - the conditions which led to a fire are logically likely to have occurred on and/or before the day that the fire was discovered. The results of the feature at addition for a few representative weather variable are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example of three weather variables showing the effect of adding min, max and mean](https://github.com/gperdrizet/wildfire/blob/master/figures/min_max_mean_added.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step in the data preparation was to further smooth the data by taking the daily mean for each weather variable. This was done to match the resolution of the weather data to the fire data. The weather data has a periodicity of 3 hours, while most of the fire data is no more precise that 'discovery day'. The results of the smoothing for a few representative weather variable are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example of three weather variables showing the effect of smoothing via daily average](https://github.com/gperdrizet/wildfire/blob/master/figures/smoothed_data.png?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
