{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "As a first pass at developing a machine learning model to predict California wildfires I will evaluate and tune several different gradient boosting algorithms. The procedure will be as follows:\n",
    "\n",
    "1. Determine the best gradient boosting for the data\n",
    "2. Determine the best scoring function/metric for optimization\n",
    "3. Tune model hyperparameters\n",
    "4. Investigate feature importance and possibly trim/apply dimensionality reduction techniques\n",
    "\n",
    "There are two anticipated issues which will need to be dealt with first:\n",
    "\n",
    "1. Large dataset size - current working dataset has 7.3 million observations of 25 variables and this is likely to grow as the project progresses\n",
    "2. Highly imbalanced data (~20 times more observations without fire than with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports and notebook setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from time import time\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from IPython.display import display_markdown\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/training_data/1992-1997_training_data.csv'\n",
    "rand_seed = 123\n",
    "data_sample_size = 10000\n",
    "test_train_split_ratio = 0.3\n",
    "xgb_jobs = 5\n",
    "optimization_jobs = 3\n",
    "search_iterations = 100\n",
    "search_scoring_func = make_scorer(average_precision_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_random_sample(data, k):\n",
    "    n = len(data)\n",
    "    \n",
    "    indices = random.sample(range(0, n), k)\n",
    "    \n",
    "    return data.iloc[indices]\n",
    "\n",
    "def print_model_score(model, x_train, y_train, x_test, y_test):\n",
    "    training_score = average_precision_score(model.predict(x_train), y_train)\n",
    "    test_score = average_precision_score(model.predict(x_test), y_test)\n",
    "    \n",
    "    #print('Average precision-recall score, training set: {}'.format(np.round(training_score, 2)))\n",
    "    #print('<b>Average precision-recall score, test set: {}</b>'.format(np.round(test_score,2)))\n",
    "    \n",
    "    display_markdown('**Average precision-recall score, training set: {}**'.format(np.round(training_score, 2)), raw=True)\n",
    "    display_markdown('**Average precision-recall score, test set: {}**'.format(np.round(test_score, 2)), raw=True)\n",
    "\n",
    "    \n",
    "def display_confusion_matrix(model, class_names, x_test, y_test):\n",
    "\n",
    "    raw_cm = confusion_matrix(y_test, model.predict(x_test))\n",
    "    print(\"Raw count confusion matrix\")\n",
    "    print(raw_cm)\n",
    "    \n",
    "    normalized_cm = plot_confusion_matrix(model, x_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true')\n",
    "\n",
    "    normalized_cm.ax_.set_title(\"Normalized confusion matrix\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def stratified_train_test_split(data, targets, rand_seed):\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=test_train_split_ratio, \n",
    "        random_state=rand_seed\n",
    "    )\n",
    "\n",
    "    for train_index, test_index in sss.split(data, targets):\n",
    "        x_train, x_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = targets.iloc[train_index], targets.iloc[test_index]\n",
    "        \n",
    "        return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "\n",
    "def tune_hyperparameters(\n",
    "    known_params,\n",
    "    param_dist, \n",
    "    x_train, \n",
    "    y_train, \n",
    "    num_jobs, \n",
    "    search_iterations, \n",
    "    search_scoring_func\n",
    "):\n",
    "\n",
    "    # initalize XGBoost classifier\n",
    "    xgb_mod = XGBClassifier(**known_params)\n",
    "\n",
    "    # set up random search\n",
    "    xgb_random_search = RandomizedSearchCV(\n",
    "        xgb_mod, \n",
    "        param_distributions=param_dist,\n",
    "        scoring=search_scoring_func,\n",
    "        n_iter=search_iterations,\n",
    "        n_jobs=num_jobs\n",
    "    )\n",
    "\n",
    "    # run and time search\n",
    "    start = time()\n",
    "    xgb_best_model = xgb_random_search.fit(x_train, y_train)\n",
    "    print(\"RandomizedSearchCV took %.f min. for %d candidate\"\n",
    "          \" parameter settings.\" % (((time() - start)/60), search_iterations))\n",
    "    \n",
    "    return xgb_best_model, xgb_random_search\n",
    "\n",
    "def regularize_grid(x, y, z, resolution):\n",
    "\n",
    "    # target grid to interpolate to\n",
    "    xi = np.arange(min(x), max(x), ((max(x) - min(x)) / resolution))\n",
    "    yi = np.arange(min(y), max(y), ((max(y) - min(y)) / resolution))\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "    # interpolate\n",
    "    zi = griddata((x, y), z, (xi, yi), method='linear')\n",
    "    \n",
    "    return xi, yi, zi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total observations: 7.3E+06\n",
      "Ignitions count: 3.7E+05\n",
      "Non ignitions count: 6.9E+06\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv(data_file, low_memory=False)\n",
    "\n",
    "# set ignition to 0 for noxels with no fire\n",
    "data['ignition'].fillna(0, inplace=True)\n",
    "\n",
    "# count number of observations in each class\n",
    "ignition_count = len(data[data[\"ignition\"] == 1])\n",
    "no_ignition_count = len(data) - ignition_count\n",
    "print('Total observations: {:.1E}'.format(len(data)))\n",
    "print('Ignitions count: {:.1E}'.format(ignition_count))\n",
    "print('Non ignitions count: {:.1E}'.format(no_ignition_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so before we dig into this much farther, I am noticing an issue. For some fires we know the discovery time and some we do not. I can imagine a few ways of dealing with this:\n",
    "\n",
    "1. Throw out fires for which we do not know the discovery time\n",
    "2. Average everything at the day resolution level\n",
    "\n",
    "I am leaning toward option two because:\n",
    "\n",
    "1. Discovery time is not ignition time so the weather state during that exact hour may not be pertinent anyway - for example: what if the fire had already been burning for hours and the temperature had changed drastically?\n",
    "2. The data is already skewed toward no ignition observations. I would rather not have to throw out even more fires. Also, doing so will mean that I have noxels where there was a fire which was not included in the dataset. \n",
    "\n",
    "Specific plan is to roll a daily moving average across the data, including ignition value. This will assign an ignition value to all noxels within 24 hr. of a fires discovery day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll data with 24 hr windowed mean\n",
    "data_moving_avg = data.groupby(['lat', 'lon']).rolling(24, on=\"weather_bin_time\").mean()\n",
    "data_moving_avg.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# clean up dataframe\n",
    "data_moving_avg['weather_bin_time'] = pd.to_datetime(data_moving_avg['weather_bin_time'])\n",
    "data_moving_avg['weather_bin_month'] = data_moving_avg['weather_bin_time'].dt.month\n",
    "data_moving_avg['weather_bin_year'] = data_moving_avg['weather_bin_time'].dt.year\n",
    "data_moving_avg.drop(['fire_discovery_time', 'size', 'size_class', 'weather_bin_time'], axis=1, inplace=True)\n",
    "data_moving_avg.dropna(inplace=True)\n",
    "\n",
    "# split positive and negative datsets up\n",
    "ignitions = data_moving_avg[data_moving_avg['ignition'] > 0]\n",
    "no_ignitions = data_moving_avg[data_moving_avg['ignition'] == 0]\n",
    "\n",
    "# due to the moving average we will have some positive observations with\n",
    "# fractional ignition values\n",
    "ignitions = ignitions.assign(ignition=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we need to pseudorandomly sample the positive and negative datasets. This will accomplish two goals:\n",
    "    \n",
    "1. Make the dataset size smaller\n",
    "2. Equalize the number of positive and negative observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total observations: 2.0E+04\n",
      "Ignitions count: 1.0E+04\n",
      "Non ignitions count: 1.0E+04\n"
     ]
    }
   ],
   "source": [
    "no_ignitions_sample = k_random_sample(no_ignitions, data_sample_size)\n",
    "ignitions_sample = k_random_sample(ignitions, data_sample_size)\n",
    "\n",
    "sampled_data = no_ignitions_sample.append(ignitions_sample)\n",
    "class_names = np.array(['No ignition', 'Ignition'])\n",
    "\n",
    "print('Total observations: {:.1E}'.format(len(sampled_data)))\n",
    "print('Ignitions count: {:.1E}'.format(len(ignitions_sample)))\n",
    "print('Non ignitions count: {:.1E}'.format(len(no_ignitions_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = sampled_data['ignition']\n",
    "data = sampled_data.drop(['ignition'], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.values, targets.values, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Game on!** Let's keep track of our results as we begin to play with the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default model: XGBoost vs CATBoost vs random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model description</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Training score</th>\n",
       "      <th>Test score</th>\n",
       "      <th>False positive rate</th>\n",
       "      <th>False negative rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>XGBoost with defaults</td>\n",
       "      <td>XGBClassifier()</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CatBoost with defaults</td>\n",
       "      <td>CatBoostClassifier()</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Random forest with defaults</td>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model description                Classifier  Training score  \\\n",
       "0        XGBoost with defaults           XGBClassifier()            0.76   \n",
       "1       CatBoost with defaults      CatBoostClassifier()            0.87   \n",
       "2  Random forest with defaults  RandomForestClassifier()            1.00   \n",
       "\n",
       "   Test score  False positive rate  False negative rate  \n",
       "0        0.75                 0.25                 0.21  \n",
       "1        0.78                 0.21                 0.18  \n",
       "2        0.76                 0.23                 0.20  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_scores_columns = ['Model description', 'Classifier', 'Training score', 'Test score', 'False positive rate', 'False negative rate']\n",
    "model_scores = pd.DataFrame(columns=model_scores_columns)\n",
    "\n",
    "# XGBoost\n",
    "xgb_mod = XGBClassifier(n_jobs = (xgb_jobs * optimization_jobs)) \n",
    "xgb_mod.fit(x_train, y_train)\n",
    "\n",
    "training_score = average_precision_score(xgb_mod.predict(x_train), y_train)\n",
    "test_score = average_precision_score(xgb_mod.predict(x_test), y_test)\n",
    "cm = confusion_matrix(y_test, xgb_mod.predict(x_test))\n",
    "\n",
    "TN = cm[0][0]\n",
    "FN = cm[1][0]\n",
    "FP = cm[0][1]\n",
    "\n",
    "false_neg_rate = FN / (FN + TN)\n",
    "false_pos_rate = FP / (FP + TN)\n",
    "\n",
    "model_scores = model_scores.append(pd.Series(['XGBoost with defaults', 'XGBClassifier()', np.round(training_score,2), np.round(test_score,2), np.round(false_pos_rate,2), np.round(false_neg_rate,2)], index=model_scores.columns), ignore_index=True)\n",
    "\n",
    "# CatBoost\n",
    "catboost_mod = CatBoostClassifier(thread_count = (xgb_jobs * optimization_jobs))\n",
    "catboost_mod.fit(x_train, y_train, silent=True)\n",
    "\n",
    "training_score = average_precision_score(catboost_mod.predict(x_train), y_train)\n",
    "test_score = average_precision_score(catboost_mod.predict(x_test), y_test)\n",
    "cm = confusion_matrix(y_test, catboost_mod.predict(x_test))\n",
    "\n",
    "TN = cm[0][0]\n",
    "FN = cm[1][0]\n",
    "FP = cm[0][1]\n",
    "\n",
    "false_neg_rate = FN / (FN + TN)\n",
    "false_pos_rate = FP / (FP + TN)\n",
    "\n",
    "model_scores = model_scores.append(pd.Series(['CatBoost with defaults', 'CatBoostClassifier()', np.round(training_score,2), np.round(test_score,2), np.round(false_pos_rate,2), np.round(false_neg_rate,2)], index=model_scores.columns), ignore_index=True)\n",
    "\n",
    "# Random forest\n",
    "rand_forest_mod = RandomForestClassifier(n_jobs = (xgb_jobs * optimization_jobs)) \n",
    "rand_forest_mod.fit(x_train, y_train)\n",
    "\n",
    "training_score = average_precision_score(rand_forest_mod.predict(x_train), y_train)\n",
    "test_score = average_precision_score(rand_forest_mod.predict(x_test), y_test)\n",
    "cm = confusion_matrix(y_test, rand_forest_mod.predict(x_test))\n",
    "\n",
    "TN = cm[0][0]\n",
    "FN = cm[1][0]\n",
    "FP = cm[0][1]\n",
    "\n",
    "false_neg_rate = FN / (FN + TN)\n",
    "false_pos_rate = FP / (FP + TN)\n",
    "\n",
    "model_scores = model_scores.append(pd.Series(['Random forest with defaults', 'RandomForestClassifier()', np.round(training_score,2), np.round(test_score,2), np.round(false_pos_rate,2), np.round(false_neg_rate,2)], index=model_scores.columns), ignore_index=True)\n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from false negative and false positive rates CatBoost is our winner. Because of the nature of the data, it is extremely important to minimize the false negative rate. False positives might be economically wasteful, but false negatives have the potential to cause injury and loss of life. Provisionally, let's continue on with CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring function selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "tools/enum_parser/enum_serialization_runtime/enum_runtime.cpp:32: Key 'Accuracy' not found in enum EScoreFunction. Valid options are: 'SolarL2', 'Cosine', 'NewtonL2', 'NewtonCosine', 'LOOL2', 'SatL2', 'L2'. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-8cd95bd6465c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mscore_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscoring_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcatboost_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatboost_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   3791\u001b[0m         self._fit(X, y, cat_features, text_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[1;32m   3792\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3793\u001b[0;31m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[1;32m   3794\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1676\u001b[0m             \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m             \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1678\u001b[0;31m             \u001b[0msave_snapshot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1679\u001b[0m         )\n\u001b[1;32m   1680\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_prepare_train_params\u001b[0;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0m_check_param_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_params_type_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m         \u001b[0m_check_train_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m         \u001b[0meval_set_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._check_train_params\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._check_train_params\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: tools/enum_parser/enum_serialization_runtime/enum_runtime.cpp:32: Key 'Accuracy' not found in enum EScoreFunction. Valid options are: 'SolarL2', 'Cosine', 'NewtonL2', 'NewtonCosine', 'LOOL2', 'SatL2', 'L2'. "
     ]
    }
   ],
   "source": [
    "model_scores_columns = ['Scoring function','False positive rate', 'False negative rate']\n",
    "model_scores = pd.DataFrame(columns=model_scores_columns)\n",
    "\n",
    "scoring_functions = [\n",
    "    'Accuracy',\n",
    "    'AUC',\n",
    "    'Logloss',\n",
    "    'F1',\n",
    "    'Precision',\n",
    "    'Recall'\n",
    "]\n",
    "\n",
    "i = 0\n",
    "\n",
    "for scoring_function in scoring_functions:\n",
    "    catboost_mod = CatBoostClassifier(\n",
    "        thread_count = (xgb_jobs * optimization_jobs),\n",
    "        score_function = scoring_function\n",
    "    )\n",
    "    catboost_mod.fit(x_train, y_train, silent=True)\n",
    "\n",
    "    cm = confusion_matrix(y_test, catboost_mod.predict(x_test))\n",
    "\n",
    "    TN = cm[0][0]\n",
    "    FN = cm[1][0]\n",
    "    FP = cm[0][1]\n",
    "\n",
    "    false_neg_rate = FN / (FN + TN)\n",
    "    false_pos_rate = FP / (FP + TN)\n",
    "\n",
    "    model_scores = model_scores.append(pd.Series([scoring_function, np.round(false_pos_rate,2), np.round(false_neg_rate,2)], index=model_scores.columns), ignore_index=True)\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "# model_scores_columns = ['Model description', 'Classifier', 'Training score', 'Test score', 'False positive rate', 'False negative rate']\n",
    "# model_scores = pd.DataFrame(columns=model_scores_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scoring function</th>\n",
       "      <th>False positive rate</th>\n",
       "      <th>False negative rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logloss</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scoring function  False positive rate  False negative rate\n",
       "0         Accuracy                 0.21                 0.18\n",
       "1              AUC                 0.21                 0.18\n",
       "2          Logloss                 0.21                 0.18\n",
       "3               F1                 0.21                 0.18\n",
       "4        Precision                 0.21                 0.18\n",
       "5           Recall                 0.21                 0.18"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
