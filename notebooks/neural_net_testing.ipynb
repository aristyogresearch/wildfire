{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Turn off TensorFlow warning messages in program output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = '../data/stratified_training_data/1992-2015_training_data_raw_n100000_ks_pval0.3.1.csv'\n",
    "test_data_file = '../data/stratified_training_data/1992-2015_training_data_raw_n100000_ks_pval0.3.2.csv'\n",
    "\n",
    "# Datatypes for dataframe loading\n",
    "dtypes = {\n",
    "    'weather_bin_time': object,\n",
    "    'lat': float,\n",
    "    'lon': float,\n",
    "    'air.sfc': float,\n",
    "    'rhum.2m': float,\n",
    "    'dpt.2m': float,\n",
    "    'pres.sfc': float,\n",
    "    'uwnd.10m': float,\n",
    "    'vwnd.10m': float,\n",
    "    'veg': float,\n",
    "    'dlwrf': float,\n",
    "    'dswrf': float,\n",
    "    'lcdc': float,\n",
    "    'hcdc': float,\n",
    "    'mcdc': float,\n",
    "    'hpbl': float,\n",
    "    'ulwrf.sfc': float,\n",
    "    'ignition': float,\n",
    "    'weather_bin_day': int,\n",
    "    'weather_bin_month': int,\n",
    "    'weather_bin_year': int\n",
    "}\n",
    "\n",
    "# Features to use during training \n",
    "features = [\n",
    "    'weather_bin_month',\n",
    "    'weather_bin_year',\n",
    "    'air.sfc',\n",
    "    'rhum.2m',\n",
    "    'dpt.2m',\n",
    "    'pres.sfc',\n",
    "    'uwnd.10m', \n",
    "    'vwnd.10m',\n",
    "    'veg',\n",
    "    'lat',\n",
    "    'lon',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(training_data_file, dtype=dtypes)\n",
    "testing_data = pd.read_csv(test_data_file, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training observations: 5.672E+04\n",
      " Ignitions count: 2.607E+03\n",
      " Non ignitions count: 5.411E+04\n",
      "\n",
      "Total testing observations: 5.672E+04\n",
      " Ignitions count: 2.641E+03\n",
      " Non ignitions count: 5.408E+04\n"
     ]
    }
   ],
   "source": [
    "# count number of observations in each class\n",
    "no_ignition_count = len(training_data[training_data[\"ignition\"] == 0])\n",
    "ignition_count = len(training_data) - no_ignition_count\n",
    "print('Total training observations: {:.3E}'.format(len(training_data)))\n",
    "print(' Ignitions count: {:.3E}'.format(ignition_count))\n",
    "print(' Non ignitions count: {:.3E}'.format(no_ignition_count))\n",
    "\n",
    "class_weight = no_ignition_count / ignition_count\n",
    "\n",
    "print()\n",
    "\n",
    "no_ignition_count = len(testing_data[testing_data[\"ignition\"] == 0])\n",
    "ignition_count = len(testing_data) - no_ignition_count\n",
    "print('Total testing observations: {:.3E}'.format(len(testing_data)))\n",
    "print(' Ignitions count: {:.3E}'.format(ignition_count))\n",
    "print(' Non ignitions count: {:.3E}'.format(no_ignition_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data[features].values\n",
    "\n",
    "training_ignition = pd.DataFrame()\n",
    "training_ignition[['no_ignition']] = abs(training_data[['ignition']] - 1)\n",
    "training_ignition[['ignition']] = training_data[['ignition']]\n",
    "Y_training = np.array(training_ignition)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = testing_data[features].values\n",
    "\n",
    "testing_ignitions = pd.DataFrame()\n",
    "testing_ignitions[['no_ignition']] = abs(testing_data[['ignition']] - 1)\n",
    "testing_ignitions[['ignition']] = testing_data[['ignition']]\n",
    "Y_testing = testing_ignitions.values\n",
    "\n",
    "pd.options.mode.chained_assignment = 'warn'  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "#Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "#Y_scaled_testing = Y_scaler.transform(Y_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core tensorflow 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many inputs and outputs are in our neural network\n",
    "number_of_inputs = X_scaled_training.shape[1]\n",
    "number_of_outputs = 2\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 10\n",
    "layer_2_nodes = 50\n",
    "layer_3_nodes = 50\n",
    "layer_4_nodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Input Layer\n",
    "with tf.compat.v1.variable_scope('input'):\n",
    "    X = tf.compat.v1.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
    "\n",
    "    \n",
    "# Layer 1\n",
    "with tf.compat.v1.variable_scope('layer_1'):\n",
    "    weights = tf.compat.v1.get_variable(\n",
    "        name='weights1', \n",
    "        shape=[number_of_inputs, layer_1_nodes], \n",
    "        initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\")\n",
    "    )\n",
    "    biases = tf.compat.v1.get_variable(\n",
    "        name='biases1', \n",
    "        shape=[layer_1_nodes], \n",
    "        initializer=tf.compat.v1.zeros_initializer()\n",
    "    )\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "\n",
    "    \n",
    "# Layer 2\n",
    "with tf.compat.v1.variable_scope('layer_2'):\n",
    "    weights = tf.compat.v1.get_variable(\n",
    "        name='weights2', \n",
    "        shape=[layer_1_nodes, layer_2_nodes], \n",
    "        initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\")\n",
    "    )\n",
    "    biases = tf.compat.v1.get_variable(\n",
    "        name='biases2', \n",
    "        shape=[layer_2_nodes], \n",
    "        initializer=tf.compat.v1.zeros_initializer()\n",
    "    )\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "\n",
    "    \n",
    "# Layer 3\n",
    "with tf.compat.v1.variable_scope('layer_3'):\n",
    "    weights = tf.compat.v1.get_variable(\n",
    "        name='weights3', \n",
    "        shape=[layer_2_nodes, layer_3_nodes], \n",
    "        initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\")\n",
    "    )\n",
    "    biases = tf.compat.v1.get_variable(\n",
    "        name='biases3', \n",
    "        shape=[layer_3_nodes], \n",
    "        initializer=tf.compat.v1.zeros_initializer()\n",
    "    )\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "    \n",
    "\n",
    "# Layer 4\n",
    "with tf.compat.v1.variable_scope('layer_4'):\n",
    "    weights = tf.compat.v1.get_variable(\n",
    "        name='weights4', \n",
    "        shape=[layer_3_nodes, layer_4_nodes], \n",
    "        initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\")\n",
    "    )\n",
    "    biases = tf.compat.v1.get_variable(\n",
    "        name='biases4', \n",
    "        shape=[layer_4_nodes], \n",
    "        initializer=tf.compat.v1.zeros_initializer()\n",
    "    )\n",
    "    layer_4_output = tf.nn.relu(tf.matmul(layer_3_output, weights) + biases)\n",
    "    \n",
    "    \n",
    "# Output Layer\n",
    "with tf.compat.v1.variable_scope('output'):\n",
    "    weights = tf.compat.v1.get_variable(\n",
    "        name='weights5', \n",
    "        shape=[layer_4_nodes, number_of_outputs], \n",
    "        initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\")\n",
    "    )\n",
    "    biases = tf.compat.v1.get_variable(\n",
    "        name='biases5', \n",
    "        shape=[number_of_outputs], \n",
    "        initializer=tf.compat.v1.zeros_initializer()\n",
    "    )\n",
    "    prediction = tf.nn.softmax(tf.matmul(layer_4_output, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "with tf.compat.v1.variable_scope('cost'):\n",
    "    Y = tf.compat.v1.placeholder(tf.float32) # removed shape argument\n",
    "    #cost = tf.reduce_mean(input_tensor=tf.math.squared_difference(prediction, Y))\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(\n",
    "        labels=Y,\n",
    "        logits=prediction,\n",
    "        pos_weight=class_weight,\n",
    "        name=None\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.compat.v1.variable_scope('train'):\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.9236 4.9242\n",
      "50 3.8016 3.8041\n",
      "100 3.7943 3.7968\n",
      "150 3.7935 3.796\n",
      "200 3.7932 3.7957\n",
      "250 3.7931 3.7957\n",
      "300 3.7931 3.7956\n",
      "350 3.7931 3.7956\n",
      "400 3.7931 3.7956\n",
      "450 3.7931 3.7956\n",
      "Training is complete!\n",
      "Final training/testing cost: 3.793100118637085/3.795599937438965\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "training_epochs = 500\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations\n",
    "with tf.compat.v1.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_training})\n",
    "\n",
    "        # Every 50 training steps, log our progress\n",
    "        if epoch % 50 == 0:\n",
    "            training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_training})\n",
    "            testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_testing})\n",
    "            print(epoch, np.round(training_cost,4), np.round(testing_cost,4))\n",
    "\n",
    "    # Training is now complete!\n",
    "    print(\"Training is complete!\")\n",
    "\n",
    "    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_testing})\n",
    "    print(\"Final training/testing cost: {}/{}\".format(np.round(training_cost, 4), np.round(testing_cost, 4)))\n",
    "    \n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted = session.run(prediction, feed_dict={X: X_scaled_training})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.argmax(Y_training, axis=1)\n",
    "pred = np.argmax(Y_predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true, pred)\n",
    "\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
