{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_objective_2D #, plot_histogram\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall', 'fp', 'fn']\n",
    "    \n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(3,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        \n",
    "    if metric == 'loss':\n",
    "        plt.ylim([0, plt.ylim()[1]])\n",
    "        \n",
    "    elif metric == 'auc':\n",
    "        plt.ylim([0.8,1])\n",
    "        \n",
    "    else:\n",
    "        #plt.ylim([0,1])\n",
    "        plt.legend()\n",
    "\n",
    "def multivariate_data(\n",
    "    dataset,\n",
    "    target, \n",
    "    start_index, \n",
    "    end_index, \n",
    "    history_size,\n",
    "    target_size, \n",
    "    step\n",
    "):\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    \n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        labels.append(target[i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def log_dir_name(\n",
    "    learning_rate,\n",
    "    past_history,\n",
    "    lstm_units,\n",
    "    hidden_layers,\n",
    "    hidden_units,\n",
    "    #lstm_l2_lambda,\n",
    "    hidden_l2_lambda,\n",
    "    class_0_weight,\n",
    "    class_1_weight\n",
    "):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./LSTM_logs/past_history_{1}_hidden_layers_{3}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(\n",
    "        learning_rate,\n",
    "        past_history,\n",
    "        lstm_units,\n",
    "        hidden_layers,\n",
    "        hidden_units,\n",
    "        #lstm_l2_lambda,\n",
    "        hidden_l2_lambda,\n",
    "        class_0_weight,\n",
    "        class_1_weight\n",
    "    )\n",
    "\n",
    "    return log_dir\n",
    "\n",
    "def f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2 * ((precision*10) * recall) / ((precision*10) + recall + K.epsilon())\n",
    "    \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/training_data/1992-2015_training_data_added_features.csv'\n",
    "\n",
    "# Datatypes for dataframe loading\n",
    "dtypes = {\n",
    "    'lat': float,\n",
    "    'lon': float,\n",
    "    'weather_bin_year': int,\n",
    "    'weather_bin_month': int,\n",
    "    'weather_bin_day': int,\n",
    "    'air.2m': float,\n",
    "    'apcp': float,\n",
    "    'rhum.2m': float,\n",
    "    'dpt.2m': float,\n",
    "    'pres.sfc': float,\n",
    "    'uwnd.10m': float,\n",
    "    'vwnd.10m': float,\n",
    "    'veg': float,\n",
    "    'vis': float,\n",
    "    'ignition': float,\n",
    "    'mean.air.2m': float,\n",
    "    'mean.apcp': float,\n",
    "    'mean.rhum.2m': float,\n",
    "    'mean.dpt.2m': float,\n",
    "    'mean.pres.sfc': float,\n",
    "    'mean.uwnd.10m': float,\n",
    "    'mean.vwnd.10m': float,\n",
    "    'mean.veg': float,\n",
    "    'mean.vis': float,\n",
    "    'max.air.2m': float,\n",
    "    'max.apcp': float,\n",
    "    'max.rhum.2m': float,\n",
    "    'max.dpt.2m': float,\n",
    "    'max.pres.sfc': float,\n",
    "    'max.uwnd.10m': float,\n",
    "    'max.vwnd.10m': float,\n",
    "    'max.veg': float,\n",
    "    'max.vis': float,\n",
    "    'min.air.2m': float,\n",
    "    'min.apcp': float,\n",
    "    'min.rhum.2m': float,\n",
    "    'min.dpt.2m': float,\n",
    "    'min.pres.sfc': float,\n",
    "    'min.uwnd.10m': float,\n",
    "    'min.vwnd.10m': float,\n",
    "    'min.veg': float,\n",
    "    'min.vis': float,\n",
    "    'total_fires': float\n",
    "\n",
    "}\n",
    "\n",
    "# Features to use during training \n",
    "features = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'weather_bin_month',\n",
    "    'veg',\n",
    "    'ignition',\n",
    "    'mean.air.2m',\n",
    "    'mean.apcp',\n",
    "    'mean.rhum.2m',\n",
    "    'mean.dpt.2m',\n",
    "    'mean.pres.sfc',\n",
    "    'mean.uwnd.10m',\n",
    "    'mean.vwnd.10m',\n",
    "    'mean.veg',\n",
    "    'mean.vis',\n",
    "    'mean.air.2m',\n",
    "    'mean.apcp',\n",
    "    'mean.rhum.2m',\n",
    "    'mean.dpt.2m',\n",
    "    'mean.pres.sfc',\n",
    "    'mean.uwnd.10m',\n",
    "    'mean.vwnd.10m',\n",
    "    'mean.vis',\n",
    "    'max.air.2m',\n",
    "    'max.apcp',\n",
    "    'max.rhum.2m',\n",
    "    'max.dpt.2m',\n",
    "    'max.pres.sfc',\n",
    "    'max.uwnd.10m',\n",
    "    'max.vwnd.10m',\n",
    "    'max.vis',\n",
    "    'min.air.2m',\n",
    "    'min.apcp',\n",
    "    'min.rhum.2m',\n",
    "    'min.dpt.2m',\n",
    "    'min.pres.sfc',\n",
    "    'min.uwnd.10m',\n",
    "    'min.vwnd.10m',\n",
    "    'min.vis',\n",
    "    'total_fires'\n",
    "]\n",
    "\n",
    "features_to_scale = [\n",
    "    'veg',\n",
    "    'mean.air.2m',\n",
    "    'mean.apcp',\n",
    "    'mean.rhum.2m',\n",
    "    'mean.dpt.2m',\n",
    "    'mean.pres.sfc',\n",
    "    'mean.uwnd.10m',\n",
    "    'mean.vwnd.10m',\n",
    "    'mean.vis',\n",
    "    'max.air.2m',\n",
    "    'max.apcp',\n",
    "    'max.rhum.2m',\n",
    "    'max.dpt.2m',\n",
    "    'max.pres.sfc',\n",
    "    'max.uwnd.10m',\n",
    "    'max.vwnd.10m',\n",
    "    'max.vis',\n",
    "    'min.air.2m',\n",
    "    'min.apcp',\n",
    "    'min.rhum.2m',\n",
    "    'min.dpt.2m',\n",
    "    'min.pres.sfc',\n",
    "    'min.uwnd.10m',\n",
    "    'min.vwnd.10m',\n",
    "    'min.vis',\n",
    "    'total_fires'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(data_file, index_col=0, parse_dates=True, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out columns of intrest\n",
    "data = raw_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one spatial bin with fires\n",
    "data = data[(data['lat'] == 39.42233) & (data['lon'] == -120.6546)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also drop lat, lon, day and year columns (unnecessary)\n",
    "data.drop(['lat', 'lon'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode month\n",
    "column_names = [\n",
    "    'January',\n",
    "    'February',\n",
    "    'March',\n",
    "    'April',\n",
    "    'May',\n",
    "    'June',\n",
    "    'July',\n",
    "    'August',\n",
    "    'Septermber',\n",
    "    'October',\n",
    "    'November',\n",
    "    'December'\n",
    "]\n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Training data\n",
    "month = np.array(data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "\n",
    "onehot_month_df['datetime'] = pd.to_datetime(data.index)\n",
    "onehot_month_df = onehot_month_df.set_index('datetime')\n",
    "data = pd.concat([data, onehot_month_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[features_to_scale])\n",
    "data[features_to_scale] = scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date time index\n",
    "# one_bin_training_data = one_bin_training_data.sort_index()\n",
    "data = data.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veg</th>\n",
       "      <th>ignition</th>\n",
       "      <th>mean.air.2m</th>\n",
       "      <th>mean.apcp</th>\n",
       "      <th>mean.rhum.2m</th>\n",
       "      <th>mean.dpt.2m</th>\n",
       "      <th>mean.pres.sfc</th>\n",
       "      <th>mean.uwnd.10m</th>\n",
       "      <th>mean.vwnd.10m</th>\n",
       "      <th>mean.veg</th>\n",
       "      <th>...</th>\n",
       "      <th>March</th>\n",
       "      <th>April</th>\n",
       "      <th>May</th>\n",
       "      <th>June</th>\n",
       "      <th>July</th>\n",
       "      <th>August</th>\n",
       "      <th>Septermber</th>\n",
       "      <th>October</th>\n",
       "      <th>November</th>\n",
       "      <th>December</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1992-01-01</td>\n",
       "      <td>0.503702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.138237</td>\n",
       "      <td>-0.368419</td>\n",
       "      <td>-0.485131</td>\n",
       "      <td>-1.694950</td>\n",
       "      <td>0.575796</td>\n",
       "      <td>-0.787113</td>\n",
       "      <td>0.389269</td>\n",
       "      <td>70.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992-01-02</td>\n",
       "      <td>0.503702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.916780</td>\n",
       "      <td>-0.359414</td>\n",
       "      <td>-0.717171</td>\n",
       "      <td>-1.648949</td>\n",
       "      <td>-0.455567</td>\n",
       "      <td>-0.610320</td>\n",
       "      <td>-0.105418</td>\n",
       "      <td>70.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992-01-03</td>\n",
       "      <td>0.503702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.003120</td>\n",
       "      <td>0.155398</td>\n",
       "      <td>0.521572</td>\n",
       "      <td>-0.763310</td>\n",
       "      <td>-1.945359</td>\n",
       "      <td>0.406430</td>\n",
       "      <td>0.978409</td>\n",
       "      <td>70.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992-01-04</td>\n",
       "      <td>0.503702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.013071</td>\n",
       "      <td>1.580237</td>\n",
       "      <td>1.798763</td>\n",
       "      <td>0.130060</td>\n",
       "      <td>-2.642089</td>\n",
       "      <td>0.746722</td>\n",
       "      <td>2.634629</td>\n",
       "      <td>70.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992-01-05</td>\n",
       "      <td>0.503702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.245384</td>\n",
       "      <td>0.645080</td>\n",
       "      <td>1.815982</td>\n",
       "      <td>-0.175384</td>\n",
       "      <td>-3.348068</td>\n",
       "      <td>0.378398</td>\n",
       "      <td>1.619435</td>\n",
       "      <td>70.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 veg  ignition  mean.air.2m  mean.apcp  mean.rhum.2m  \\\n",
       "1992-01-01  0.503702       0.0    -1.138237  -0.368419     -0.485131   \n",
       "1992-01-02  0.503702       0.0    -0.916780  -0.359414     -0.717171   \n",
       "1992-01-03  0.503702       0.0    -1.003120   0.155398      0.521572   \n",
       "1992-01-04  0.503702       0.0    -1.013071   1.580237      1.798763   \n",
       "1992-01-05  0.503702       0.0    -1.245384   0.645080      1.815982   \n",
       "\n",
       "            mean.dpt.2m  mean.pres.sfc  mean.uwnd.10m  mean.vwnd.10m  \\\n",
       "1992-01-01    -1.694950       0.575796      -0.787113       0.389269   \n",
       "1992-01-02    -1.648949      -0.455567      -0.610320      -0.105418   \n",
       "1992-01-03    -0.763310      -1.945359       0.406430       0.978409   \n",
       "1992-01-04     0.130060      -2.642089       0.746722       2.634629   \n",
       "1992-01-05    -0.175384      -3.348068       0.378398       1.619435   \n",
       "\n",
       "            mean.veg  ...  March  April  May  June  July  August  Septermber  \\\n",
       "1992-01-01      70.7  ...    0.0    0.0  0.0   0.0   0.0     0.0         0.0   \n",
       "1992-01-02      70.7  ...    0.0    0.0  0.0   0.0   0.0     0.0         0.0   \n",
       "1992-01-03      70.7  ...    0.0    0.0  0.0   0.0   0.0     0.0         0.0   \n",
       "1992-01-04      70.7  ...    0.0    0.0  0.0   0.0   0.0     0.0         0.0   \n",
       "1992-01-05      70.7  ...    0.0    0.0  0.0   0.0   0.0     0.0         0.0   \n",
       "\n",
       "            October  November  December  \n",
       "1992-01-01      0.0       0.0       0.0  \n",
       "1992-01-02      0.0       0.0       0.0  \n",
       "1992-01-03      0.0       0.0       0.0  \n",
       "1992-01-04      0.0       0.0       0.0  \n",
       "1992-01-05      0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data up into training, testing and validation sets\n",
    "test_data = data.tail(int(len(data)*0.1))\n",
    "leftover_data = data.iloc[:-int(len(data)*0.1)]\n",
    "\n",
    "validation_data = data.tail(int(len(leftover_data)*0.3))\n",
    "training_data = data.iloc[:-int(len(leftover_data)*0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "training_data = np.array(training_data)\n",
    "validation_data = np.array(validation_data)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_target = 1\n",
    "step = 1\n",
    "\n",
    "initial_bias = -1.4\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "# weight_for_0 = 0.5 \n",
    "# weight_for_1 = 13\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 100\n",
    "STEPS_PER_EPOCH = (len(training_data) * 0.25) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(validation_data) * 0.25) // BATCH_SIZE\n",
    "\n",
    "path_best_model = '../trained_models/best_skopt_LTSM.keras'\n",
    "best_fraction_incorrect = 1.0\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'), \n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    f1\n",
    "]\n",
    "\n",
    "dim_learning_rate = Real(\n",
    "    low=0.00001, \n",
    "    high=0.1, \n",
    "    prior='log-uniform',\n",
    "    name='learning_rate'\n",
    ")\n",
    "\n",
    "dim_past_history = Integer(\n",
    "    low=1,\n",
    "    high=30, \n",
    "    name='past_history'\n",
    ")\n",
    "\n",
    "dim_lstm_units = Integer(\n",
    "    low=5, \n",
    "    high=500,\n",
    "    name='lstm_units'\n",
    ")\n",
    "\n",
    "dim_hidden_layers = Integer(\n",
    "    low=1, \n",
    "    high=10,\n",
    "    name='hidden_layers'\n",
    ")\n",
    "\n",
    "dim_hidden_units = Integer(\n",
    "    low=5, \n",
    "    high=500,\n",
    "    name='hidden_units'\n",
    ")\n",
    "\n",
    "# dim_lstm_l2_lambda = Real(\n",
    "#     low=0.0001, \n",
    "#     high=0.1,\n",
    "#     prior='log-uniform',\n",
    "#     name='lstm_l2_lambda'\n",
    "# )\n",
    "\n",
    "dim_hidden_l2_lambda = Real(\n",
    "    low=0.0001, \n",
    "    high=0.1,\n",
    "    prior='log-uniform',\n",
    "    name='hidden_l2_lambda'\n",
    ")\n",
    "\n",
    "dim_class_0_weight = Real(\n",
    "    low=0.1, \n",
    "    high=1,\n",
    "    name='class_0_weight'\n",
    ")\n",
    "\n",
    "dim_class_1_weight = Integer(\n",
    "    low=10, \n",
    "    high=20,\n",
    "    name='class_1_weight'\n",
    ")\n",
    "\n",
    "default_parameters = [0.001, 3, 50, 2, 50, 0.1, 0.5, 15]\n",
    "\n",
    "dimensions = [\n",
    "    dim_learning_rate,\n",
    "    dim_past_history,\n",
    "    dim_lstm_units,\n",
    "    dim_hidden_layers,\n",
    "    dim_hidden_units,\n",
    "    #dim_lstm_l2_lambda,\n",
    "    dim_hidden_l2_lambda,\n",
    "    dim_class_0_weight,\n",
    "    dim_class_1_weight\n",
    "]\n",
    "\n",
    "# Use early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    input_dim,\n",
    "    learning_rate,\n",
    "    lstm_units,\n",
    "    hidden_layers,\n",
    "    hidden_units,\n",
    "    #lstm_l2_lambda,\n",
    "    hidden_l2_lambda\n",
    "):    \n",
    "    input_shape = (100, input_dim[0], input_dim[1])\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.LSTM(\n",
    "        lstm_units,\n",
    "        batch_input_shape=input_shape,\n",
    "#         bias_initializer=keras.initializers.VarianceScaling(\n",
    "#             scale=1.0,\n",
    "#             mode='fan_in', \n",
    "#             distribution='normal', \n",
    "#             seed=None\n",
    "#         ),\n",
    "#         kernel_regularizer=keras.regularizers.l2(lstm_l2_lambda),\n",
    "#         activation = 'relu',\n",
    "         stateful = True\n",
    "    ))\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(keras.layers.Dense(\n",
    "            hidden_units,\n",
    "            bias_initializer=keras.initializers.VarianceScaling(\n",
    "                scale=1.0,\n",
    "                mode='fan_in', \n",
    "                distribution='normal', \n",
    "                seed=None\n",
    "            ),\n",
    "            kernel_regularizer=keras.regularizers.l2(hidden_l2_lambda),\n",
    "            activation = 'relu'\n",
    "        ))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation = 'sigmoid',\n",
    "        bias_initializer = output_bias)\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr = learning_rate), \n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(\n",
    "    learning_rate,\n",
    "    past_history,\n",
    "    lstm_units,\n",
    "    hidden_layers,\n",
    "    hidden_units,\n",
    "    #lstm_l2_lambda,\n",
    "    hidden_l2_lambda,\n",
    "    class_0_weight,\n",
    "    class_1_weight\n",
    "):\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('past history:', past_history)\n",
    "    print('LSTM units:', lstm_units)\n",
    "    print('hidden layers:', hidden_layers)\n",
    "    print('hidden units:', hidden_units)\n",
    "    #print('lstm l2 lambda: {0:.1e}'.format(lstm_l2_lambda))\n",
    "    print('hidden l2 lambda: {0:.1e}'.format(hidden_l2_lambda))\n",
    "    print('class 0 weight:', class_0_weight)\n",
    "    print('class 1 weight:', class_1_weight)\n",
    "    print()\n",
    "    \n",
    "    # create data stream\n",
    "    x_train, y_train = multivariate_data(\n",
    "        training_data, \n",
    "        training_data[:, 1], \n",
    "        0,\n",
    "        None,\n",
    "        past_history,\n",
    "        future_target, \n",
    "        step\n",
    "    )\n",
    "    \n",
    "    x_validation, y_validation = multivariate_data(\n",
    "        validation_data, \n",
    "        validation_data[:, 1], \n",
    "        0,\n",
    "        None,\n",
    "        past_history,\n",
    "        future_target, \n",
    "        step\n",
    "    )\n",
    "    \n",
    "    start_index = (x_train.shape[0] - (x_train.shape[0] % 100))\n",
    "    end_index = x_train.shape[0]\n",
    "    \n",
    "    x_train = np.delete(x_train, range(start_index, end_index), axis=0)\n",
    "    y_train = np.delete(y_train, range(start_index, end_index), axis=0)\n",
    "    \n",
    "    start_index = (x_validation.shape[0] - (x_validation.shape[0] % 100))\n",
    "    end_index = x_validation.shape[0]\n",
    "    \n",
    "    x_validation = np.delete(x_validation, range(start_index, end_index), axis=0)\n",
    "    y_validation = np.delete(y_validation, range(start_index, end_index), axis=0)\n",
    "    \n",
    "    input_dim = x_train.shape[-2:]\n",
    "    \n",
    "    class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = make_model(\n",
    "        input_dim,\n",
    "        learning_rate = learning_rate,\n",
    "        lstm_units = lstm_units,\n",
    "        hidden_layers = hidden_layers,\n",
    "        hidden_units = hidden_units,\n",
    "        #lstm_l2_lambda = lstm_l2_lambda,\n",
    "        hidden_l2_lambda = hidden_l2_lambda,\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    print()\n",
    "\n",
    "    # Dir-name for the TensorBoard log-files.\n",
    "    log_dir = log_dir_name(\n",
    "        learning_rate,\n",
    "        past_history,\n",
    "        lstm_units,\n",
    "        hidden_layers,\n",
    "        hidden_units,\n",
    "        #lstm_l2_lambda,\n",
    "        hidden_l2_lambda,\n",
    "        class_0_weight,\n",
    "        class_1_weight\n",
    "    )\n",
    "    \n",
    "    # Create a callback-function for Keras which will be\n",
    "    # run after each epoch has ended during training.\n",
    "    # This saves the log-files for TensorBoard.\n",
    "    # Note that there are complications when histogram_freq=1.\n",
    "    # It might give strange errors and it also does not properly\n",
    "    # support Keras data-generators for the validation-set.\n",
    "    callback_log = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False\n",
    "    )\n",
    "   \n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        callbacks = [early_stopping],\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        validation_steps=VALIDATION_STEPS,\n",
    "        class_weight=class_weight,\n",
    "        workers=8\n",
    "    )\n",
    "\n",
    "    # Get fraction incorrect on the validation-set\n",
    "    # after the last training-epoch.\n",
    "          \n",
    "    val_fp = history.history['val_fp'][-1]\n",
    "    val_fn = history.history['val_fn'][-1]\n",
    "    val_tp = history.history['val_tp'][-1]\n",
    "    val_tn = history.history['val_tn'][-1]\n",
    "          \n",
    "    fraction_incorrect = (val_fn /(val_fn + val_tp + K.epsilon())) + (val_fp / (val_fp + val_tn + K.epsilon()))\n",
    "    \n",
    "    print()\n",
    "    print(\"Validation fraction incorrect: {0:.2}\".format(fraction_incorrect))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_fraction_incorrect\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if fraction_incorrect < best_fraction_incorrect:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_fraction_incorrect = fraction_incorrect\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return fraction_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-03\n",
      "past history: 3\n",
      "LSTM units: 50\n",
      "hidden layers: 2\n",
      "hidden units: 50\n",
      "hidden l2 lambda: 1.0e-01\n",
      "class 0 weight: 0.5\n",
      "class 1 weight: 15\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (100, 50)                 19800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 50)                 2550      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 50)                 2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (100, 1)                  51        \n",
      "=================================================================\n",
      "Total params: 24,951\n",
      "Trainable params: 24,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 6300 samples, validate on 2300 samples\n",
      "Epoch 1/15\n",
      "1500/6300 [======>.......................] - ETA: 15s - loss: 9.9303 - tp: 28.0000 - fp: 468.0000 - tn: 962.0000 - fn: 42.0000 - accuracy: 0.6600 - precision: 0.0565 - recall: 0.4000 - auc: 0.5524 - f1: 0.2934 - val_loss: 1.9503 - val_tp: 19.0000 - val_fp: 466.0000 - val_tn: 15.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.0680 - val_precision: 0.0392 - val_recall: 1.0000 - val_auc: 0.6869 - val_f1: 0.4648Epoch 2/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 8.3435 - tp: 70.0000 - fp: 1309.0000 - tn: 121.0000 - fn: 0.0000e+00 - accuracy: 0.1273 - precision: 0.0508 - recall: 1.0000 - auc: 0.7375 - f1: 0.6544 - val_loss: 1.6385 - val_tp: 19.0000 - val_fp: 398.0000 - val_tn: 83.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.2040 - val_precision: 0.0456 - val_recall: 1.0000 - val_auc: 0.6989 - val_f1: 0.4805Epoch 3/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 7.0411 - tp: 76.0000 - fp: 922.0000 - tn: 500.0000 - fn: 2.0000 - accuracy: 0.3840 - precision: 0.0762 - recall: 0.9744 - auc: 0.7457 - f1: 0.8166 - val_loss: 1.3743 - val_tp: 18.0000 - val_fp: 324.0000 - val_tn: 157.0000 - val_fn: 1.0000 - val_accuracy: 0.3500 - val_precision: 0.0526 - val_recall: 0.9474 - val_auc: 0.6898 - val_f1: 0.4769Epoch 4/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 5.8600 - tp: 69.0000 - fp: 871.0000 - tn: 558.0000 - fn: 2.0000 - accuracy: 0.4180 - precision: 0.0734 - recall: 0.9718 - auc: 0.7970 - f1: 0.7649 - val_loss: 1.1546 - val_tp: 17.0000 - val_fp: 252.0000 - val_tn: 229.0000 - val_fn: 2.0000 - val_accuracy: 0.4920 - val_precision: 0.0632 - val_recall: 0.8947 - val_auc: 0.6943 - val_f1: 0.4778Epoch 5/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 4.9762 - tp: 83.0000 - fp: 799.0000 - tn: 617.0000 - fn: 1.0000 - accuracy: 0.4667 - precision: 0.0941 - recall: 0.9881 - auc: 0.7537 - f1: 0.9220 - val_loss: 0.9741 - val_tp: 17.0000 - val_fp: 277.0000 - val_tn: 204.0000 - val_fn: 2.0000 - val_accuracy: 0.4420 - val_precision: 0.0578 - val_recall: 0.8947 - val_auc: 0.7095 - val_f1: 0.4600Epoch 6/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 4.2067 - tp: 83.0000 - fp: 672.0000 - tn: 739.0000 - fn: 6.0000 - accuracy: 0.5480 - precision: 0.1099 - recall: 0.9326 - auc: 0.7787 - f1: 0.9782 - val_loss: 0.8185 - val_tp: 18.0000 - val_fp: 256.0000 - val_tn: 225.0000 - val_fn: 1.0000 - val_accuracy: 0.4860 - val_precision: 0.0657 - val_recall: 0.9474 - val_auc: 0.7179 - val_f1: 0.4915Epoch 7/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 3.4439 - tp: 59.0000 - fp: 623.0000 - tn: 816.0000 - fn: 2.0000 - accuracy: 0.5833 - precision: 0.0865 - recall: 0.9672 - auc: 0.8231 - f1: 0.8730 - val_loss: 0.6856 - val_tp: 17.0000 - val_fp: 203.0000 - val_tn: 278.0000 - val_fn: 2.0000 - val_accuracy: 0.5900 - val_precision: 0.0773 - val_recall: 0.8947 - val_auc: 0.7335 - val_f1: 0.5019Epoch 8/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 2.9461 - tp: 52.0000 - fp: 521.0000 - tn: 917.0000 - fn: 10.0000 - accuracy: 0.6460 - precision: 0.0908 - recall: 0.8387 - auc: 0.8020 - f1: 0.8250 - val_loss: 0.5813 - val_tp: 17.0000 - val_fp: 217.0000 - val_tn: 264.0000 - val_fn: 2.0000 - val_accuracy: 0.5620 - val_precision: 0.0726 - val_recall: 0.8947 - val_auc: 0.7359 - val_f1: 0.4858Epoch 9/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 2.5958 - tp: 72.0000 - fp: 666.0000 - tn: 754.0000 - fn: 8.0000 - accuracy: 0.5507 - precision: 0.0976 - recall: 0.9000 - auc: 0.7816 - f1: 0.8825 - val_loss: 0.5025 - val_tp: 19.0000 - val_fp: 262.0000 - val_tn: 219.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.4760 - val_precision: 0.0676 - val_recall: 1.0000 - val_auc: 0.7349 - val_f1: 0.5141Epoch 10/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 2.2096 - tp: 76.0000 - fp: 700.0000 - tn: 720.0000 - fn: 4.0000 - accuracy: 0.5307 - precision: 0.0979 - recall: 0.9500 - auc: 0.7798 - f1: 0.9396 - val_loss: 0.4309 - val_tp: 19.0000 - val_fp: 252.0000 - val_tn: 229.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.4960 - val_precision: 0.0701 - val_recall: 1.0000 - val_auc: 0.7342 - val_f1: 0.5164Epoch 11/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 1.8396 - tp: 60.0000 - fp: 588.0000 - tn: 847.0000 - fn: 5.0000 - accuracy: 0.6047 - precision: 0.0926 - recall: 0.9231 - auc: 0.8167 - f1: 0.8821 - val_loss: 0.3679 - val_tp: 17.0000 - val_fp: 206.0000 - val_tn: 275.0000 - val_fn: 2.0000 - val_accuracy: 0.5840 - val_precision: 0.0762 - val_recall: 0.8947 - val_auc: 0.7297 - val_f1: 0.4972Epoch 12/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 1.6217 - tp: 68.0000 - fp: 598.0000 - tn: 831.0000 - fn: 3.0000 - accuracy: 0.5993 - precision: 0.1021 - recall: 0.9577 - auc: 0.7999 - f1: 0.9539 - val_loss: 0.3220 - val_tp: 18.0000 - val_fp: 217.0000 - val_tn: 264.0000 - val_fn: 1.0000 - val_accuracy: 0.5640 - val_precision: 0.0766 - val_recall: 0.9474 - val_auc: 0.7275 - val_f1: 0.5157Epoch 13/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 1.4039 - tp: 57.0000 - fp: 549.0000 - tn: 890.0000 - fn: 4.0000 - accuracy: 0.6313 - precision: 0.0941 - recall: 0.9344 - auc: 0.8065 - f1: 0.8683 - val_loss: 0.2836 - val_tp: 17.0000 - val_fp: 208.0000 - val_tn: 273.0000 - val_fn: 2.0000 - val_accuracy: 0.5800 - val_precision: 0.0756 - val_recall: 0.8947 - val_auc: 0.7236 - val_f1: 0.4970Epoch 14/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 1.3112 - tp: 70.0000 - fp: 620.0000 - tn: 805.0000 - fn: 5.0000 - accuracy: 0.5833 - precision: 0.1014 - recall: 0.9333 - auc: 0.7908 - f1: 0.9630 - val_loss: 0.2555 - val_tp: 19.0000 - val_fp: 241.0000 - val_tn: 240.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5180 - val_precision: 0.0731 - val_recall: 1.0000 - val_auc: 0.7203 - val_f1: 0.5256Epoch 15/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 1.2248 - tp: 83.0000 - fp: 705.0000 - tn: 707.0000 - fn: 5.0000 - accuracy: 0.5267 - precision: 0.1053 - recall: 0.9432 - auc: 0.7688 - f1: 0.9630 - val_loss: 0.2337 - val_tp: 19.0000 - val_fp: 247.0000 - val_tn: 234.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5060 - val_precision: 0.0714 - val_recall: 1.0000 - val_auc: 0.7354 - val_f1: 0.5214\n",
      "Validation fraction incorrect: 0.51\n",
      "\n",
      "learning rate: 1.5e-02\n",
      "past history: 6\n",
      "LSTM units: 391\n",
      "hidden layers: 6\n",
      "hidden units: 226\n",
      "hidden l2 lambda: 2.0e-04\n",
      "class 0 weight: 0.5133240027692806\n",
      "class 1 weight: 13\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (100, 391)                688160    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 226)                88592     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 226)                51302     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (100, 226)                51302     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (100, 226)                51302     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (100, 226)                51302     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (100, 226)                51302     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (100, 1)                  227       \n",
      "=================================================================\n",
      "Total params: 1,033,489\n",
      "Trainable params: 1,033,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 6300 samples, validate on 2300 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/6300 [======>.......................] - ETA: 18s - loss: 3.8613 - tp: 23.0000 - fp: 461.0000 - tn: 969.0000 - fn: 47.0000 - accuracy: 0.6613 - precision: 0.0475 - recall: 0.3286 - auc: 0.4897 - f1: 0.2112 - val_loss: 0.2069 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 482.0000 - val_fn: 18.0000 - val_accuracy: 0.9640 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3192 - val_f1: 0.0000e+00Epoch 2/15\n",
      "1500/6300 [======>.......................] - ETA: 4s - loss: 1.0427 - tp: 57.0000 - fp: 800.0000 - tn: 620.0000 - fn: 23.0000 - accuracy: 0.4513 - precision: 0.0665 - recall: 0.7125 - auc: 0.5837 - f1: 0.5723 - val_loss: 0.1938 - val_tp: 18.0000 - val_fp: 331.0000 - val_tn: 151.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.3380 - val_precision: 0.0516 - val_recall: 1.0000 - val_auc: 0.7843 - val_f1: 0.4822Epoch 3/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.9476 - tp: 46.0000 - fp: 573.0000 - tn: 856.0000 - fn: 25.0000 - accuracy: 0.6013 - precision: 0.0743 - recall: 0.6479 - auc: 0.6817 - f1: 0.5734 - val_loss: 0.1974 - val_tp: 18.0000 - val_fp: 263.0000 - val_tn: 219.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.4740 - val_precision: 0.0641 - val_recall: 1.0000 - val_auc: 0.7846 - val_f1: 0.4986Epoch 4/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.8353 - tp: 60.0000 - fp: 517.0000 - tn: 906.0000 - fn: 17.0000 - accuracy: 0.6440 - precision: 0.1040 - recall: 0.7792 - auc: 0.7867 - f1: 0.7976 - val_loss: 0.1597 - val_tp: 10.0000 - val_fp: 128.0000 - val_tn: 354.0000 - val_fn: 8.0000 - val_accuracy: 0.7280 - val_precision: 0.0725 - val_recall: 0.5556 - val_auc: 0.7607 - val_f1: 0.3654Epoch 5/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.9043 - tp: 55.0000 - fp: 627.0000 - tn: 797.0000 - fn: 21.0000 - accuracy: 0.5680 - precision: 0.0806 - recall: 0.7237 - auc: 0.7144 - f1: 0.6041 - val_loss: 0.1824 - val_tp: 17.0000 - val_fp: 292.0000 - val_tn: 190.0000 - val_fn: 1.0000 - val_accuracy: 0.4140 - val_precision: 0.0550 - val_recall: 0.9444 - val_auc: 0.7286 - val_f1: 0.4664Epoch 6/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 1.0198 - tp: 72.0000 - fp: 1128.0000 - tn: 287.0000 - fn: 13.0000 - accuracy: 0.2393 - precision: 0.0600 - recall: 0.8471 - auc: 0.5587 - f1: 0.6245 - val_loss: 0.1763 - val_tp: 18.0000 - val_fp: 344.0000 - val_tn: 138.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.3120 - val_precision: 0.0497 - val_recall: 1.0000 - val_auc: 0.7287 - val_f1: 0.4879Epoch 7/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.8725 - tp: 7.0000 - fp: 97.0000 - tn: 1346.0000 - fn: 50.0000 - accuracy: 0.9020 - precision: 0.0673 - recall: 0.1228 - auc: 0.5442 - f1: 0.1045 - val_loss: 0.1711 - val_tp: 16.0000 - val_fp: 231.0000 - val_tn: 251.0000 - val_fn: 2.0000 - val_accuracy: 0.5340 - val_precision: 0.0648 - val_recall: 0.8889 - val_auc: 0.7496 - val_f1: 0.4659Epoch 8/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.8136 - tp: 60.0000 - fp: 863.0000 - tn: 573.0000 - fn: 4.0000 - accuracy: 0.4220 - precision: 0.0650 - recall: 0.9375 - auc: 0.7217 - f1: 0.7073 - val_loss: 0.1685 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 482.0000 - val_fn: 18.0000 - val_accuracy: 0.9640 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7684 - val_f1: 0.0000e+00Epoch 9/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.8057 - tp: 47.0000 - fp: 402.0000 - tn: 1025.0000 - fn: 26.0000 - accuracy: 0.7147 - precision: 0.1047 - recall: 0.6438 - auc: 0.7690 - f1: 0.6566 - val_loss: 0.1479 - val_tp: 15.0000 - val_fp: 186.0000 - val_tn: 296.0000 - val_fn: 3.0000 - val_accuracy: 0.6220 - val_precision: 0.0746 - val_recall: 0.8333 - val_auc: 0.7619 - val_f1: 0.4691Epoch 10/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.8743 - tp: 79.0000 - fp: 757.0000 - tn: 656.0000 - fn: 8.0000 - accuracy: 0.4900 - precision: 0.0945 - recall: 0.9080 - auc: 0.7432 - f1: 0.9070 - val_loss: 0.1646 - val_tp: 18.0000 - val_fp: 320.0000 - val_tn: 162.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.3600 - val_precision: 0.0533 - val_recall: 1.0000 - val_auc: 0.7363 - val_f1: 0.4803Epoch 11/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.8435 - tp: 51.0000 - fp: 740.0000 - tn: 687.0000 - fn: 22.0000 - accuracy: 0.4920 - precision: 0.0645 - recall: 0.6986 - auc: 0.6634 - f1: 0.5908 - val_loss: 0.1564 - val_tp: 18.0000 - val_fp: 330.0000 - val_tn: 152.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.3400 - val_precision: 0.0517 - val_recall: 1.0000 - val_auc: 0.7554 - val_f1: 0.4826Epoch 12/15\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.7736 - tp: 61.0000 - fp: 667.0000 - tn: 761.0000 - fn: 11.0000 - accuracy: 0.5480 - precision: 0.0838 - recall: 0.8472 - auc: 0.7385 - f1: 0.8147 - val_loss: 0.1428 - val_tp: 12.0000 - val_fp: 145.0000 - val_tn: 337.0000 - val_fn: 6.0000 - val_accuracy: 0.6980 - val_precision: 0.0764 - val_recall: 0.6667 - val_auc: 0.7427 - val_f1: 0.4270Epoch 13/15\n",
      "1500/6300 [======>.......................] - ETA: 4s - loss: 0.8317 - tp: 37.0000 - fp: 481.0000 - tn: 964.0000 - fn: 18.0000 - accuracy: 0.6673 - precision: 0.0714 - recall: 0.6727 - auc: 0.7227 - f1: 0.5903Restoring model weights from the end of the best epoch.\n",
      "1500/6300 [======>.......................] - ETA: 5s - loss: 0.8315 - tp: 37.0000 - fp: 481.0000 - tn: 964.0000 - fn: 18.0000 - accuracy: 0.6673 - precision: 0.0714 - recall: 0.6727 - auc: 0.7227 - f1: 0.5903 - val_loss: 0.1640 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 482.0000 - val_fn: 18.0000 - val_accuracy: 0.9640 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7334 - val_f1: 0.0000e+00Epoch 00013: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 3.7e-05\n",
      "past history: 20\n",
      "LSTM units: 33\n",
      "hidden layers: 7\n",
      "hidden units: 470\n",
      "hidden l2 lambda: 1.0e-04\n",
      "class 0 weight: 0.992990403362096\n",
      "class 1 weight: 16\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (100, 33)                 10824     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 470)                15980     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 470)                221370    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (100, 470)                221370    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (100, 470)                221370    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (100, 470)                221370    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (100, 470)                221370    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (100, 470)                221370    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (100, 1)                  471       \n",
      "=================================================================\n",
      "Total params: 1,355,495\n",
      "Trainable params: 1,355,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 6300 samples, validate on 2300 samples\n",
      "Epoch 1/15\n",
      "1500/6300 [======>.......................] - ETA: 19s - loss: 1.7237 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1426.0000 - fn: 74.0000 - accuracy: 0.9507 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5218 - f1: 0.0000e+00 - val_loss: 0.2868 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5093 - val_f1: 0.0000e+00Epoch 2/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.7318 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1417.0000 - fn: 83.0000 - accuracy: 0.9447 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5612 - f1: 0.0000e+00 - val_loss: 0.2775 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6509 - val_f1: 0.0000e+00Epoch 3/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.5019 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1428.0000 - fn: 72.0000 - accuracy: 0.9520 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5551 - f1: 0.0000e+00 - val_loss: 0.2734 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7258 - val_f1: 0.0000e+00Epoch 4/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.4089 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1433.0000 - fn: 67.0000 - accuracy: 0.9553 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5584 - f1: 0.0000e+00 - val_loss: 0.2750 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7384 - val_f1: 0.0000e+00Epoch 5/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.5391 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1415.0000 - fn: 85.0000 - accuracy: 0.9433 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6490 - f1: 0.0000e+00 - val_loss: 0.2791 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7125 - val_f1: 0.0000e+00Epoch 6/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.3971 - tp: 3.0000 - fp: 36.0000 - tn: 1397.0000 - fn: 64.0000 - accuracy: 0.9333 - precision: 0.0769 - recall: 0.0448 - auc: 0.6495 - f1: 0.0704 - val_loss: 0.2730 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7216 - val_f1: 0.0000e+00Epoch 7/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.2948 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1443.0000 - fn: 57.0000 - accuracy: 0.9620 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6657 - f1: 0.0000e+00 - val_loss: 0.2670 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7268 - val_f1: 0.0000e+00Epoch 8/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.4807 - tp: 0.0000e+00 - fp: 1.0000 - tn: 1420.0000 - fn: 79.0000 - accuracy: 0.9467 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6286 - f1: 0.0000e+00 - val_loss: 0.2699 - val_tp: 0.0000e+00 - val_fp: 5.0000 - val_tn: 478.0000 - val_fn: 17.0000 - val_accuracy: 0.9560 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7333 - val_f1: 0.0000e+00Epoch 9/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.4868 - tp: 42.0000 - fp: 363.0000 - tn: 1052.0000 - fn: 43.0000 - accuracy: 0.7293 - precision: 0.1037 - recall: 0.4941 - auc: 0.7139 - f1: 0.6162 - val_loss: 0.2724 - val_tp: 16.0000 - val_fp: 252.0000 - val_tn: 231.0000 - val_fn: 1.0000 - val_accuracy: 0.4940 - val_precision: 0.0597 - val_recall: 0.9412 - val_auc: 0.7354 - val_f1: 0.4585Epoch 10/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.4470 - tp: 49.0000 - fp: 452.0000 - tn: 966.0000 - fn: 33.0000 - accuracy: 0.6767 - precision: 0.0978 - recall: 0.5976 - auc: 0.6983 - f1: 0.6963 - val_loss: 0.2649 - val_tp: 11.0000 - val_fp: 172.0000 - val_tn: 311.0000 - val_fn: 6.0000 - val_accuracy: 0.6440 - val_precision: 0.0601 - val_recall: 0.6471 - val_auc: 0.7329 - val_f1: 0.3553Epoch 11/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.3131 - tp: 42.0000 - fp: 503.0000 - tn: 931.0000 - fn: 24.0000 - accuracy: 0.6487 - precision: 0.0771 - recall: 0.6364 - auc: 0.7219 - f1: 0.6771 - val_loss: 0.2548 - val_tp: 1.0000 - val_fp: 29.0000 - val_tn: 454.0000 - val_fn: 16.0000 - val_accuracy: 0.9100 - val_precision: 0.0333 - val_recall: 0.0588 - val_auc: 0.7324 - val_f1: 0.0976Epoch 12/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.3190 - tp: 27.0000 - fp: 254.0000 - tn: 1175.0000 - fn: 44.0000 - accuracy: 0.8013 - precision: 0.0961 - recall: 0.3803 - auc: 0.7381 - f1: 0.5337 - val_loss: 0.2502 - val_tp: 8.0000 - val_fp: 100.0000 - val_tn: 383.0000 - val_fn: 9.0000 - val_accuracy: 0.7820 - val_precision: 0.0741 - val_recall: 0.4706 - val_auc: 0.7344 - val_f1: 0.3314Epoch 13/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.2031 - tp: 26.0000 - fp: 223.0000 - tn: 1217.0000 - fn: 34.0000 - accuracy: 0.8287 - precision: 0.1044 - recall: 0.4333 - auc: 0.7488 - f1: 0.5781 - val_loss: 0.2435 - val_tp: 8.0000 - val_fp: 93.0000 - val_tn: 390.0000 - val_fn: 9.0000 - val_accuracy: 0.7960 - val_precision: 0.0792 - val_recall: 0.4706 - val_auc: 0.7360 - val_f1: 0.3384Epoch 14/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.3526 - tp: 61.0000 - fp: 598.0000 - tn: 821.0000 - fn: 20.0000 - accuracy: 0.5880 - precision: 0.0926 - recall: 0.7531 - auc: 0.7147 - f1: 0.7998 - val_loss: 0.2431 - val_tp: 11.0000 - val_fp: 182.0000 - val_tn: 301.0000 - val_fn: 6.0000 - val_accuracy: 0.6240 - val_precision: 0.0570 - val_recall: 0.6471 - val_auc: 0.7393 - val_f1: 0.3446Epoch 15/15\n",
      "1500/6300 [======>.......................] - ETA: 3s - loss: 1.2462 - tp: 50.0000 - fp: 459.0000 - tn: 967.0000 - fn: 24.0000 - accuracy: 0.6780 - precision: 0.0982 - recall: 0.6757 - auc: 0.7552 - f1: 0.7668 - val_loss: 0.2387 - val_tp: 11.0000 - val_fp: 182.0000 - val_tn: 301.0000 - val_fn: 6.0000 - val_accuracy: 0.6240 - val_precision: 0.0570 - val_recall: 0.6471 - val_auc: 0.7390 - val_f1: 0.3434\n",
      "Validation fraction incorrect: 0.73\n",
      "\n",
      "learning rate: 2.8e-03\n",
      "past history: 1\n",
      "LSTM units: 16\n",
      "hidden layers: 6\n",
      "hidden units: 203\n",
      "hidden l2 lambda: 1.4e-04\n",
      "class 0 weight: 0.9763799669573134\n",
      "class 1 weight: 12\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (100, 16)                 4160      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 203)                3451      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 203)                41412     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (100, 203)                41412     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (100, 203)                41412     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (100, 203)                41412     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (100, 203)                41412     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (100, 1)                  204       \n",
      "=================================================================\n",
      "Total params: 214,875\n",
      "Trainable params: 214,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 6300 samples, validate on 2300 samples\n",
      "Epoch 1/15\n",
      "1500/6300 [======>.......................] - ETA: 16s - loss: 1.0537 - tp: 8.0000 - fp: 186.0000 - tn: 1253.0000 - fn: 53.0000 - accuracy: 0.8407 - precision: 0.0412 - recall: 0.1311 - auc: 0.5174 - f1: 0.0741 - val_loss: 0.2185 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 480.0000 - val_fn: 20.0000 - val_accuracy: 0.9600 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5125 - val_f1: 0.0000e+00Epoch 2/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/6300 [======>.......................] - ETA: 0s - loss: 1.1379 - tp: 9.0000 - fp: 52.0000 - tn: 1367.0000 - fn: 72.0000 - accuracy: 0.9173 - precision: 0.1475 - recall: 0.1111 - auc: 0.5464 - f1: 0.1242 - val_loss: 0.2046 - val_tp: 18.0000 - val_fp: 212.0000 - val_tn: 268.0000 - val_fn: 2.0000 - val_accuracy: 0.5720 - val_precision: 0.0783 - val_recall: 0.9000 - val_auc: 0.7582 - val_f1: 0.4955Epoch 3/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.8656 - tp: 57.0000 - fp: 419.0000 - tn: 1010.0000 - fn: 14.0000 - accuracy: 0.7113 - precision: 0.1197 - recall: 0.8028 - auc: 0.7862 - f1: 0.9543 - val_loss: 0.1708 - val_tp: 17.0000 - val_fp: 195.0000 - val_tn: 285.0000 - val_fn: 3.0000 - val_accuracy: 0.6040 - val_precision: 0.0802 - val_recall: 0.8500 - val_auc: 0.7507 - val_f1: 0.4801Epoch 4/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.9423 - tp: 45.0000 - fp: 373.0000 - tn: 1046.0000 - fn: 36.0000 - accuracy: 0.7273 - precision: 0.1077 - recall: 0.5556 - auc: 0.7384 - f1: 0.6187Epoch 5/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.8281 - tp: 58.0000 - fp: 509.0000 - tn: 919.0000 - fn: 14.0000 - accuracy: 0.6513 - precision: 0.1023 - recall: 0.8056 - auc: 0.7791 - f1: 0.8073 - val_loss: 0.1684 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 480.0000 - val_fn: 20.0000 - val_accuracy: 0.9600 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7647 - val_f1: 0.0000e+00Epoch 6/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.8635 - tp: 42.0000 - fp: 403.0000 - tn: 1026.0000 - fn: 29.0000 - accuracy: 0.7120 - precision: 0.0944 - recall: 0.5915 - auc: 0.7375 - f1: 0.5879          Epoch 7/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.8311 - tp: 62.0000 - fp: 441.0000 - tn: 982.0000 - fn: 15.0000 - accuracy: 0.6960 - precision: 0.1233 - recall: 0.8052 - auc: 0.7848 - f1: 0.9332 - val_loss: 0.1613 - val_tp: 17.0000 - val_fp: 170.0000 - val_tn: 310.0000 - val_fn: 3.0000 - val_accuracy: 0.6540 - val_precision: 0.0909 - val_recall: 0.8500 - val_auc: 0.7426 - val_f1: 0.5064Epoch 8/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.7475 - tp: 42.0000 - fp: 355.0000 - tn: 1076.0000 - fn: 27.0000 - accuracy: 0.7453 - precision: 0.1058 - recall: 0.6087 - auc: 0.8041 - f1: 0.6786 - val_loss: 0.1654 - val_tp: 18.0000 - val_fp: 217.0000 - val_tn: 263.0000 - val_fn: 2.0000 - val_accuracy: 0.5620 - val_precision: 0.0766 - val_recall: 0.9000 - val_auc: 0.7488 - val_f1: 0.4980Epoch 9/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.8693 - tp: 36.0000 - fp: 313.0000 - tn: 1109.0000 - fn: 42.0000 - accuracy: 0.7633 - precision: 0.1032 - recall: 0.4615 - auc: 0.7672 - f1: 0.5187 - val_loss: 0.1619 - val_tp: 17.0000 - val_fp: 167.0000 - val_tn: 313.0000 - val_fn: 3.0000 - val_accuracy: 0.6600 - val_precision: 0.0924 - val_recall: 0.8500 - val_auc: 0.7432 - val_f1: 0.5094Epoch 10/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.7739 - tp: 47.0000 - fp: 334.0000 - tn: 1097.0000 - fn: 22.0000 - accuracy: 0.7627 - precision: 0.1234 - recall: 0.6812 - auc: 0.8086 - f1: 0.7382 - val_loss: 0.1636 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 480.0000 - val_fn: 20.0000 - val_accuracy: 0.9600 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7433 - val_f1: 0.0000e+00Epoch 11/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.8835 - tp: 48.0000 - fp: 377.0000 - tn: 1045.0000 - fn: 30.0000 - accuracy: 0.7287 - precision: 0.1129 - recall: 0.6154 - auc: 0.7618 - f1: 0.6529 - val_loss: 0.1675 - val_tp: 18.0000 - val_fp: 195.0000 - val_tn: 285.0000 - val_fn: 2.0000 - val_accuracy: 0.6060 - val_precision: 0.0845 - val_recall: 0.9000 - val_auc: 0.7529 - val_f1: 0.5165Epoch 12/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.8612 - tp: 63.0000 - fp: 454.0000 - tn: 965.0000 - fn: 18.0000 - accuracy: 0.6853 - precision: 0.1219 - recall: 0.7778 - auc: 0.7952 - f1: 0.9499 - val_loss: 0.1715 - val_tp: 18.0000 - val_fp: 186.0000 - val_tn: 294.0000 - val_fn: 2.0000 - val_accuracy: 0.6240 - val_precision: 0.0882 - val_recall: 0.9000 - val_auc: 0.7492 - val_f1: 0.5242Epoch 13/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.7927 - tp: 26.0000 - fp: 307.0000 - tn: 1127.0000 - fn: 40.0000 - accuracy: 0.7687 - precision: 0.0781 - recall: 0.3939 - auc: 0.7623 - f1: 0.5136 - val_loss: 0.1645 - val_tp: 18.0000 - val_fp: 184.0000 - val_tn: 296.0000 - val_fn: 2.0000 - val_accuracy: 0.6280 - val_precision: 0.0891 - val_recall: 0.9000 - val_auc: 0.7520 - val_f1: 0.5268Epoch 14/15\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.8327 - tp: 51.0000 - fp: 437.0000 - tn: 996.0000 - fn: 16.0000 - accuracy: 0.6980 - precision: 0.1045 - recall: 0.7612 - auc: 0.7851 - f1: 0.8644  Restoring model weights from the end of the best epoch.\n",
      "1500/6300 [======>.......................] - ETA: 0s - loss: 0.8326 - tp: 51.0000 - fp: 437.0000 - tn: 996.0000 - fn: 16.0000 - accuracy: 0.6980 - precision: 0.1045 - recall: 0.7612 - auc: 0.7851 - f1: 0.8644 - val_loss: 0.1638 - val_tp: 15.0000 - val_fp: 153.0000 - val_tn: 327.0000 - val_fn: 5.0000 - val_accuracy: 0.6840 - val_precision: 0.0893 - val_recall: 0.7500 - val_auc: 0.7510 - val_f1: 0.4625Epoch 00014: early stopping\n",
      "\n",
      "Validation fraction incorrect: 0.57\n",
      "\n",
      "learning rate: 2.3e-05\n",
      "past history: 19\n",
      "LSTM units: 194\n",
      "hidden layers: 10\n",
      "hidden units: 236\n",
      "hidden l2 lambda: 3.8e-02\n",
      "class 0 weight: 0.7122767847290018\n",
      "class 1 weight: 15\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (100, 194)                188568    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 236)                46020     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 236)                55932     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (100, 236)                55932     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (100, 236)                55932     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (100, 236)                55932     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (100, 236)                55932     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (100, 236)                55932     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (100, 236)                55932     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (100, 236)                55932     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (100, 236)                55932     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (100, 1)                  237       \n",
      "=================================================================\n",
      "Total params: 738,213\n",
      "Trainable params: 738,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 6300 samples, validate on 2300 samples\n",
      "Epoch 1/15\n",
      "1500/6300 [======>.......................] - ETA: 21s - loss: 89.8846 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1424.0000 - fn: 76.0000 - accuracy: 0.9493 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5466 - f1: 0.0000e+00 - val_loss: 19.3618 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5776 - val_f1: 0.0000e+00Epoch 2/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/6300 [======>.......................] - ETA: 5s - loss: 88.9606 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1431.0000 - fn: 69.0000 - accuracy: 0.9540 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5556 - f1: 0.0000e+00 - val_loss: 19.1866 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5021 - val_f1: 0.0000e+00Epoch 3/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 88.3591 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1417.0000 - fn: 83.0000 - accuracy: 0.9447 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6096 - f1: 0.0000e+00 - val_loss: 19.0130 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5306 - val_f1: 0.0000e+00Epoch 4/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 87.3208 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1433.0000 - fn: 67.0000 - accuracy: 0.9553 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5886 - f1: 0.0000e+00 - val_loss: 18.8410 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7214 - val_f1: 0.0000e+00Epoch 5/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 86.6702 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1423.0000 - fn: 77.0000 - accuracy: 0.9487 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6290 - f1: 0.0000e+00 - val_loss: 18.6707 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5369 - val_f1: 0.0000e+00Epoch 6/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 85.8115 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1428.0000 - fn: 72.0000 - accuracy: 0.9520 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6306 - f1: 0.0000e+00 - val_loss: 18.5017 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5306 - val_f1: 0.0000e+00Epoch 7/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 84.9634 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1433.0000 - fn: 67.0000 - accuracy: 0.9553 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6357 - f1: 0.0000e+00 - val_loss: 18.3342 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6387 - val_f1: 0.0000e+00Epoch 8/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 84.2074 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1432.0000 - fn: 68.0000 - accuracy: 0.9547 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5922 - f1: 0.0000e+00 - val_loss: 18.1684 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5773 - val_f1: 0.0000e+00Epoch 9/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 83.7146 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1411.0000 - fn: 89.0000 - accuracy: 0.9407 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5992 - f1: 0.0000e+00 - val_loss: 18.0040 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6350 - val_f1: 0.0000e+00Epoch 10/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 82.5939 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1439.0000 - fn: 61.0000 - accuracy: 0.9593 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6211 - f1: 0.0000e+00 - val_loss: 17.8411 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6078 - val_f1: 0.0000e+00Epoch 11/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 81.9460 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1431.0000 - fn: 69.0000 - accuracy: 0.9540 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6486 - f1: 0.0000e+00 - val_loss: 17.6797 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7670 - val_f1: 0.0000e+00Epoch 12/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 81.2265 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1429.0000 - fn: 71.0000 - accuracy: 0.9527 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6256 - f1: 0.0000e+00 - val_loss: 17.5197 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6122 - val_f1: 0.0000e+00Epoch 13/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 80.5819 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1421.0000 - fn: 79.0000 - accuracy: 0.9473 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6651 - f1: 0.0000e+00 - val_loss: 17.3611 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6430 - val_f1: 0.0000e+00Epoch 14/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 79.8909 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1417.0000 - fn: 83.0000 - accuracy: 0.9447 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6607 - f1: 0.0000e+00 - val_loss: 17.2039 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7783 - val_f1: 0.0000e+00Epoch 15/15\n",
      "1500/6300 [======>.......................] - ETA: 6s - loss: 79.1586 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1417.0000 - fn: 83.0000 - accuracy: 0.9447 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6574 - f1: 0.0000e+00 - val_loss: 17.0480 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 483.0000 - val_fn: 17.0000 - val_accuracy: 0.9660 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7695 - val_f1: 0.0000e+00\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.1e-05\n",
      "past history: 28\n",
      "LSTM units: 284\n",
      "hidden layers: 4\n",
      "hidden units: 13\n",
      "hidden l2 lambda: 4.9e-04\n",
      "class 0 weight: 0.3169229194234106\n",
      "class 1 weight: 17\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (100, 284)                378288    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 13)                 3705      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 13)                 182       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (100, 13)                 182       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (100, 13)                 182       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (100, 1)                  14        \n",
      "=================================================================\n",
      "Total params: 382,553\n",
      "Trainable params: 382,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 6300 samples, validate on 2300 samples\n",
      "Epoch 1/15\n",
      "1500/6300 [======>.......................] - ETA: 29s - loss: 2.0278 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1421.0000 - fn: 79.0000 - accuracy: 0.9473 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5767 - f1: 0.0000e+00 - val_loss: 0.2560 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5623 - val_f1: 0.0000e+00Epoch 2/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/6300 [======>.......................] - ETA: 13s - loss: 1.9552 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1423.0000 - fn: 77.0000 - accuracy: 0.9487 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5341 - f1: 0.0000e+00 - val_loss: 0.2542 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5808 - val_f1: 0.0000e+00Epoch 3/15\n",
      "1500/6300 [======>.......................] - ETA: 13s - loss: 1.9186 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1424.0000 - fn: 76.0000 - accuracy: 0.9493 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5930 - f1: 0.0000e+00 - val_loss: 0.2532 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5742 - val_f1: 0.0000e+00Epoch 4/15\n",
      "1500/6300 [======>.......................] - ETA: 13s - loss: 1.5477 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1439.0000 - fn: 61.0000 - accuracy: 0.9593 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5566 - f1: 0.0000e+00 - val_loss: 0.2520 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5330 - val_f1: 0.0000e+00Epoch 5/15\n",
      "1500/6300 [======>.......................] - ETA: 11s - loss: 2.0214 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1419.0000 - fn: 81.0000 - accuracy: 0.9460 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5621 - f1: 0.0000e+00 - val_loss: 0.2508 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6111 - val_f1: 0.0000e+00Epoch 6/15\n",
      "1500/6300 [======>.......................] - ETA: 11s - loss: 1.6524 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1434.0000 - fn: 66.0000 - accuracy: 0.9560 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6252 - f1: 0.0000e+00 - val_loss: 0.2498 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6303 - val_f1: 0.0000e+00Epoch 7/15\n",
      "1500/6300 [======>.......................] - ETA: 12s - loss: 1.8586 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1425.0000 - fn: 75.0000 - accuracy: 0.9500 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6329 - f1: 0.0000e+00 - val_loss: 0.2483 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6155 - val_f1: 0.0000e+00Epoch 8/15\n",
      "1500/6300 [======>.......................] - ETA: 14s - loss: 1.8225 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1426.0000 - fn: 74.0000 - accuracy: 0.9507 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5683 - f1: 0.0000e+00 - val_loss: 0.2467 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6014 - val_f1: 0.0000e+00Epoch 9/15\n",
      "1500/6300 [======>.......................] - ETA: 13s - loss: 1.8336 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1425.0000 - fn: 75.0000 - accuracy: 0.9500 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6895 - f1: 0.0000e+00 - val_loss: 0.2451 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6649 - val_f1: 0.0000e+00Epoch 10/15\n",
      "1500/6300 [======>.......................] - ETA: 13s - loss: 1.5426 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1437.0000 - fn: 63.0000 - accuracy: 0.9580 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6438 - f1: 0.0000e+00 - val_loss: 0.2438 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6340 - val_f1: 0.0000e+00Epoch 11/15\n",
      "1500/6300 [======>.......................] - ETA: 13s - loss: 1.7628 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1427.0000 - fn: 73.0000 - accuracy: 0.9513 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6067 - f1: 0.0000e+00 - val_loss: 0.2422 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5979 - val_f1: 0.0000e+00Epoch 12/15\n",
      "1500/6300 [======>.......................] - ETA: 13s - loss: 1.8660 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1422.0000 - fn: 78.0000 - accuracy: 0.9480 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6733 - f1: 0.0000e+00 - val_loss: 0.2404 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6610 - val_f1: 0.0000e+00Epoch 13/15\n",
      "1500/6300 [======>.......................] - ETA: 13s - loss: 1.6467 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1431.0000 - fn: 69.0000 - accuracy: 0.9540 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6321 - f1: 0.0000e+00 - val_loss: 0.2388 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6357 - val_f1: 0.0000e+00Epoch 14/15\n",
      "1500/6300 [======>.......................] - ETA: 13s - loss: 1.8818 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 1420.0000 - fn: 80.0000 - accuracy: 0.9467 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6831 - f1: 0.0000e+00 - val_loss: 0.2370 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 485.0000 - val_fn: 15.0000 - val_accuracy: 0.9700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6608 - val_f1: 0.0000e+00Epoch 15/15\n",
      " 300/6300 [>.............................] - ETA: 14s - loss: 1.7620 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 285.0000 - fn: 15.0000 - accuracy: 0.9500 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6754 - f1: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(\n",
    "    func=fitness,\n",
    "    dimensions=dimensions,\n",
    "    acq_func='EI', # Expected Improvement.\n",
    "    n_calls=40,\n",
    "    x0=default_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_names = [\n",
    "    'learning_rate',\n",
    "    'past_history',\n",
    "    'lstm_units',\n",
    "    'hidden_layers',\n",
    "    'hidden_units',\n",
    "    #'lstm_l2_lambda',\n",
    "    'hidden_l2_lambda',\n",
    "    'class_0_weight',\n",
    "    'class_1_weight'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_objective(result=search_result, dimension_names=dim_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try a longer training run with the winning hyperparameters** Some of these, we will manually tweak based on the above graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = search_result.space\n",
    "winning_hyperparams = space.point_to_dict(search_result.x)\n",
    "winning_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = winning_hyperparams['learning_rate']\n",
    "learning_rate = 0.001\n",
    "past_history = winning_hyperparams['past_history']\n",
    "lstm_units = winning_hyperparams['lstm_units']\n",
    "hidden_layers = winning_hyperparams['hidden_layers']\n",
    "hidden_units = winning_hyperparams['hidden_units']\n",
    "#lstm_l2_lambda = winning_hyperparams['lstm_l2_lambda']\n",
    "hidden_l2_lambda = winning_hyperparams['hidden_l2_lambda']\n",
    "#hidden_l2_lambda = 0.005\n",
    "class_0_weight = winning_hyperparams['class_0_weight']\n",
    "class_1_weight = winning_hyperparams['class_1_weight']\n",
    "\n",
    "future_target = 1\n",
    "step = 1\n",
    "\n",
    "initial_bias = -1.4\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "    \n",
    "class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 100\n",
    "STEPS_PER_EPOCH = (len(training_data) * 0.9) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(validation_data) * 0.9) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = multivariate_data(\n",
    "    training_data, \n",
    "    training_data[:, 1], \n",
    "    0,\n",
    "    None,\n",
    "    past_history,\n",
    "    future_target, \n",
    "    step\n",
    ")\n",
    "\n",
    "start_index = (x_train.shape[0] - (x_train.shape[0] % BATCH_SIZE))\n",
    "end_index = x_train.shape[0]\n",
    "x_train = np.delete(x_train, range(start_index, end_index), axis=0)\n",
    "y_train = np.delete(y_train, range(start_index, end_index), axis=0)\n",
    "\n",
    "x_validation, y_validation = multivariate_data(\n",
    "    validation_data, \n",
    "    validation_data[:, 1], \n",
    "    0,\n",
    "    None,\n",
    "    past_history,\n",
    "    future_target, \n",
    "    step\n",
    ")\n",
    "\n",
    "start_index = (x_validation.shape[0] - (x_validation.shape[0] % BATCH_SIZE))\n",
    "end_index = x_validation.shape[0]\n",
    "x_validation = np.delete(x_validation, range(start_index, end_index), axis=0)\n",
    "y_validation = np.delete(y_validation, range(start_index, end_index), axis=0)\n",
    "\n",
    "input_dim = x_train.shape[-2:]\n",
    "\n",
    "input_shape = (BATCH_SIZE, input_dim[0], input_dim[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(\n",
    "    lstm_units,\n",
    "    batch_input_shape=input_shape,\n",
    "#         bias_initializer=keras.initializers.VarianceScaling(\n",
    "#             scale=1.0,\n",
    "#             mode='fan_in', \n",
    "#             distribution='normal', \n",
    "#             seed=None\n",
    "#         ),\n",
    "#         kernel_regularizer=keras.regularizers.l2(lstm_l2_lambda),\n",
    "#         activation = 'relu',\n",
    "     stateful = True\n",
    "))\n",
    "for i in range(hidden_layers):\n",
    "    model.add(keras.layers.Dense(\n",
    "        hidden_units,\n",
    "        bias_initializer=keras.initializers.VarianceScaling(\n",
    "            scale=1.0,\n",
    "            mode='fan_in', \n",
    "            distribution='normal', \n",
    "            seed=None\n",
    "        ),\n",
    "        kernel_regularizer=keras.regularizers.l2(hidden_l2_lambda),\n",
    "        activation = 'relu'\n",
    "    ))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    1,\n",
    "    activation = 'sigmoid',\n",
    "    bias_initializer = output_bias)\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr = learning_rate), \n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(x_validation, y_validation),\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    class_weight=class_weight,\n",
    "    workers=8\n",
    ")\n",
    "\n",
    "model.save('../trained_models/best_LSTM.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplots(2,2,figsize=(12,8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(y_train)), \n",
    "    y_train,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(y_train)), \n",
    "    predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(155,176)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(y_train)), \n",
    "    y_train,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(y_train)), \n",
    "    predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(175,195)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(y_train)), \n",
    "    y_train,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(y_train)), \n",
    "    predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "plt.xlim(195,215)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "\n",
    "plt.plot(\n",
    "    range(len(y_train)), \n",
    "    y_train,\n",
    "    color = \"darkred\",\n",
    "    label ='True ignitions'\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(y_train)), \n",
    "    predictions,\n",
    "    color = \"darkgray\",\n",
    "    label ='predicted ignitions'\n",
    ")\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Ignition')\n",
    "plt.title('Predicted vs. actual ignition')\n",
    "plt.legend()\n",
    "#plt.xlim(195,215)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
