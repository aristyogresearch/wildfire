{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "#from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_objective_2D #, plot_histogram\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dir_name(\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda,\n",
    "    class_0_weight,\n",
    "    class_1_weight\n",
    "):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./MLP_logs/hidden_layers{0}_neurons_per_layer{1}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(\n",
    "        learning_rate,\n",
    "        hidden_layers,\n",
    "        neurons_per_layer,\n",
    "        dropout_rate,\n",
    "        l2_lambda,\n",
    "        class_0_weight,\n",
    "        class_1_weight\n",
    "    )\n",
    "\n",
    "    return log_dir\n",
    "\n",
    "\n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'f1']\n",
    "    \n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(3,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        \n",
    "    if metric == 'loss':\n",
    "        plt.ylim([0, plt.ylim()[1]])\n",
    "        \n",
    "    elif metric == 'auc':\n",
    "        plt.ylim([0.8,1])\n",
    "        \n",
    "    else:\n",
    "        #plt.ylim([0,1])\n",
    "        plt.legend()\n",
    "        \n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    normalized_cm = np.empty([2, 2])\n",
    "    normalized_cm[0][0] = cm[0][0] / (cm[0][0] + cm[0][1])\n",
    "    normalized_cm[0][1] = cm[0][1] / (cm[0][0] + cm[0][1])\n",
    "    normalized_cm[1][0] = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "    normalized_cm[1][1] = cm[1][1] / (cm[1][0] + cm[1][1])\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(normalized_cm, annot=True)\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('No fire (True Negatives): ', cm[0][0])\n",
    "    print('False alarms (False Positives): ', cm[0][1])\n",
    "    print('Fires missed (False Negatives): ', cm[1][0])\n",
    "    print('Fires detected (True Positives): ', cm[1][1])\n",
    "    print('Total fires: ', np.sum(cm[1]))\n",
    "    \n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    #plt.xlim([-0.5,20])\n",
    "    #plt.ylim([80,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "def f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/stratified_training_data/1992-2015_training_data_added_features_n500000_ks_pval0.3.1.csv'\n",
    "validation_file = '../data/stratified_training_data/1992-2015_training_data_added_features_n500000_ks_pval0.3.2.csv'\n",
    "test_file = '../data/stratified_training_data/1992-2015_training_data_added_features_n500000_ks_pval0.3.3.csv'\n",
    "\n",
    "# Datatypes for dataframe loading\n",
    "dtypes = {\n",
    "    'lat': float,\n",
    "    'lon': float,\n",
    "    'weather_bin_year': int,\n",
    "    'weather_bin_month': int,\n",
    "    'weather_bin_day': int,\n",
    "    'air.2m': float,\n",
    "    'apcp': float,\n",
    "    'rhum.2m': float,\n",
    "    'dpt.2m': float,\n",
    "    'pres.sfc': float,\n",
    "    'uwnd.10m': float,\n",
    "    'vwnd.10m': float,\n",
    "    'veg': float,\n",
    "    'vis': float,\n",
    "    'ignition': float,\n",
    "    'mean.air.2m': float,\n",
    "    'mean.apcp': float,\n",
    "    'mean.rhum.2m': float,\n",
    "    'mean.dpt.2m': float,\n",
    "    'mean.pres.sfc': float,\n",
    "    'mean.uwnd.10m': float,\n",
    "    'mean.vwnd.10m': float,\n",
    "    'mean.veg': float,\n",
    "    'mean.vis': float,\n",
    "    'max.air.2m': float,\n",
    "    'max.apcp': float,\n",
    "    'max.rhum.2m': float,\n",
    "    'max.dpt.2m': float,\n",
    "    'max.pres.sfc': float,\n",
    "    'max.uwnd.10m': float,\n",
    "    'max.vwnd.10m': float,\n",
    "    'max.veg': float,\n",
    "    'max.vis': float,\n",
    "    'min.air.2m': float,\n",
    "    'min.apcp': float,\n",
    "    'min.rhum.2m': float,\n",
    "    'min.dpt.2m': float,\n",
    "    'min.pres.sfc': float,\n",
    "    'min.uwnd.10m': float,\n",
    "    'min.vwnd.10m': float,\n",
    "    'min.veg': float,\n",
    "    'min.vis': float,\n",
    "    'total_fires': float\n",
    "\n",
    "}\n",
    "\n",
    "# Features to use during training \n",
    "features = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'weather_bin_month',\n",
    "    'veg',\n",
    "    'ignition',\n",
    "    'mean.air.2m',\n",
    "    'mean.apcp',\n",
    "    'mean.rhum.2m',\n",
    "    'mean.dpt.2m',\n",
    "    'mean.pres.sfc',\n",
    "    'mean.uwnd.10m',\n",
    "    'mean.vwnd.10m',\n",
    "    'mean.vis',\n",
    "    'max.air.2m',\n",
    "    'max.apcp',\n",
    "    'max.rhum.2m',\n",
    "    'max.dpt.2m',\n",
    "    'max.pres.sfc',\n",
    "    'max.uwnd.10m',\n",
    "    'max.vwnd.10m',\n",
    "    'max.vis',\n",
    "    'min.air.2m',\n",
    "    'min.apcp',\n",
    "    'min.rhum.2m',\n",
    "    'min.dpt.2m',\n",
    "    'min.pres.sfc',\n",
    "    'min.uwnd.10m',\n",
    "    'min.vwnd.10m',\n",
    "    'min.vis',\n",
    "    'total_fires'\n",
    "]\n",
    "\n",
    "features_to_scale = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'veg',\n",
    "    'mean.air.2m',\n",
    "    'mean.apcp',\n",
    "    'mean.rhum.2m',\n",
    "    'mean.dpt.2m',\n",
    "    'mean.pres.sfc',\n",
    "    'mean.uwnd.10m',\n",
    "    'mean.vwnd.10m',\n",
    "    'mean.vis',\n",
    "    'max.air.2m',\n",
    "    'max.apcp',\n",
    "    'max.rhum.2m',\n",
    "    'max.dpt.2m',\n",
    "    'max.pres.sfc',\n",
    "    'max.uwnd.10m',\n",
    "    'max.vwnd.10m',\n",
    "    'max.vis',\n",
    "    'min.air.2m',\n",
    "    'min.apcp',\n",
    "    'min.rhum.2m',\n",
    "    'min.dpt.2m',\n",
    "    'min.pres.sfc',\n",
    "    'min.uwnd.10m',\n",
    "    'min.vwnd.10m',\n",
    "    'min.vis',\n",
    "    'total_fires'\n",
    "]\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'), \n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    f1\n",
    "]\n",
    "\n",
    "dim_learning_rate = Real(\n",
    "    low=1e-5, \n",
    "    high=1e-1, \n",
    "    prior='log-uniform',\n",
    "    name='learning_rate'\n",
    ")\n",
    "\n",
    "dim_hidden_layers = Integer(\n",
    "    low=5,\n",
    "    high=15, \n",
    "    name='hidden_layers'\n",
    ")\n",
    "\n",
    "dim_neurons_per_layer = Integer(\n",
    "    low=10, \n",
    "    high=100, \n",
    "    name='neurons_per_layer'\n",
    ")\n",
    "\n",
    "dim_dropout_rate = Real(\n",
    "    low=0.1, \n",
    "    high=0.5, \n",
    "    name='dropout_rate'\n",
    ")\n",
    "\n",
    "dim_l2_lambda = Real(\n",
    "    low=0.001, \n",
    "    high=0.1, \n",
    "    name='l2_lambda'\n",
    ")\n",
    "\n",
    "dim_class_0_weight = Real(\n",
    "    low=0.1,\n",
    "    high=5,\n",
    "    name='class_0_weight'\n",
    ")\n",
    "\n",
    "dim_class_1_weight = Real(\n",
    "    low=10,\n",
    "    high=15,\n",
    "    name=\"class_1_weight\"\n",
    ")\n",
    "\n",
    "dimensions = [\n",
    "    dim_learning_rate,\n",
    "    dim_hidden_layers,\n",
    "    dim_neurons_per_layer,\n",
    "    dim_dropout_rate,\n",
    "    dim_l2_lambda,\n",
    "    dim_class_0_weight,\n",
    "    dim_class_1_weight\n",
    "]\n",
    "\n",
    "default_parameters = [0.001, 10, 50, 0.25, 0.05, 0.5, 12]\n",
    "\n",
    "path_best_model = '../trained_models/best_skopt_MLP.keras'\n",
    "best_fraction_incorrect = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file, dtype=dtypes)\n",
    "validation = pd.read_csv(validation_file, dtype=dtypes)\n",
    "test = pd.read_csv(test_file, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "training_data = data[features]\n",
    "validation_data = validation[features]\n",
    "test_data = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# One hot encode month\n",
    "\n",
    "column_names = [\n",
    "    'January',\n",
    "    'February',\n",
    "    'March',\n",
    "    'April',\n",
    "    'May',\n",
    "    'June',\n",
    "    'July',\n",
    "    'August',\n",
    "    'Septermber',\n",
    "    'October',\n",
    "    'November',\n",
    "    'December'\n",
    "]\n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Training data\n",
    "month = np.array(training_data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "training_data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "training_data = pd.concat([training_data, onehot_month_df], axis=1)\n",
    "\n",
    "# Validation data\n",
    "month = np.array(validation_data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "validation_data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "validation_data = pd.concat([validation_data, onehot_month_df], axis=1)\n",
    "\n",
    "# Test data data\n",
    "month = np.array(test_data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "test_data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "test_data = pd.concat([test_data, onehot_month_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>veg</th>\n",
       "      <th>ignition</th>\n",
       "      <th>mean.air.2m</th>\n",
       "      <th>mean.apcp</th>\n",
       "      <th>mean.rhum.2m</th>\n",
       "      <th>mean.dpt.2m</th>\n",
       "      <th>mean.pres.sfc</th>\n",
       "      <th>mean.uwnd.10m</th>\n",
       "      <th>...</th>\n",
       "      <th>March</th>\n",
       "      <th>April</th>\n",
       "      <th>May</th>\n",
       "      <th>June</th>\n",
       "      <th>July</th>\n",
       "      <th>August</th>\n",
       "      <th>Septermber</th>\n",
       "      <th>October</th>\n",
       "      <th>November</th>\n",
       "      <th>December</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>34.18678</td>\n",
       "      <td>-118.8088</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.170681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.576653</td>\n",
       "      <td>279.735122</td>\n",
       "      <td>96650.870000</td>\n",
       "      <td>0.064065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37.03086</td>\n",
       "      <td>-119.0176</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.386004</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>33.640771</td>\n",
       "      <td>277.393281</td>\n",
       "      <td>79824.146797</td>\n",
       "      <td>-0.407725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37.49919</td>\n",
       "      <td>-119.8433</td>\n",
       "      <td>33.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.263178</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>60.458148</td>\n",
       "      <td>279.509951</td>\n",
       "      <td>89032.984219</td>\n",
       "      <td>-0.223786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>36.23206</td>\n",
       "      <td>-118.5007</td>\n",
       "      <td>46.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.102028</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>45.363947</td>\n",
       "      <td>256.344916</td>\n",
       "      <td>76906.623375</td>\n",
       "      <td>0.438386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>32.91990</td>\n",
       "      <td>-114.8649</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.117580</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>26.899587</td>\n",
       "      <td>269.320998</td>\n",
       "      <td>99661.673406</td>\n",
       "      <td>1.417608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       lon   veg  ignition  mean.air.2m  mean.apcp  mean.rhum.2m  \\\n",
       "0  34.18678 -118.8088  25.5       0.0   289.170681   0.000000     57.576653   \n",
       "1  37.03086 -119.0176  43.3       0.0   294.386004   0.000122     33.640771   \n",
       "2  37.49919 -119.8433  33.9       0.0   287.263178   0.139648     60.458148   \n",
       "3  36.23206 -118.5007  46.9       0.0   267.102028   0.000318     45.363947   \n",
       "4  32.91990 -114.8649   3.5       0.0   289.117580   0.000506     26.899587   \n",
       "\n",
       "   mean.dpt.2m  mean.pres.sfc  mean.uwnd.10m  ...  March  April  May  June  \\\n",
       "0   279.735122   96650.870000       0.064065  ...    0.0    0.0  0.0   0.0   \n",
       "1   277.393281   79824.146797      -0.407725  ...    0.0    0.0  0.0   0.0   \n",
       "2   279.509951   89032.984219      -0.223786  ...    0.0    1.0  0.0   0.0   \n",
       "3   256.344916   76906.623375       0.438386  ...    0.0    0.0  0.0   0.0   \n",
       "4   269.320998   99661.673406       1.417608  ...    0.0    0.0  0.0   0.0   \n",
       "\n",
       "   July  August  Septermber  October  November  December  \n",
       "0   0.0     0.0         0.0      0.0       0.0       0.0  \n",
       "1   0.0     1.0         0.0      0.0       0.0       0.0  \n",
       "2   0.0     0.0         0.0      0.0       0.0       0.0  \n",
       "3   0.0     0.0         0.0      0.0       0.0       1.0  \n",
       "4   0.0     0.0         0.0      0.0       1.0       0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 449258 entries, 0 to 449257\n",
      "Data columns (total 41 columns):\n",
      "lat              449258 non-null float64\n",
      "lon              449258 non-null float64\n",
      "veg              449258 non-null float64\n",
      "ignition         449258 non-null float64\n",
      "mean.air.2m      449258 non-null float64\n",
      "mean.apcp        449258 non-null float64\n",
      "mean.rhum.2m     449258 non-null float64\n",
      "mean.dpt.2m      449258 non-null float64\n",
      "mean.pres.sfc    449258 non-null float64\n",
      "mean.uwnd.10m    449258 non-null float64\n",
      "mean.vwnd.10m    449258 non-null float64\n",
      "mean.vis         449258 non-null float64\n",
      "max.air.2m       449258 non-null float64\n",
      "max.apcp         449258 non-null float64\n",
      "max.rhum.2m      449258 non-null float64\n",
      "max.dpt.2m       449258 non-null float64\n",
      "max.pres.sfc     449258 non-null float64\n",
      "max.uwnd.10m     449258 non-null float64\n",
      "max.vwnd.10m     449258 non-null float64\n",
      "max.vis          449258 non-null float64\n",
      "min.air.2m       449258 non-null float64\n",
      "min.apcp         449258 non-null float64\n",
      "min.rhum.2m      449258 non-null float64\n",
      "min.dpt.2m       449258 non-null float64\n",
      "min.pres.sfc     449258 non-null float64\n",
      "min.uwnd.10m     449258 non-null float64\n",
      "min.vwnd.10m     449258 non-null float64\n",
      "min.vis          449258 non-null float64\n",
      "total_fires      449258 non-null float64\n",
      "January          449258 non-null float64\n",
      "February         449258 non-null float64\n",
      "March            449258 non-null float64\n",
      "April            449258 non-null float64\n",
      "May              449258 non-null float64\n",
      "June             449258 non-null float64\n",
      "July             449258 non-null float64\n",
      "August           449258 non-null float64\n",
      "Septermber       449258 non-null float64\n",
      "October          449258 non-null float64\n",
      "November         449258 non-null float64\n",
      "December         449258 non-null float64\n",
      "dtypes: float64(41)\n",
      "memory usage: 140.5 MB\n"
     ]
    }
   ],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(training_data.pop('ignition'))\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(validation_data.pop('ignition'))\n",
    "test_labels = np.array(test_data.pop('ignition'))\n",
    "\n",
    "train_features = np.array(training_data)\n",
    "val_features = np.array(validation_data)\n",
    "test_features = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (449258,)\n",
      "Validation labels shape: (449257,)\n",
      "Test labels shape: (449258,)\n",
      "Training features shape: (449258, 40)\n",
      "Validation features shape: (449257, 40)\n",
      "Test features shape: (449258, 40)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "# weight_for_0 = (1 / no_ignition_count)*(total)/2.0 \n",
    "# weight_for_1 = (1 / ignition_count)*(total)/2.0\n",
    "\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "# print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "# print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "\n",
    "ignition_count = sum(train_labels)\n",
    "no_ignition_count = len(train_labels) - ignition_count\n",
    "\n",
    "initial_bias = np.log([ignition_count/no_ignition_count])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1000\n",
    "STEPS_PER_EPOCH = (len(train_features) * 0.2) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(val_features) * 0.2) // BATCH_SIZE\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    output_bias,\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda\n",
    "):    \n",
    "    # Define the keras model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Add input layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        neurons_per_layer, \n",
    "        activation = 'relu', \n",
    "        input_dim = train_features.shape[-1],\n",
    "    ))\n",
    "\n",
    "    # Add fully connected hidden layers\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(keras.layers.Dense(\n",
    "            neurons_per_layer,\n",
    "            bias_initializer=keras.initializers.VarianceScaling(\n",
    "                scale=1.0,\n",
    "                mode='fan_in', \n",
    "                distribution='normal', \n",
    "                seed=None\n",
    "            ),\n",
    "            kernel_regularizer=keras.regularizers.l2(l2_lambda),\n",
    "            activation = 'relu')\n",
    "        )\n",
    "    \n",
    "    # Add dropout layer\n",
    "    model.add(keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        1, \n",
    "        activation = 'sigmoid', \n",
    "        bias_initializer = output_bias\n",
    "    ))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda,\n",
    "    class_0_weight,\n",
    "    class_1_weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "    \n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('hidden layers:', hidden_layers)\n",
    "    print('neurons per layer:', neurons_per_layer)\n",
    "    print('dropout rate: {}'.format(np.round(dropout_rate,2)))\n",
    "    print('l2 lambda: {0:.1e}'.format(l2_lambda))\n",
    "    print('class weight: {}, {}'.format(np.round(class_weight[0],1), np.round(class_weight[1],2)))\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = make_model(\n",
    "        output_bias,\n",
    "        learning_rate = learning_rate,\n",
    "        hidden_layers = hidden_layers,\n",
    "        neurons_per_layer = neurons_per_layer,\n",
    "        dropout_rate = dropout_rate,\n",
    "        l2_lambda = l2_lambda\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    print()\n",
    "\n",
    "    # Dir-name for the TensorBoard log-files.\n",
    "    log_dir = log_dir_name(\n",
    "        learning_rate,\n",
    "        hidden_layers,\n",
    "        neurons_per_layer,\n",
    "        dropout_rate,\n",
    "        l2_lambda,\n",
    "        class_0_weight,\n",
    "        class_1_weight\n",
    "    )\n",
    "    \n",
    "    # Create a callback-function for Keras which will be\n",
    "    # run after each epoch has ended during training.\n",
    "    # This saves the log-files for TensorBoard.\n",
    "    # Note that there are complications when histogram_freq=1.\n",
    "    # It might give strange errors and it also does not properly\n",
    "    # support Keras data-generators for the validation-set.\n",
    "    callback_log = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False\n",
    "    )\n",
    "   \n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        callbacks = [early_stopping],\n",
    "        validation_data=(val_features, val_labels),\n",
    "        validation_steps=VALIDATION_STEPS,\n",
    "        class_weight=class_weight,\n",
    "        workers=8\n",
    "    )\n",
    "\n",
    "    # Get fraction incorrect on the validation-set\n",
    "    # after the last training-epoch.\n",
    "          \n",
    "    val_fp = history.history['val_fp'][-1]\n",
    "    val_fn = history.history['val_fn'][-1]\n",
    "    val_tp = history.history['val_tp'][-1]\n",
    "    val_tn = history.history['val_tn'][-1]\n",
    "          \n",
    "    fraction_incorrect = (val_fn /(val_fn + val_tp + K.epsilon())) + (val_fp / (val_fp + val_tn + K.epsilon()))\n",
    "    \n",
    "    print()\n",
    "    print(\"Validation fraction incorrect: {0:.2}\".format(fraction_incorrect))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_fraction_incorrect\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if fraction_incorrect < best_fraction_incorrect:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_fraction_incorrect = fraction_incorrect\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return fraction_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-03\n",
      "hidden layers: 10\n",
      "neurons per layer: 50\n",
      "dropout rate: 0.25\n",
      "l2 lambda: 5.0e-02\n",
      "class weight: 0.5, 12\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                2050      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 27,601\n",
      "Trainable params: 27,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 89000/449258 [====>.........................] - ETA: 24s - loss: 16.0355 - tp: 431.0000 - fp: 9556.0000 - tn: 76206.0000 - fn: 2807.0000 - accuracy: 0.8611 - precision: 0.0432 - recall: 0.1331 - auc: 0.5115 - f1: 0.0335 - val_loss: 1.6826 - val_tp: 129.0000 - val_fp: 373.0000 - val_tn: 85350.0000 - val_fn: 3148.0000 - val_accuracy: 0.9604 - val_precision: 0.2570 - val_recall: 0.0394 - val_auc: 0.7537 - val_f1: 0.0657Epoch 2/20\n",
      " 89000/449258 [====>.........................] - ETA: 7s - loss: 5.1593 - tp: 937.0000 - fp: 21584.0000 - tn: 64116.0000 - fn: 2363.0000 - accuracy: 0.7309 - precision: 0.0416 - recall: 0.2839 - auc: 0.5293 - f1: 0.0725 - val_loss: 0.5556 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6026 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 89000/449258 [====>.........................] - ETA: 7s - loss: 1.8262 - tp: 788.0000 - fp: 20637.0000 - tn: 65121.0000 - fn: 2454.0000 - accuracy: 0.7406 - precision: 0.0368 - recall: 0.2431 - auc: 0.4994 - f1: 0.0643 - val_loss: 0.2290 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 89000/449258 [====>.........................] - ETA: 7s - loss: 0.9333 - tp: 974.0000 - fp: 24941.0000 - tn: 60746.0000 - fn: 2339.0000 - accuracy: 0.6935 - precision: 0.0376 - recall: 0.2940 - auc: 0.4982 - f1: 0.0669 - val_loss: 0.1502 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 89000/449258 [====>.........................] - ETA: 7s - loss: 0.7181 - tp: 971.0000 - fp: 25671.0000 - tn: 60075.0000 - fn: 2283.0000 - accuracy: 0.6859 - precision: 0.0364 - recall: 0.2984 - auc: 0.5047 - f1: 0.0651 - val_loss: 0.1333 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 88258/449258 [====>.........................] - ETA: 8s - loss: 0.6763 - tp: 952.0000 - fp: 25476.0000 - tn: 59568.0000 - fn: 2262.0000 - accuracy: 0.6857 - precision: 0.0360 - recall: 0.2962 - auc: 0.4925 - f1: 0.0633 - val_loss: 0.1293 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 89000/449258 [====>.........................] - ETA: 8s - loss: 0.6692 - tp: 1088.0000 - fp: 28424.0000 - tn: 57293.0000 - fn: 2195.0000 - accuracy: 0.6560 - precision: 0.0369 - recall: 0.3314 - auc: 0.4914 - f1: 0.0661 - val_loss: 0.1280 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 89000/449258 [====>.........................] - ETA: 8s - loss: 0.6572 - tp: 991.0000 - fp: 28315.0000 - tn: 57460.0000 - fn: 2234.0000 - accuracy: 0.6568 - precision: 0.0338 - recall: 0.3073 - auc: 0.4925 - f1: 0.0601 - val_loss: 0.1278 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 89000/449258 [====>.........................] - ETA: 8s - loss: 0.6558 - tp: 1108.0000 - fp: 29202.0000 - tn: 56540.0000 - fn: 2150.0000 - accuracy: 0.6477 - precision: 0.0366 - recall: 0.3401 - auc: 0.5000 - f1: 0.0658 - val_loss: 0.1273 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 89000/449258 [====>.........................] - ETA: 8s - loss: 0.6649 - tp: 1217.0000 - fp: 31607.0000 - tn: 54054.0000 - fn: 2122.0000 - accuracy: 0.6210 - precision: 0.0371 - recall: 0.3645 - auc: 0.4925 - f1: 0.0671 - val_loss: 0.1270 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 86258/449258 [====>.........................] - ETA: 6s - loss: 0.6650 - tp: 1244.0000 - fp: 32617.0000 - tn: 50387.0000 - fn: 2010.0000 - accuracy: 0.5986 - precision: 0.0367 - recall: 0.3823 - auc: 0.4965 - f1: 0.0668Restoring model weights from the end of the best epoch.\n",
      " 88258/449258 [====>.........................] - ETA: 8s - loss: 0.6652 - tp: 1268.0000 - fp: 33136.0000 - tn: 51787.0000 - fn: 2067.0000 - accuracy: 0.6011 - precision: 0.0369 - recall: 0.3802 - auc: 0.4974 - f1: 0.0670 - val_loss: 0.1272 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.5e-02\n",
      "hidden layers: 7\n",
      "neurons per layer: 80\n",
      "dropout rate: 0.34\n",
      "l2 lambda: 4.5e-02\n",
      "class weight: 0.6, 12.3\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 80)                3280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 48,721\n",
      "Trainable params: 48,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 89000/449258 [====>.........................] - ETA: 25s - loss: 2.1131 - tp: 1659.0000 - fp: 14286.0000 - tn: 71476.0000 - fn: 1579.0000 - accuracy: 0.8217 - precision: 0.1040 - recall: 0.5124 - auc: 0.7454 - f1: 0.1765 - val_loss: 0.1298 - val_tp: 2113.0000 - val_fp: 11824.0000 - val_tn: 73899.0000 - val_fn: 1164.0000 - val_accuracy: 0.8541 - val_precision: 0.1516 - val_recall: 0.6448 - val_auc: 0.8522 - val_f1: 0.2444Epoch 2/20\n",
      " 89000/449258 [====>.........................] - ETA: 14s - loss: 0.6683 - tp: 2142.0000 - fp: 14238.0000 - tn: 71462.0000 - fn: 1158.0000 - accuracy: 0.8270 - precision: 0.1308 - recall: 0.6491 - auc: 0.8300 - f1: 0.2217 - val_loss: 0.1228 - val_tp: 2306.0000 - val_fp: 13837.0000 - val_tn: 71886.0000 - val_fn: 971.0000 - val_accuracy: 0.8336 - val_precision: 0.1428 - val_recall: 0.7037 - val_auc: 0.8598 - val_f1: 0.2366Epoch 3/20\n",
      " 89000/449258 [====>.........................] - ETA: 14s - loss: 0.6385 - tp: 2144.0000 - fp: 14416.0000 - tn: 71342.0000 - fn: 1098.0000 - accuracy: 0.8257 - precision: 0.1295 - recall: 0.6613 - auc: 0.8369 - f1: 0.2222 - val_loss: 0.1194 - val_tp: 2527.0000 - val_fp: 16881.0000 - val_tn: 68842.0000 - val_fn: 750.0000 - val_accuracy: 0.8019 - val_precision: 0.1302 - val_recall: 0.7711 - val_auc: 0.8617 - val_f1: 0.2223Epoch 4/20\n",
      " 89000/449258 [====>.........................] - ETA: 14s - loss: 0.6286 - tp: 2284.0000 - fp: 15125.0000 - tn: 70562.0000 - fn: 1029.0000 - accuracy: 0.8185 - precision: 0.1312 - recall: 0.6894 - auc: 0.8402 - f1: 0.2290 - val_loss: 0.1187 - val_tp: 2664.0000 - val_fp: 19685.0000 - val_tn: 66038.0000 - val_fn: 613.0000 - val_accuracy: 0.7719 - val_precision: 0.1192 - val_recall: 0.8129 - val_auc: 0.8654 - val_f1: 0.2073Epoch 5/20\n",
      " 89000/449258 [====>.........................] - ETA: 14s - loss: 0.6148 - tp: 2263.0000 - fp: 14870.0000 - tn: 70876.0000 - fn: 991.0000 - accuracy: 0.8218 - precision: 0.1321 - recall: 0.6955 - auc: 0.8456 - f1: 0.2291 - val_loss: 0.1156 - val_tp: 2460.0000 - val_fp: 15528.0000 - val_tn: 70195.0000 - val_fn: 817.0000 - val_accuracy: 0.8163 - val_precision: 0.1368 - val_recall: 0.7507 - val_auc: 0.8652 - val_f1: 0.2307Epoch 6/20\n",
      " 88258/449258 [====>.........................] - ETA: 14s - loss: 0.6028 - tp: 2268.0000 - fp: 15153.0000 - tn: 69891.0000 - fn: 946.0000 - accuracy: 0.8176 - precision: 0.1302 - recall: 0.7057 - auc: 0.8462 - f1: 0.2226 - val_loss: 0.1160 - val_tp: 2648.0000 - val_fp: 19215.0000 - val_tn: 66508.0000 - val_fn: 629.0000 - val_accuracy: 0.7770 - val_precision: 0.1211 - val_recall: 0.8081 - val_auc: 0.8682 - val_f1: 0.2101Epoch 7/20\n",
      " 89000/449258 [====>.........................] - ETA: 8s - loss: 0.6023 - tp: 2367.0000 - fp: 15872.0000 - tn: 69845.0000 - fn: 916.0000 - accuracy: 0.8114 - precision: 0.1298 - recall: 0.7210 - auc: 0.8496 - f1: 0.2263 - val_loss: 0.1144 - val_tp: 2753.0000 - val_fp: 21060.0000 - val_tn: 64663.0000 - val_fn: 524.0000 - val_accuracy: 0.7575 - val_precision: 0.1156 - val_recall: 0.8401 - val_auc: 0.8700 - val_f1: 0.2027Epoch 8/20\n",
      " 89000/449258 [====>.........................] - ETA: 8s - loss: 0.5992 - tp: 2308.0000 - fp: 15743.0000 - tn: 70032.0000 - fn: 917.0000 - accuracy: 0.8128 - precision: 0.1279 - recall: 0.7157 - auc: 0.8473 - f1: 0.2257 - val_loss: 0.1146 - val_tp: 2183.0000 - val_fp: 11337.0000 - val_tn: 74386.0000 - val_fn: 1094.0000 - val_accuracy: 0.8603 - val_precision: 0.1615 - val_recall: 0.6662 - val_auc: 0.8683 - val_f1: 0.2589Epoch 9/20\n",
      " 89000/449258 [====>.........................] - ETA: 9s - loss: 0.5905 - tp: 2351.0000 - fp: 15854.0000 - tn: 69888.0000 - fn: 907.0000 - accuracy: 0.8117 - precision: 0.1291 - recall: 0.7216 - auc: 0.8498 - f1: 0.2253 - val_loss: 0.1118 - val_tp: 2701.0000 - val_fp: 19778.0000 - val_tn: 65945.0000 - val_fn: 576.0000 - val_accuracy: 0.7713 - val_precision: 0.1202 - val_recall: 0.8242 - val_auc: 0.8708 - val_f1: 0.2091Epoch 10/20\n",
      " 89000/449258 [====>.........................] - ETA: 9s - loss: 0.5815 - tp: 2461.0000 - fp: 16023.0000 - tn: 69638.0000 - fn: 878.0000 - accuracy: 0.8101 - precision: 0.1331 - recall: 0.7370 - auc: 0.8567 - f1: 0.2323 - val_loss: 0.1106 - val_tp: 2654.0000 - val_fp: 18503.0000 - val_tn: 67220.0000 - val_fn: 623.0000 - val_accuracy: 0.7851 - val_precision: 0.1254 - val_recall: 0.8099 - val_auc: 0.8710 - val_f1: 0.2165Epoch 11/20\n",
      " 88258/449258 [====>.........................] - ETA: 11s - loss: 0.5888 - tp: 2499.0000 - fp: 16672.0000 - tn: 68251.0000 - fn: 836.0000 - accuracy: 0.8016 - precision: 0.1304 - recall: 0.7493 - auc: 0.8538 - f1: 0.2269 - val_loss: 0.1102 - val_tp: 2482.0000 - val_fp: 15529.0000 - val_tn: 70194.0000 - val_fn: 795.0000 - val_accuracy: 0.8166 - val_precision: 0.1378 - val_recall: 0.7574 - val_auc: 0.8717 - val_f1: 0.2324Epoch 12/20\n",
      " 89000/449258 [====>.........................] - ETA: 11s - loss: 0.5741 - tp: 2333.0000 - fp: 15791.0000 - tn: 70007.0000 - fn: 869.0000 - accuracy: 0.8128 - precision: 0.1287 - recall: 0.7286 - auc: 0.8529 - f1: 0.2242 - val_loss: 0.1142 - val_tp: 1962.0000 - val_fp: 8727.0000 - val_tn: 76996.0000 - val_fn: 1315.0000 - val_accuracy: 0.8872 - val_precision: 0.1836 - val_recall: 0.5987 - val_auc: 0.8713 - val_f1: 0.2795Epoch 13/20\n",
      " 89000/449258 [====>.........................] - ETA: 11s - loss: 0.5767 - tp: 2448.0000 - fp: 16192.0000 - tn: 69517.0000 - fn: 843.0000 - accuracy: 0.8086 - precision: 0.1313 - recall: 0.7438 - auc: 0.8539 - f1: 0.2307 - val_loss: 0.1132 - val_tp: 2024.0000 - val_fp: 9558.0000 - val_tn: 76165.0000 - val_fn: 1253.0000 - val_accuracy: 0.8785 - val_precision: 0.1748 - val_recall: 0.6176 - val_auc: 0.8712 - val_f1: 0.2711Epoch 14/20\n",
      " 89000/449258 [====>.........................] - ETA: 11s - loss: 0.5727 - tp: 2329.0000 - fp: 15679.0000 - tn: 70110.0000 - fn: 882.0000 - accuracy: 0.8139 - precision: 0.1293 - recall: 0.7253 - auc: 0.8523 - f1: 0.2249 - val_loss: 0.1135 - val_tp: 2616.0000 - val_fp: 17758.0000 - val_tn: 67965.0000 - val_fn: 661.0000 - val_accuracy: 0.7930 - val_precision: 0.1284 - val_recall: 0.7983 - val_auc: 0.8726 - val_f1: 0.2205Epoch 15/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 0.5719 - tp: 2407.0000 - fp: 15777.0000 - tn: 69949.0000 - fn: 867.0000 - accuracy: 0.8130 - precision: 0.1324 - recall: 0.7352 - auc: 0.8555 - f1: 0.2263 - val_loss: 0.1080 - val_tp: 2577.0000 - val_fp: 17091.0000 - val_tn: 68632.0000 - val_fn: 700.0000 - val_accuracy: 0.8001 - val_precision: 0.1310 - val_recall: 0.7864 - val_auc: 0.8734 - val_f1: 0.2239Epoch 16/20\n",
      " 88258/449258 [====>.........................] - ETA: 8s - loss: 0.5731 - tp: 2378.0000 - fp: 15734.0000 - tn: 69292.0000 - fn: 854.0000 - accuracy: 0.8121 - precision: 0.1313 - recall: 0.7358 - auc: 0.8525 - f1: 0.2314 - val_loss: 0.1096 - val_tp: 2764.0000 - val_fp: 20641.0000 - val_tn: 65082.0000 - val_fn: 513.0000 - val_accuracy: 0.7623 - val_precision: 0.1181 - val_recall: 0.8435 - val_auc: 0.8735 - val_f1: 0.2066Epoch 17/20\n",
      " 89000/449258 [====>.........................] - ETA: 8s - loss: 0.5581 - tp: 2480.0000 - fp: 16000.0000 - tn: 69719.0000 - fn: 801.0000 - accuracy: 0.8112 - precision: 0.1342 - recall: 0.7559 - auc: 0.8614 - f1: 0.2307 - val_loss: 0.1101 - val_tp: 2796.0000 - val_fp: 21726.0000 - val_tn: 63997.0000 - val_fn: 481.0000 - val_accuracy: 0.7505 - val_precision: 0.1140 - val_recall: 0.8532 - val_auc: 0.8741 - val_f1: 0.2006Epoch 18/20\n",
      " 89000/449258 [====>.........................] - ETA: 9s - loss: 0.5665 - tp: 2410.0000 - fp: 16479.0000 - tn: 69272.0000 - fn: 839.0000 - accuracy: 0.8054 - precision: 0.1276 - recall: 0.7418 - auc: 0.8552 - f1: 0.2263 - val_loss: 0.1108 - val_tp: 2052.0000 - val_fp: 9590.0000 - val_tn: 76133.0000 - val_fn: 1225.0000 - val_accuracy: 0.8785 - val_precision: 0.1763 - val_recall: 0.6262 - val_auc: 0.8739 - val_f1: 0.2738Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89000/449258 [====>.........................] - ETA: 9s - loss: 0.5684 - tp: 2463.0000 - fp: 16609.0000 - tn: 69094.0000 - fn: 834.0000 - accuracy: 0.8040 - precision: 0.1291 - recall: 0.7470 - auc: 0.8543 - f1: 0.2242 - val_loss: 0.1074 - val_tp: 2722.0000 - val_fp: 20025.0000 - val_tn: 65698.0000 - val_fn: 555.0000 - val_accuracy: 0.7688 - val_precision: 0.1197 - val_recall: 0.8306 - val_auc: 0.8749 - val_f1: 0.2086Epoch 20/20\n",
      " 89000/449258 [====>.........................] - ETA: 8s - loss: 0.5655 - tp: 2434.0000 - fp: 16605.0000 - tn: 69130.0000 - fn: 831.0000 - accuracy: 0.8041 - precision: 0.1278 - recall: 0.7455 - auc: 0.8541 - f1: 0.2241 - val_loss: 0.1089 - val_tp: 2212.0000 - val_fp: 11569.0000 - val_tn: 74154.0000 - val_fn: 1065.0000 - val_accuracy: 0.8580 - val_precision: 0.1605 - val_recall: 0.6750 - val_auc: 0.8736 - val_f1: 0.2581\n",
      "Validation fraction incorrect: 0.46\n",
      "\n",
      "learning rate: 2.2e-04\n",
      "hidden layers: 6\n",
      "neurons per layer: 69\n",
      "dropout rate: 0.12\n",
      "l2 lambda: 7.2e-02\n",
      "class weight: 4.7, 10.0\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 69)                2829      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 69)                4830      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 70        \n",
      "=================================================================\n",
      "Total params: 31,879\n",
      "Trainable params: 31,879\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 89000/449258 [====>.........................] - ETA: 23s - loss: 27.6060 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85762.0000 - fn: 3238.0000 - accuracy: 0.9636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5747 - f1: 0.0000e+00 - val_loss: 4.7581 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7463 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 21.0805 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85700.0000 - fn: 3300.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7193 - f1: 0.0000e+00 - val_loss: 3.6275 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7887 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 16.0468 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85758.0000 - fn: 3242.0000 - accuracy: 0.9636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7773 - f1: 0.0000e+00 - val_loss: 2.7611 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8038 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 12.2166 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85687.0000 - fn: 3313.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8079 - f1: 0.0000e+00 - val_loss: 2.0992 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8183 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 9.2802 - tp: 9.0000 - fp: 7.0000 - tn: 85739.0000 - fn: 3245.0000 - accuracy: 0.9635 - precision: 0.5625 - recall: 0.0028 - auc: 0.8242 - f1: 0.0050 - val_loss: 1.5983 - val_tp: 17.0000 - val_fp: 7.0000 - val_tn: 85716.0000 - val_fn: 3260.0000 - val_accuracy: 0.9633 - val_precision: 0.7083 - val_recall: 0.0052 - val_auc: 0.8295 - val_f1: 0.0106Epoch 6/20\n",
      " 88258/449258 [====>.........................] - ETA: 10s - loss: 7.0787 - tp: 26.0000 - fp: 22.0000 - tn: 85022.0000 - fn: 3188.0000 - accuracy: 0.9636 - precision: 0.5417 - recall: 0.0081 - auc: 0.8273 - f1: 0.0153 - val_loss: 1.2220 - val_tp: 73.0000 - val_fp: 38.0000 - val_tn: 85685.0000 - val_fn: 3204.0000 - val_accuracy: 0.9636 - val_precision: 0.6577 - val_recall: 0.0223 - val_auc: 0.8385 - val_f1: 0.0418Epoch 7/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 5.4377 - tp: 93.0000 - fp: 99.0000 - tn: 85618.0000 - fn: 3190.0000 - accuracy: 0.9630 - precision: 0.4844 - recall: 0.0283 - auc: 0.8395 - f1: 0.0517 - val_loss: 0.9418 - val_tp: 102.0000 - val_fp: 75.0000 - val_tn: 85648.0000 - val_fn: 3175.0000 - val_accuracy: 0.9635 - val_precision: 0.5763 - val_recall: 0.0311 - val_auc: 0.8439 - val_f1: 0.0572Epoch 8/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 4.2087 - tp: 111.0000 - fp: 127.0000 - tn: 85648.0000 - fn: 3114.0000 - accuracy: 0.9636 - precision: 0.4664 - recall: 0.0344 - auc: 0.8429 - f1: 0.0634 - val_loss: 0.7358 - val_tp: 116.0000 - val_fp: 88.0000 - val_tn: 85635.0000 - val_fn: 3161.0000 - val_accuracy: 0.9635 - val_precision: 0.5686 - val_recall: 0.0354 - val_auc: 0.8460 - val_f1: 0.0644Epoch 9/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 3.3273 - tp: 113.0000 - fp: 156.0000 - tn: 85586.0000 - fn: 3145.0000 - accuracy: 0.9629 - precision: 0.4201 - recall: 0.0347 - auc: 0.8407 - f1: 0.0618 - val_loss: 0.5862 - val_tp: 122.0000 - val_fp: 95.0000 - val_tn: 85628.0000 - val_fn: 3155.0000 - val_accuracy: 0.9635 - val_precision: 0.5622 - val_recall: 0.0372 - val_auc: 0.8486 - val_f1: 0.0676Epoch 10/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 2.6825 - tp: 223.0000 - fp: 217.0000 - tn: 85444.0000 - fn: 3116.0000 - accuracy: 0.9626 - precision: 0.5068 - recall: 0.0668 - auc: 0.8501 - f1: 0.1157 - val_loss: 0.4784 - val_tp: 244.0000 - val_fp: 312.0000 - val_tn: 85411.0000 - val_fn: 3033.0000 - val_accuracy: 0.9624 - val_precision: 0.4388 - val_recall: 0.0745 - val_auc: 0.8510 - val_f1: 0.1236Epoch 11/20\n",
      " 88258/449258 [====>.........................] - ETA: 10s - loss: 2.2381 - tp: 212.0000 - fp: 347.0000 - tn: 84576.0000 - fn: 3123.0000 - accuracy: 0.9607 - precision: 0.3792 - recall: 0.0636 - auc: 0.8491 - f1: 0.1052 - val_loss: 0.4026 - val_tp: 160.0000 - val_fp: 142.0000 - val_tn: 85581.0000 - val_fn: 3117.0000 - val_accuracy: 0.9634 - val_precision: 0.5298 - val_recall: 0.0488 - val_auc: 0.8526 - val_f1: 0.0863Epoch 12/20\n",
      " 89000/449258 [====>.........................] - ETA: 7s - loss: 1.8841 - tp: 175.0000 - fp: 269.0000 - tn: 85529.0000 - fn: 3027.0000 - accuracy: 0.9630 - precision: 0.3941 - recall: 0.0547 - auc: 0.8472 - f1: 0.0932 - val_loss: 0.3495 - val_tp: 203.0000 - val_fp: 212.0000 - val_tn: 85511.0000 - val_fn: 3074.0000 - val_accuracy: 0.9631 - val_precision: 0.4892 - val_recall: 0.0619 - val_auc: 0.8522 - val_f1: 0.1064Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89000/449258 [====>.........................] - ETA: 7s - loss: 1.6683 - tp: 231.0000 - fp: 327.0000 - tn: 85382.0000 - fn: 3060.0000 - accuracy: 0.9619 - precision: 0.4140 - recall: 0.0702 - auc: 0.8540 - f1: 0.1167 - val_loss: 0.3131 - val_tp: 228.0000 - val_fp: 268.0000 - val_tn: 85455.0000 - val_fn: 3049.0000 - val_accuracy: 0.9627 - val_precision: 0.4597 - val_recall: 0.0696 - val_auc: 0.8527 - val_f1: 0.1169Epoch 14/20\n",
      " 89000/449258 [====>.........................] - ETA: 9s - loss: 1.5075 - tp: 218.0000 - fp: 321.0000 - tn: 85468.0000 - fn: 2993.0000 - accuracy: 0.9628 - precision: 0.4045 - recall: 0.0679 - auc: 0.8494 - f1: 0.1127 - val_loss: 0.2883 - val_tp: 237.0000 - val_fp: 294.0000 - val_tn: 85429.0000 - val_fn: 3040.0000 - val_accuracy: 0.9625 - val_precision: 0.4463 - val_recall: 0.0723 - val_auc: 0.8547 - val_f1: 0.1210Epoch 15/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 1.4289 - tp: 234.0000 - fp: 336.0000 - tn: 85390.0000 - fn: 3040.0000 - accuracy: 0.9621 - precision: 0.4105 - recall: 0.0715 - auc: 0.8455 - f1: 0.1166 - val_loss: 0.2720 - val_tp: 210.0000 - val_fp: 222.0000 - val_tn: 85501.0000 - val_fn: 3067.0000 - val_accuracy: 0.9630 - val_precision: 0.4861 - val_recall: 0.0641 - val_auc: 0.8557 - val_f1: 0.1091Epoch 16/20\n",
      " 88258/449258 [====>.........................] - ETA: 8s - loss: 1.3397 - tp: 270.0000 - fp: 397.0000 - tn: 84629.0000 - fn: 2962.0000 - accuracy: 0.9619 - precision: 0.4048 - recall: 0.0835 - auc: 0.8554 - f1: 0.1355 - val_loss: 0.2612 - val_tp: 219.0000 - val_fp: 235.0000 - val_tn: 85488.0000 - val_fn: 3058.0000 - val_accuracy: 0.9630 - val_precision: 0.4824 - val_recall: 0.0668 - val_auc: 0.8559 - val_f1: 0.1136Epoch 17/20\n",
      " 89000/449258 [====>.........................] - ETA: 12s - loss: 1.3030 - tp: 253.0000 - fp: 427.0000 - tn: 85292.0000 - fn: 3028.0000 - accuracy: 0.9612 - precision: 0.3721 - recall: 0.0771 - auc: 0.8562 - f1: 0.1258 - val_loss: 0.2539 - val_tp: 216.0000 - val_fp: 228.0000 - val_tn: 85495.0000 - val_fn: 3061.0000 - val_accuracy: 0.9630 - val_precision: 0.4865 - val_recall: 0.0659 - val_auc: 0.8559 - val_f1: 0.1121Epoch 18/20\n",
      " 89000/449258 [====>.........................] - ETA: 11s - loss: 1.2685 - tp: 268.0000 - fp: 394.0000 - tn: 85357.0000 - fn: 2981.0000 - accuracy: 0.9621 - precision: 0.4048 - recall: 0.0825 - auc: 0.8554 - f1: 0.1337 - val_loss: 0.2495 - val_tp: 193.0000 - val_fp: 188.0000 - val_tn: 85535.0000 - val_fn: 3084.0000 - val_accuracy: 0.9632 - val_precision: 0.5066 - val_recall: 0.0589 - val_auc: 0.8563 - val_f1: 0.1017Epoch 19/20\n",
      " 89000/449258 [====>.........................] - ETA: 11s - loss: 1.2674 - tp: 253.0000 - fp: 430.0000 - tn: 85273.0000 - fn: 3044.0000 - accuracy: 0.9610 - precision: 0.3704 - recall: 0.0767 - auc: 0.8494 - f1: 0.1227 - val_loss: 0.2464 - val_tp: 199.0000 - val_fp: 197.0000 - val_tn: 85526.0000 - val_fn: 3078.0000 - val_accuracy: 0.9632 - val_precision: 0.5025 - val_recall: 0.0607 - val_auc: 0.8569 - val_f1: 0.1042Epoch 20/20\n",
      " 89000/449258 [====>.........................] - ETA: 11s - loss: 1.2400 - tp: 287.0000 - fp: 431.0000 - tn: 85304.0000 - fn: 2978.0000 - accuracy: 0.9617 - precision: 0.3997 - recall: 0.0879 - auc: 0.8520 - f1: 0.1434 - val_loss: 0.2439 - val_tp: 254.0000 - val_fp: 343.0000 - val_tn: 85380.0000 - val_fn: 3023.0000 - val_accuracy: 0.9622 - val_precision: 0.4255 - val_recall: 0.0775 - val_auc: 0.8566 - val_f1: 0.1272\n",
      "Validation fraction incorrect: 0.93\n",
      "\n",
      "learning rate: 9.3e-02\n",
      "hidden layers: 11\n",
      "neurons per layer: 65\n",
      "dropout rate: 0.1\n",
      "l2 lambda: 3.3e-03\n",
      "class weight: 2.7, 12.0\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 65)                2665      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 65)                4290      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 66        \n",
      "=================================================================\n",
      "Total params: 49,921\n",
      "Trainable params: 49,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 89000/449258 [====>.........................] - ETA: 34s - loss: 19.2200 - tp: 74.0000 - fp: 2110.0000 - tn: 83652.0000 - fn: 3164.0000 - accuracy: 0.9407 - precision: 0.0339 - recall: 0.0229 - auc: 0.4980 - f1: 0.0020 - val_loss: 1.8952 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 89000/449258 [====>.........................] - ETA: 14s - loss: 7.8594 - tp: 108.0000 - fp: 529.0000 - tn: 85171.0000 - fn: 3192.0000 - accuracy: 0.9582 - precision: 0.1695 - recall: 0.0327 - auc: 0.6781 - f1: 0.0249 - val_loss: 1.2749 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8241 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 89000/449258 [====>.........................] - ETA: 14s - loss: 5.5581 - tp: 744.0000 - fp: 3218.0000 - tn: 82540.0000 - fn: 2498.0000 - accuracy: 0.9358 - precision: 0.1878 - recall: 0.2295 - auc: 0.8173 - f1: 0.1769 - val_loss: 0.9516 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8573 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 89000/449258 [====>.........................] - ETA: 15s - loss: 4.3217 - tp: 935.0000 - fp: 4208.0000 - tn: 81479.0000 - fn: 2378.0000 - accuracy: 0.9260 - precision: 0.1818 - recall: 0.2822 - auc: 0.8215 - f1: 0.1768 - val_loss: 0.7593 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8520 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 89000/449258 [====>.........................] - ETA: 14s - loss: 3.5010 - tp: 883.0000 - fp: 3658.0000 - tn: 82088.0000 - fn: 2371.0000 - accuracy: 0.9323 - precision: 0.1945 - recall: 0.2714 - auc: 0.8318 - f1: 0.1954 - val_loss: 0.6186 - val_tp: 1635.0000 - val_fp: 5909.0000 - val_tn: 79814.0000 - val_fn: 1642.0000 - val_accuracy: 0.9152 - val_precision: 0.2167 - val_recall: 0.4989 - val_auc: 0.8613 - val_f1: 0.3009Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88258/449258 [====>.........................] - ETA: 14s - loss: 2.9110 - tp: 995.0000 - fp: 3869.0000 - tn: 81175.0000 - fn: 2219.0000 - accuracy: 0.9310 - precision: 0.2046 - recall: 0.3096 - auc: 0.8403 - f1: 0.2322 - val_loss: 0.5249 - val_tp: 1592.0000 - val_fp: 5371.0000 - val_tn: 80352.0000 - val_fn: 1685.0000 - val_accuracy: 0.9207 - val_precision: 0.2286 - val_recall: 0.4858 - val_auc: 0.8701 - val_f1: 0.3092Epoch 7/20\n",
      " 89000/449258 [====>.........................] - ETA: 16s - loss: 2.4800 - tp: 1052.0000 - fp: 3507.0000 - tn: 82210.0000 - fn: 2231.0000 - accuracy: 0.9355 - precision: 0.2308 - recall: 0.3204 - auc: 0.8512 - f1: 0.2661 - val_loss: 0.4593 - val_tp: 380.0000 - val_fp: 679.0000 - val_tn: 85044.0000 - val_fn: 2897.0000 - val_accuracy: 0.9598 - val_precision: 0.3588 - val_recall: 0.1160 - val_auc: 0.8551 - val_f1: 0.1713Epoch 8/20\n",
      " 89000/449258 [====>.........................] - ETA: 15s - loss: 2.1859 - tp: 1006.0000 - fp: 3708.0000 - tn: 82067.0000 - fn: 2219.0000 - accuracy: 0.9334 - precision: 0.2134 - recall: 0.3119 - auc: 0.8414 - f1: 0.2524 - val_loss: 0.3975 - val_tp: 1038.0000 - val_fp: 2538.0000 - val_tn: 83185.0000 - val_fn: 2239.0000 - val_accuracy: 0.9463 - val_precision: 0.2903 - val_recall: 0.3168 - val_auc: 0.8676 - val_f1: 0.3012Epoch 9/20\n",
      " 89000/449258 [====>.........................] - ETA: 15s - loss: 1.9366 - tp: 905.0000 - fp: 3316.0000 - tn: 82426.0000 - fn: 2353.0000 - accuracy: 0.9363 - precision: 0.2144 - recall: 0.2778 - auc: 0.8474 - f1: 0.2257 - val_loss: 0.3561 - val_tp: 1237.0000 - val_fp: 3329.0000 - val_tn: 82394.0000 - val_fn: 2040.0000 - val_accuracy: 0.9397 - val_precision: 0.2709 - val_recall: 0.3775 - val_auc: 0.8657 - val_f1: 0.3137Epoch 10/20\n",
      " 89000/449258 [====>.........................] - ETA: 16s - loss: 1.7449 - tp: 1077.0000 - fp: 3537.0000 - tn: 82124.0000 - fn: 2262.0000 - accuracy: 0.9348 - precision: 0.2334 - recall: 0.3226 - auc: 0.8536 - f1: 0.2687 - val_loss: 0.3224 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8698 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 88258/449258 [====>.........................] - ETA: 9s - loss: 1.6092 - tp: 1020.0000 - fp: 3650.0000 - tn: 81273.0000 - fn: 2315.0000 - accuracy: 0.9324 - precision: 0.2184 - recall: 0.3058 - auc: 0.8496 - f1: 0.2486 - val_loss: 0.2922 - val_tp: 1188.0000 - val_fp: 3144.0000 - val_tn: 82579.0000 - val_fn: 2089.0000 - val_accuracy: 0.9412 - val_precision: 0.2742 - val_recall: 0.3625 - val_auc: 0.8744 - val_f1: 0.3105Epoch 12/20\n",
      " 89000/449258 [====>.........................] - ETA: 9s - loss: 1.4498 - tp: 932.0000 - fp: 3073.0000 - tn: 82725.0000 - fn: 2270.0000 - accuracy: 0.9400 - precision: 0.2327 - recall: 0.2911 - auc: 0.8543 - f1: 0.2516 - val_loss: 0.2794 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8685 - val_f1: 0.0000e+00Epoch 13/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 1.3647 - tp: 1088.0000 - fp: 3556.0000 - tn: 82153.0000 - fn: 2203.0000 - accuracy: 0.9353 - precision: 0.2343 - recall: 0.3306 - auc: 0.8528 - f1: 0.2723 - val_loss: 0.2628 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8666 - val_f1: 0.0000e+00Epoch 14/20\n",
      " 89000/449258 [====>.........................] - ETA: 12s - loss: 1.2869 - tp: 970.0000 - fp: 3251.0000 - tn: 82538.0000 - fn: 2241.0000 - accuracy: 0.9383 - precision: 0.2298 - recall: 0.3021 - auc: 0.8479 - f1: 0.2558 - val_loss: 0.2465 - val_tp: 1892.0000 - val_fp: 7513.0000 - val_tn: 78210.0000 - val_fn: 1385.0000 - val_accuracy: 0.9000 - val_precision: 0.2012 - val_recall: 0.5774 - val_auc: 0.8718 - val_f1: 0.2967Epoch 15/20\n",
      " 89000/449258 [====>.........................] - ETA: 12s - loss: 1.2234 - tp: 969.0000 - fp: 3258.0000 - tn: 82468.0000 - fn: 2305.0000 - accuracy: 0.9375 - precision: 0.2292 - recall: 0.2960 - auc: 0.8512 - f1: 0.2467 - val_loss: 0.2258 - val_tp: 1229.0000 - val_fp: 3168.0000 - val_tn: 82555.0000 - val_fn: 2048.0000 - val_accuracy: 0.9414 - val_precision: 0.2795 - val_recall: 0.3750 - val_auc: 0.8741 - val_f1: 0.3183Epoch 16/20\n",
      " 88258/449258 [====>.........................] - ETA: 12s - loss: 1.1637 - tp: 1100.0000 - fp: 3430.0000 - tn: 81596.0000 - fn: 2132.0000 - accuracy: 0.9370 - precision: 0.2428 - recall: 0.3403 - auc: 0.8524 - f1: 0.2786 - val_loss: 0.2198 - val_tp: 1211.0000 - val_fp: 3154.0000 - val_tn: 82569.0000 - val_fn: 2066.0000 - val_accuracy: 0.9413 - val_precision: 0.2774 - val_recall: 0.3695 - val_auc: 0.8753 - val_f1: 0.3155Epoch 17/20\n",
      " 89000/449258 [====>.........................] - ETA: 12s - loss: 1.1410 - tp: 996.0000 - fp: 3486.0000 - tn: 82233.0000 - fn: 2285.0000 - accuracy: 0.9352 - precision: 0.2222 - recall: 0.3036 - auc: 0.8521 - f1: 0.2481 - val_loss: 0.2144 - val_tp: 839.0000 - val_fp: 1702.0000 - val_tn: 84021.0000 - val_fn: 2438.0000 - val_accuracy: 0.9535 - val_precision: 0.3302 - val_recall: 0.2560 - val_auc: 0.8722 - val_f1: 0.2867Epoch 18/20\n",
      " 89000/449258 [====>.........................] - ETA: 12s - loss: 1.0938 - tp: 1024.0000 - fp: 3465.0000 - tn: 82286.0000 - fn: 2225.0000 - accuracy: 0.9361 - precision: 0.2281 - recall: 0.3152 - auc: 0.8541 - f1: 0.2598 - val_loss: 0.2085 - val_tp: 752.0000 - val_fp: 1437.0000 - val_tn: 84286.0000 - val_fn: 2525.0000 - val_accuracy: 0.9555 - val_precision: 0.3435 - val_recall: 0.2295 - val_auc: 0.8740 - val_f1: 0.2737Epoch 19/20\n",
      " 89000/449258 [====>.........................] - ETA: 10s - loss: 1.0728 - tp: 1033.0000 - fp: 3320.0000 - tn: 82383.0000 - fn: 2264.0000 - accuracy: 0.9373 - precision: 0.2373 - recall: 0.3133 - auc: 0.8527 - f1: 0.2650 - val_loss: 0.2077 - val_tp: 993.0000 - val_fp: 2348.0000 - val_tn: 83375.0000 - val_fn: 2284.0000 - val_accuracy: 0.9480 - val_precision: 0.2972 - val_recall: 0.3030 - val_auc: 0.8727 - val_f1: 0.2984Epoch 20/20\n",
      " 89000/449258 [====>.........................] - ETA: 9s - loss: 1.0528 - tp: 969.0000 - fp: 3161.0000 - tn: 82574.0000 - fn: 2296.0000 - accuracy: 0.9387 - precision: 0.2346 - recall: 0.2968 - auc: 0.8494 - f1: 0.2578 - val_loss: 0.2122 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8599 - val_f1: 0.0000e+00\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.5e-05\n",
      "hidden layers: 15\n",
      "neurons per layer: 31\n",
      "dropout rate: 0.14\n",
      "l2 lambda: 6.2e-02\n",
      "class weight: 2.0, 14.92\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 31)                1271      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 16,183\n",
      "Trainable params: 16,183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 89000/449258 [====>.........................] - ETA: 37s - loss: 29.9233 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85762.0000 - fn: 3238.0000 - accuracy: 0.9636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4966 - f1: 0.0000e+00 - val_loss: 5.8915 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5019 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 89000/449258 [====>.........................] - ETA: 13s - loss: 29.5561 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85700.0000 - fn: 3300.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4999 - f1: 0.0000e+00 - val_loss: 5.8139 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4995 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 89000/449258 [====>.........................] - ETA: 16s - loss: 29.1432 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85758.0000 - fn: 3242.0000 - accuracy: 0.9636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5014 - f1: 0.0000e+00 - val_loss: 5.7376 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 89000/449258 [====>.........................] - ETA: 17s - loss: 28.7910 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85687.0000 - fn: 3313.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5005 - f1: 0.0000e+00 - val_loss: 5.6623 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4995 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 89000/449258 [====>.........................] - ETA: 19s - loss: 28.3913 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85746.0000 - fn: 3254.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5000 - f1: 0.0000e+00 - val_loss: 5.5885 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5005 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 88258/449258 [====>.........................] - ETA: 20s - loss: 28.0145 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85044.0000 - fn: 3214.0000 - accuracy: 0.9636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5014 - f1: 0.0000e+00 - val_loss: 5.5157 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5082 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 89000/449258 [====>.........................] - ETA: 20s - loss: 27.6687 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85717.0000 - fn: 3283.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5006 - f1: 0.0000e+00 - val_loss: 5.4437 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5050 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 89000/449258 [====>.........................] - ETA: 19s - loss: 27.2835 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85775.0000 - fn: 3225.0000 - accuracy: 0.9638 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5054 - f1: 0.0000e+00 - val_loss: 5.3725 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4990 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 89000/449258 [====>.........................] - ETA: 20s - loss: 26.9411 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85742.0000 - fn: 3258.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5025 - f1: 0.0000e+00 - val_loss: 5.3024 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5007 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 89000/449258 [====>.........................] - ETA: 20s - loss: 26.6216 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85661.0000 - fn: 3339.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4994 - f1: 0.0000e+00 - val_loss: 5.2332 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5154 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 88258/449258 [====>.........................] - ETA: 20s - loss: 26.2839 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 84923.0000 - fn: 3335.0000 - accuracy: 0.9622 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4941 - f1: 0.0000e+00 - val_loss: 5.1651 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5155 - val_f1: 0.0000e+00Epoch 12/20\n",
      " 89000/449258 [====>.........................] - ETA: 21s - loss: 25.8868 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85798.0000 - fn: 3202.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5018 - f1: 0.0000e+00 - val_loss: 5.0984 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5310 - val_f1: 0.0000e+00Epoch 13/20\n",
      " 89000/449258 [====>.........................] - ETA: 20s - loss: 25.5858 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85709.0000 - fn: 3291.0000 - accuracy: 0.9630 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4965 - f1: 0.0000e+00 - val_loss: 5.0326 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5396 - val_f1: 0.0000e+00Epoch 14/20\n",
      " 89000/449258 [====>.........................] - ETA: 20s - loss: 25.2290 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85789.0000 - fn: 3211.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5049 - f1: 0.0000e+00 - val_loss: 4.9679 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5449 - val_f1: 0.0000e+00Epoch 15/20\n",
      " 89000/449258 [====>.........................] - ETA: 21s - loss: 24.9270 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85726.0000 - fn: 3274.0000 - accuracy: 0.9632 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4998 - f1: 0.0000e+00 - val_loss: 4.9040 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5289 - val_f1: 0.0000e+00Epoch 16/20\n",
      " 88258/449258 [====>.........................] - ETA: 21s - loss: 24.5990 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85026.0000 - fn: 3232.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5066 - f1: 0.0000e+00 - val_loss: 4.8410 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5505 - val_f1: 0.0000e+00Epoch 17/20\n",
      " 89000/449258 [====>.........................] - ETA: 20s - loss: 24.2969 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85719.0000 - fn: 3281.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4945 - f1: 0.0000e+00 - val_loss: 4.7788 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5531 - val_f1: 0.0000e+00Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89000/449258 [====>.........................] - ETA: 20s - loss: 23.9740 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85751.0000 - fn: 3249.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5024 - f1: 0.0000e+00 - val_loss: 4.7175 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5586 - val_f1: 0.0000e+00Epoch 19/20\n",
      " 89000/449258 [====>.........................] - ETA: 16s - loss: 23.6824 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85703.0000 - fn: 3297.0000 - accuracy: 0.9630 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4992 - f1: 0.0000e+00 - val_loss: 4.6568 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5069 - val_f1: 0.0000e+00Epoch 20/20\n",
      " 89000/449258 [====>.........................] - ETA: 13s - loss: 23.3669 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 85735.0000 - fn: 3265.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5032 - f1: 0.0000e+00 - val_loss: 4.5970 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 85723.0000 - val_fn: 3277.0000 - val_accuracy: 0.9632 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5624 - val_f1: 0.0000e+00\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 7.4e-04\n",
      "hidden layers: 14\n",
      "neurons per layer: 71\n",
      "dropout rate: 0.28\n",
      "l2 lambda: 2.3e-03\n",
      "class weight: 4.7, 12.82\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 71)                2911      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 71)                5112      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 71)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 72        \n",
      "=================================================================\n",
      "Total params: 74,551\n",
      "Trainable params: 74,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 89000/449258 [====>.........................] - ETA: 34s - loss: 2.9617 - tp: 217.0000 - fp: 487.0000 - tn: 85275.0000 - fn: 3021.0000 - accuracy: 0.9606 - precision: 0.3082 - recall: 0.0670 - auc: 0.7269 - f1: 0.0885 - val_loss: 0.4416 - val_tp: 405.0000 - val_fp: 675.0000 - val_tn: 85048.0000 - val_fn: 2872.0000 - val_accuracy: 0.9601 - val_precision: 0.3750 - val_recall: 0.1236 - val_auc: 0.8516 - val_f1: 0.1820Epoch 2/20\n",
      " 89000/449258 [====>.........................] - ETA: 14s - loss: 1.9973 - tp: 453.0000 - fp: 976.0000 - tn: 84724.0000 - fn: 2847.0000 - accuracy: 0.9570 - precision: 0.3170 - recall: 0.1373 - auc: 0.8433 - f1: 0.1784 - val_loss: 0.3419 - val_tp: 644.0000 - val_fp: 1237.0000 - val_tn: 84486.0000 - val_fn: 2633.0000 - val_accuracy: 0.9565 - val_precision: 0.3424 - val_recall: 0.1965 - val_auc: 0.8669 - val_f1: 0.2469Epoch 3/20\n",
      " 89000/449258 [====>.........................] - ETA: 18s - loss: 1.6232 - tp: 499.0000 - fp: 1080.0000 - tn: 84678.0000 - fn: 2743.0000 - accuracy: 0.9570 - precision: 0.3160 - recall: 0.1539 - auc: 0.8599 - f1: 0.2038 - val_loss: 0.2991 - val_tp: 421.0000 - val_fp: 663.0000 - val_tn: 85060.0000 - val_fn: 2856.0000 - val_accuracy: 0.9605 - val_precision: 0.3884 - val_recall: 0.1285 - val_auc: 0.8703 - val_f1: 0.1888Epoch 4/20\n",
      " 89000/449258 [====>.........................] - ETA: 21s - loss: 1.4955 - tp: 542.0000 - fp: 1129.0000 - tn: 84558.0000 - fn: 2771.0000 - accuracy: 0.9562 - precision: 0.3244 - recall: 0.1636 - auc: 0.8605 - f1: 0.2128 - val_loss: 0.2786 - val_tp: 465.0000 - val_fp: 730.0000 - val_tn: 84993.0000 - val_fn: 2812.0000 - val_accuracy: 0.9602 - val_precision: 0.3891 - val_recall: 0.1419 - val_auc: 0.8739 - val_f1: 0.2045Epoch 5/20\n",
      " 89000/449258 [====>.........................] - ETA: 22s - loss: 1.4027 - tp: 571.0000 - fp: 1159.0000 - tn: 84587.0000 - fn: 2683.0000 - accuracy: 0.9568 - precision: 0.3301 - recall: 0.1755 - auc: 0.8651 - f1: 0.2230 - val_loss: 0.2694 - val_tp: 436.0000 - val_fp: 643.0000 - val_tn: 85080.0000 - val_fn: 2841.0000 - val_accuracy: 0.9609 - val_precision: 0.4041 - val_recall: 0.1330 - val_auc: 0.8732 - val_f1: 0.1953Epoch 6/20\n",
      " 88258/449258 [====>.........................] - ETA: 21s - loss: 1.3590 - tp: 521.0000 - fp: 1098.0000 - tn: 83946.0000 - fn: 2693.0000 - accuracy: 0.9570 - precision: 0.3218 - recall: 0.1621 - auc: 0.8649 - f1: 0.2072 - val_loss: 0.2613 - val_tp: 553.0000 - val_fp: 849.0000 - val_tn: 84874.0000 - val_fn: 2724.0000 - val_accuracy: 0.9599 - val_precision: 0.3944 - val_recall: 0.1688 - val_auc: 0.8744 - val_f1: 0.2330Epoch 7/20\n",
      " 88000/449258 [====>.........................] - ETA: 15s - loss: 1.3294 - tp: 573.0000 - fp: 1169.0000 - tn: 83586.0000 - fn: 2672.0000 - accuracy: 0.9564 - precision: 0.3289 - recall: 0.1766 - auc: 0.8692 - f1: 0.2225"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(\n",
    "    func=fitness,\n",
    "    dimensions=dimensions,\n",
    "    acq_func='EI', # Expected Improvement.\n",
    "    n_calls=11,\n",
    "    x0=default_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_names = [\n",
    "    'learning_rate',\n",
    "    'hidden_layers',\n",
    "    'neurons_per_layer',\n",
    "    'dropout_rate',\n",
    "    'l2_lambda',\n",
    "    'class_0_weight',\n",
    "    'class_1_weight'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_objective(result=search_result, dimension_names=dim_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = search_result.space\n",
    "winning_hyperparams = space.point_to_dict(search_result.x)\n",
    "winning_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.001, 2, 30, 0.5, 0.1, 0.5, 15 defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = winning_hyperparams['learning_rate'] / 10\n",
    "#learning_rate = 0.0001\n",
    "hidden_layers = winning_hyperparams['hidden_layers']\n",
    "units_per_layer = winning_hyperparams['neurons_per_layer']\n",
    "dropout_rate = winning_hyperparams['dropout_rate']\n",
    "l2_lambda = winning_hyperparams['l2_lambda']\n",
    "#l2_lambda = 0.005\n",
    "class_0_weight = winning_hyperparams['class_0_weight']\n",
    "#class_0_weight = 0.6\n",
    "class_1_weight = winning_hyperparams['class_1_weight']\n",
    "\n",
    "initial_bias = np.log([ignition_count/no_ignition_count])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "    \n",
    "class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 100000\n",
    "STEPS_PER_EPOCH = (len(training_data) * 1) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(validation_data) * 1) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.layers.Dense(\n",
    "    units_per_layer, \n",
    "    activation = 'relu', \n",
    "    input_dim = train_features.shape[-1],\n",
    "))\n",
    "\n",
    "# Add fully connected hidden layers\n",
    "for i in range(hidden_layers):\n",
    "    model.add(keras.layers.Dense(\n",
    "        units_per_layer,\n",
    "        bias_initializer=keras.initializers.VarianceScaling(\n",
    "            scale=1.0,\n",
    "            mode='fan_in', \n",
    "            distribution='normal', \n",
    "            seed=None\n",
    "        ),\n",
    "        kernel_regularizer=keras.regularizers.l2(l2_lambda),\n",
    "        activation = 'relu')\n",
    "    )\n",
    "\n",
    "# Add dropout layer\n",
    "model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Add output layer\n",
    "model.add(keras.layers.Dense(\n",
    "    1, \n",
    "    activation = 'sigmoid', \n",
    "    bias_initializer = output_bias\n",
    "))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    class_weight=class_weight,\n",
    "    workers=8\n",
    ")\n",
    "\n",
    "model.save('../trained_models/best_MLP.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_features, batch_size=len(train_features))\n",
    "test_predictions = model.predict(test_features, batch_size=len(test_features))\n",
    "\n",
    "results = model.evaluate(\n",
    "    test_features,\n",
    "    test_labels,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(name, ': ', value)\n",
    "\n",
    "plot_cm(test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\"Train\", train_labels, train_predictions, color=colors[0])\n",
    "plot_roc(\"Test\", test_labels, test_predictions, color=colors[0], linestyle='--')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
