{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "#from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_objective_2D #, plot_histogram\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dir_name(\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda,\n",
    "    class_0_weight,\n",
    "    class_1_weight\n",
    "):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./MPL_logs/hidden_layers{0}_neurons_per_layer{1}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(\n",
    "        learning_rate,\n",
    "        hidden_layers,\n",
    "        neurons_per_layer,\n",
    "        dropout_rate,\n",
    "        l2_lambda,\n",
    "        class_0_weight,\n",
    "        class_1_weight\n",
    "    )\n",
    "\n",
    "    return log_dir\n",
    "\n",
    "\n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'f1']\n",
    "    \n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(3,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        \n",
    "    if metric == 'loss':\n",
    "        plt.ylim([0, plt.ylim()[1]])\n",
    "        \n",
    "    elif metric == 'auc':\n",
    "        plt.ylim([0.8,1])\n",
    "        \n",
    "    else:\n",
    "        #plt.ylim([0,1])\n",
    "        plt.legend()\n",
    "        \n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    normalized_cm = np.empty([2, 2])\n",
    "    normalized_cm[0][0] = cm[0][0] / (cm[0][0] + cm[0][1])\n",
    "    normalized_cm[0][1] = cm[0][1] / (cm[0][0] + cm[0][1])\n",
    "    normalized_cm[1][0] = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "    normalized_cm[1][1] = cm[1][1] / (cm[1][0] + cm[1][1])\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(normalized_cm, annot=True)\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('No fire (True Negatives): ', cm[0][0])\n",
    "    print('False alarms (False Positives): ', cm[0][1])\n",
    "    print('Fires missed (False Negatives): ', cm[1][0])\n",
    "    print('Fires detected (True Positives): ', cm[1][1])\n",
    "    print('Total fires: ', np.sum(cm[1]))\n",
    "    \n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    #plt.xlim([-0.5,20])\n",
    "    #plt.ylim([80,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "def f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/stratified_training_data/1992-2015_training_data_added_features_n500000_ks_pval0.3.1.csv'\n",
    "validation_file = '../data/stratified_training_data/1992-2015_training_data_added_features_n500000_ks_pval0.3.2.csv'\n",
    "test_file = '../data/stratified_training_data/1992-2015_training_data_added_features_n500000_ks_pval0.3.3.csv'\n",
    "\n",
    "# Datatypes for dataframe loading\n",
    "dtypes = {\n",
    "    'lat': float,\n",
    "    'lon': float,\n",
    "    'weather_bin_year': int,\n",
    "    'weather_bin_month': int,\n",
    "    'weather_bin_day': int,\n",
    "    'air.2m': float,\n",
    "    'apcp': float,\n",
    "    'rhum.2m': float,\n",
    "    'dpt.2m': float,\n",
    "    'pres.sfc': float,\n",
    "    'uwnd.10m': float,\n",
    "    'vwnd.10m': float,\n",
    "    'veg': float,\n",
    "    'vis': float,\n",
    "    'ignition': float,\n",
    "    'mean.air.2m': float,\n",
    "    'mean.apcp': float,\n",
    "    'mean.rhum.2m': float,\n",
    "    'mean.dpt.2m': float,\n",
    "    'mean.pres.sfc': float,\n",
    "    'mean.uwnd.10m': float,\n",
    "    'mean.vwnd.10m': float,\n",
    "    'mean.veg': float,\n",
    "    'mean.vis': float,\n",
    "    'max.air.2m': float,\n",
    "    'max.apcp': float,\n",
    "    'max.rhum.2m': float,\n",
    "    'max.dpt.2m': float,\n",
    "    'max.pres.sfc': float,\n",
    "    'max.uwnd.10m': float,\n",
    "    'max.vwnd.10m': float,\n",
    "    'max.veg': float,\n",
    "    'max.vis': float,\n",
    "    'min.air.2m': float,\n",
    "    'min.apcp': float,\n",
    "    'min.rhum.2m': float,\n",
    "    'min.dpt.2m': float,\n",
    "    'min.pres.sfc': float,\n",
    "    'min.uwnd.10m': float,\n",
    "    'min.vwnd.10m': float,\n",
    "    'min.veg': float,\n",
    "    'min.vis': float,\n",
    "    'total_fires': float\n",
    "\n",
    "}\n",
    "\n",
    "# Features to use during training \n",
    "features = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'weather_bin_month',\n",
    "    'veg',\n",
    "    'ignition',\n",
    "    'mean.air.2m',\n",
    "    'mean.apcp',\n",
    "    'mean.rhum.2m',\n",
    "    'mean.dpt.2m',\n",
    "    'mean.pres.sfc',\n",
    "    'mean.uwnd.10m',\n",
    "    'mean.vwnd.10m',\n",
    "    'mean.vis',\n",
    "    'max.air.2m',\n",
    "    'max.apcp',\n",
    "    'max.rhum.2m',\n",
    "    'max.dpt.2m',\n",
    "    'max.pres.sfc',\n",
    "    'max.uwnd.10m',\n",
    "    'max.vwnd.10m',\n",
    "    'max.vis',\n",
    "    'min.air.2m',\n",
    "    'min.apcp',\n",
    "    'min.rhum.2m',\n",
    "    'min.dpt.2m',\n",
    "    'min.pres.sfc',\n",
    "    'min.uwnd.10m',\n",
    "    'min.vwnd.10m',\n",
    "    'min.vis',\n",
    "    'total_fires'\n",
    "]\n",
    "\n",
    "features_to_scale = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'veg',\n",
    "    'mean.air.2m',\n",
    "    'mean.apcp',\n",
    "    'mean.rhum.2m',\n",
    "    'mean.dpt.2m',\n",
    "    'mean.pres.sfc',\n",
    "    'mean.uwnd.10m',\n",
    "    'mean.vwnd.10m',\n",
    "    'mean.vis',\n",
    "    'max.air.2m',\n",
    "    'max.apcp',\n",
    "    'max.rhum.2m',\n",
    "    'max.dpt.2m',\n",
    "    'max.pres.sfc',\n",
    "    'max.uwnd.10m',\n",
    "    'max.vwnd.10m',\n",
    "    'max.vis',\n",
    "    'min.air.2m',\n",
    "    'min.apcp',\n",
    "    'min.rhum.2m',\n",
    "    'min.dpt.2m',\n",
    "    'min.pres.sfc',\n",
    "    'min.uwnd.10m',\n",
    "    'min.vwnd.10m',\n",
    "    'min.vis',\n",
    "    'total_fires'\n",
    "]\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'), \n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    f1\n",
    "]\n",
    "\n",
    "dim_learning_rate = Real(\n",
    "    low=1e-5, \n",
    "    high=1e-1, \n",
    "    prior='log-uniform',\n",
    "    name='learning_rate'\n",
    ")\n",
    "\n",
    "dim_hidden_layers = Integer(\n",
    "    low=2,\n",
    "    high=30, \n",
    "    name='hidden_layers'\n",
    ")\n",
    "\n",
    "dim_neurons_per_layer = Integer(\n",
    "    low=10, \n",
    "    high=200, \n",
    "    name='neurons_per_layer'\n",
    ")\n",
    "\n",
    "dim_dropout_rate = Real(\n",
    "    low=0.01, \n",
    "    high=0.5, \n",
    "    name='dropout_rate'\n",
    ")\n",
    "\n",
    "dim_l2_lambda = Real(\n",
    "    low=0.0001, \n",
    "    high=1, \n",
    "    name='l2_lambda'\n",
    ")\n",
    "\n",
    "dim_class_0_weight = Real(\n",
    "    low=0.1,\n",
    "    high=10,\n",
    "    name='class_0_weight'\n",
    ")\n",
    "\n",
    "dim_class_1_weight = Real(\n",
    "    low=5,\n",
    "    high=20,\n",
    "    name=\"class_1_weight\"\n",
    ")\n",
    "\n",
    "dimensions = [\n",
    "    dim_learning_rate,\n",
    "    dim_hidden_layers,\n",
    "    dim_neurons_per_layer,\n",
    "    dim_dropout_rate,\n",
    "    dim_l2_lambda,\n",
    "    dim_class_0_weight,\n",
    "    dim_class_1_weight\n",
    "]\n",
    "\n",
    "default_parameters = [0.001, 2, 30, 0.5, 0.1, 0.5, 15]\n",
    "\n",
    "path_best_model = 'best_MLP.keras'\n",
    "best_fraction_incorrect = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file, dtype=dtypes)\n",
    "validation = pd.read_csv(validation_file, dtype=dtypes)\n",
    "test = pd.read_csv(test_file, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "training_data = data[features]\n",
    "validation_data = validation[features]\n",
    "test_data = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# One hot encode month\n",
    "\n",
    "column_names = [\n",
    "    'January',\n",
    "    'February',\n",
    "    'March',\n",
    "    'April',\n",
    "    'May',\n",
    "    'June',\n",
    "    'July',\n",
    "    'August',\n",
    "    'Septermber',\n",
    "    'October',\n",
    "    'November',\n",
    "    'December'\n",
    "]\n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Training data\n",
    "month = np.array(training_data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "training_data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "training_data = pd.concat([training_data, onehot_month_df], axis=1)\n",
    "\n",
    "# Validation data\n",
    "month = np.array(validation_data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "validation_data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "validation_data = pd.concat([validation_data, onehot_month_df], axis=1)\n",
    "\n",
    "# Test data data\n",
    "month = np.array(test_data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "test_data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "test_data = pd.concat([test_data, onehot_month_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>veg</th>\n",
       "      <th>ignition</th>\n",
       "      <th>mean.air.2m</th>\n",
       "      <th>mean.apcp</th>\n",
       "      <th>mean.rhum.2m</th>\n",
       "      <th>mean.dpt.2m</th>\n",
       "      <th>mean.pres.sfc</th>\n",
       "      <th>mean.uwnd.10m</th>\n",
       "      <th>...</th>\n",
       "      <th>March</th>\n",
       "      <th>April</th>\n",
       "      <th>May</th>\n",
       "      <th>June</th>\n",
       "      <th>July</th>\n",
       "      <th>August</th>\n",
       "      <th>Septermber</th>\n",
       "      <th>October</th>\n",
       "      <th>November</th>\n",
       "      <th>December</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>34.18678</td>\n",
       "      <td>-118.8088</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.170681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.576653</td>\n",
       "      <td>279.735122</td>\n",
       "      <td>96650.870000</td>\n",
       "      <td>0.064065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37.03086</td>\n",
       "      <td>-119.0176</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.386004</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>33.640771</td>\n",
       "      <td>277.393281</td>\n",
       "      <td>79824.146797</td>\n",
       "      <td>-0.407725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37.49919</td>\n",
       "      <td>-119.8433</td>\n",
       "      <td>33.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.263178</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>60.458148</td>\n",
       "      <td>279.509951</td>\n",
       "      <td>89032.984219</td>\n",
       "      <td>-0.223786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>36.23206</td>\n",
       "      <td>-118.5007</td>\n",
       "      <td>46.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.102028</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>45.363947</td>\n",
       "      <td>256.344916</td>\n",
       "      <td>76906.623375</td>\n",
       "      <td>0.438386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>32.91990</td>\n",
       "      <td>-114.8649</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.117580</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>26.899587</td>\n",
       "      <td>269.320998</td>\n",
       "      <td>99661.673406</td>\n",
       "      <td>1.417608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       lon   veg  ignition  mean.air.2m  mean.apcp  mean.rhum.2m  \\\n",
       "0  34.18678 -118.8088  25.5       0.0   289.170681   0.000000     57.576653   \n",
       "1  37.03086 -119.0176  43.3       0.0   294.386004   0.000122     33.640771   \n",
       "2  37.49919 -119.8433  33.9       0.0   287.263178   0.139648     60.458148   \n",
       "3  36.23206 -118.5007  46.9       0.0   267.102028   0.000318     45.363947   \n",
       "4  32.91990 -114.8649   3.5       0.0   289.117580   0.000506     26.899587   \n",
       "\n",
       "   mean.dpt.2m  mean.pres.sfc  mean.uwnd.10m  ...  March  April  May  June  \\\n",
       "0   279.735122   96650.870000       0.064065  ...    0.0    0.0  0.0   0.0   \n",
       "1   277.393281   79824.146797      -0.407725  ...    0.0    0.0  0.0   0.0   \n",
       "2   279.509951   89032.984219      -0.223786  ...    0.0    1.0  0.0   0.0   \n",
       "3   256.344916   76906.623375       0.438386  ...    0.0    0.0  0.0   0.0   \n",
       "4   269.320998   99661.673406       1.417608  ...    0.0    0.0  0.0   0.0   \n",
       "\n",
       "   July  August  Septermber  October  November  December  \n",
       "0   0.0     0.0         0.0      0.0       0.0       0.0  \n",
       "1   0.0     1.0         0.0      0.0       0.0       0.0  \n",
       "2   0.0     0.0         0.0      0.0       0.0       0.0  \n",
       "3   0.0     0.0         0.0      0.0       0.0       1.0  \n",
       "4   0.0     0.0         0.0      0.0       1.0       0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 449258 entries, 0 to 449257\n",
      "Data columns (total 41 columns):\n",
      "lat              449258 non-null float64\n",
      "lon              449258 non-null float64\n",
      "veg              449258 non-null float64\n",
      "ignition         449258 non-null float64\n",
      "mean.air.2m      449258 non-null float64\n",
      "mean.apcp        449258 non-null float64\n",
      "mean.rhum.2m     449258 non-null float64\n",
      "mean.dpt.2m      449258 non-null float64\n",
      "mean.pres.sfc    449258 non-null float64\n",
      "mean.uwnd.10m    449258 non-null float64\n",
      "mean.vwnd.10m    449258 non-null float64\n",
      "mean.vis         449258 non-null float64\n",
      "max.air.2m       449258 non-null float64\n",
      "max.apcp         449258 non-null float64\n",
      "max.rhum.2m      449258 non-null float64\n",
      "max.dpt.2m       449258 non-null float64\n",
      "max.pres.sfc     449258 non-null float64\n",
      "max.uwnd.10m     449258 non-null float64\n",
      "max.vwnd.10m     449258 non-null float64\n",
      "max.vis          449258 non-null float64\n",
      "min.air.2m       449258 non-null float64\n",
      "min.apcp         449258 non-null float64\n",
      "min.rhum.2m      449258 non-null float64\n",
      "min.dpt.2m       449258 non-null float64\n",
      "min.pres.sfc     449258 non-null float64\n",
      "min.uwnd.10m     449258 non-null float64\n",
      "min.vwnd.10m     449258 non-null float64\n",
      "min.vis          449258 non-null float64\n",
      "total_fires      449258 non-null float64\n",
      "January          449258 non-null float64\n",
      "February         449258 non-null float64\n",
      "March            449258 non-null float64\n",
      "April            449258 non-null float64\n",
      "May              449258 non-null float64\n",
      "June             449258 non-null float64\n",
      "July             449258 non-null float64\n",
      "August           449258 non-null float64\n",
      "Septermber       449258 non-null float64\n",
      "October          449258 non-null float64\n",
      "November         449258 non-null float64\n",
      "December         449258 non-null float64\n",
      "dtypes: float64(41)\n",
      "memory usage: 140.5 MB\n"
     ]
    }
   ],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(training_data.pop('ignition'))\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(validation_data.pop('ignition'))\n",
    "test_labels = np.array(test_data.pop('ignition'))\n",
    "\n",
    "train_features = np.array(training_data)\n",
    "val_features = np.array(validation_data)\n",
    "test_features = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (449258,)\n",
      "Validation labels shape: (449257,)\n",
      "Test labels shape: (449258,)\n",
      "Training features shape: (449258, 40)\n",
      "Validation features shape: (449257, 40)\n",
      "Test features shape: (449258, 40)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "# weight_for_0 = (1 / no_ignition_count)*(total)/2.0 \n",
    "# weight_for_1 = (1 / ignition_count)*(total)/2.0\n",
    "\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "# print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "# print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "\n",
    "ignition_count = sum(train_labels)\n",
    "no_ignition_count = len(train_labels) - ignition_count\n",
    "\n",
    "initial_bias = np.log([ignition_count/no_ignition_count])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 500\n",
    "STEPS_PER_EPOCH = (len(train_features) * 0.1) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(val_features) * 0.1) // BATCH_SIZE\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    output_bias,\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda\n",
    "):    \n",
    "    # Define the keras model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Add input layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        neurons_per_layer, \n",
    "        activation = 'relu', \n",
    "        input_dim = train_features.shape[-1],\n",
    "    ))\n",
    "\n",
    "    # Add fully connected hidden layers\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(keras.layers.Dense(\n",
    "            neurons_per_layer,\n",
    "            bias_initializer=keras.initializers.VarianceScaling(\n",
    "                scale=1.0,\n",
    "                mode='fan_in', \n",
    "                distribution='normal', \n",
    "                seed=None\n",
    "            ),\n",
    "            kernel_regularizer=keras.regularizers.l2(l2_lambda),\n",
    "            activation = 'relu')\n",
    "        )\n",
    "    \n",
    "    # Add dropout layer\n",
    "    model.add(keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        1, \n",
    "        activation = 'sigmoid', \n",
    "        bias_initializer = output_bias\n",
    "    ))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda,\n",
    "    class_0_weight,\n",
    "    class_1_weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "    \n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('hidden layers:', hidden_layers)\n",
    "    print('neurons per layer:', neurons_per_layer)\n",
    "    print('dropout rate: {}'.format(np.round(dropout_rate,2)))\n",
    "    print('l2 lambda: {0:.1e}'.format(l2_lambda))\n",
    "    print('class weight: {}, {}'.format(np.round(class_weight[0],1), np.round(class_weight[1],2)))\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = make_model(\n",
    "        output_bias,\n",
    "        learning_rate = learning_rate,\n",
    "        hidden_layers = hidden_layers,\n",
    "        neurons_per_layer = neurons_per_layer,\n",
    "        dropout_rate = dropout_rate,\n",
    "        l2_lambda = l2_lambda\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    print()\n",
    "\n",
    "    # Dir-name for the TensorBoard log-files.\n",
    "    log_dir = log_dir_name(\n",
    "        learning_rate,\n",
    "        hidden_layers,\n",
    "        neurons_per_layer,\n",
    "        dropout_rate,\n",
    "        l2_lambda,\n",
    "        class_0_weight,\n",
    "        class_1_weight\n",
    "    )\n",
    "    \n",
    "    # Create a callback-function for Keras which will be\n",
    "    # run after each epoch has ended during training.\n",
    "    # This saves the log-files for TensorBoard.\n",
    "    # Note that there are complications when histogram_freq=1.\n",
    "    # It might give strange errors and it also does not properly\n",
    "    # support Keras data-generators for the validation-set.\n",
    "    callback_log = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False\n",
    "    )\n",
    "   \n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        callbacks = [early_stopping],\n",
    "        validation_data=(val_features, val_labels),\n",
    "        validation_steps=VALIDATION_STEPS,\n",
    "        class_weight=class_weight,\n",
    "        workers=8\n",
    "    )\n",
    "\n",
    "    # Get fraction incorrect on the validation-set\n",
    "    # after the last training-epoch.\n",
    "          \n",
    "    val_fp = history.history['val_fp'][-1]\n",
    "    val_fn = history.history['val_fn'][-1]\n",
    "    val_tp = history.history['val_tp'][-1]\n",
    "    val_tn = history.history['val_tn'][-1]\n",
    "          \n",
    "    fraction_incorrect = (val_fn /(val_fn + val_tp + K.epsilon())) + (val_fp / (val_fp + val_tn + K.epsilon()))\n",
    "    \n",
    "    print()\n",
    "    print(\"Validation fraction incorrect: {0:.2}\".format(fraction_incorrect))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_fraction_incorrect\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if fraction_incorrect < best_fraction_incorrect:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_fraction_incorrect = fraction_incorrect\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return fraction_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-03\n",
      "hidden layers: 2\n",
      "neurons per layer: 30\n",
      "dropout rate: 0.5\n",
      "l2 lambda: 1.0e-01\n",
      "class weight: 0.5, 15\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                1230      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 3,121\n",
      "Trainable params: 3,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 33s - loss: 5.2422 - tp: 288.0000 - fp: 4558.0000 - tn: 38327.0000 - fn: 1327.0000 - accuracy: 0.8678 - precision: 0.0594 - recall: 0.1783 - auc: 0.5364 - f1: 0.0565 - val_loss: 0.3267 - val_tp: 784.0000 - val_fp: 6998.0000 - val_tn: 35874.0000 - val_fn: 844.0000 - val_accuracy: 0.8238 - val_precision: 0.1007 - val_recall: 0.4816 - val_auc: 0.7426 - val_f1: 0.1659Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 2.4589 - tp: 866.0000 - fp: 9214.0000 - tn: 33663.0000 - fn: 757.0000 - accuracy: 0.7759 - precision: 0.0859 - recall: 0.5336 - auc: 0.7240 - f1: 0.1466 - val_loss: 0.1634 - val_tp: 1108.0000 - val_fp: 9402.0000 - val_tn: 33470.0000 - val_fn: 520.0000 - val_accuracy: 0.7770 - val_precision: 0.1054 - val_recall: 0.6806 - val_auc: 0.8139 - val_f1: 0.1815Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 1.3503 - tp: 984.0000 - fp: 9218.0000 - tn: 33650.0000 - fn: 648.0000 - accuracy: 0.7783 - precision: 0.0965 - recall: 0.6029 - auc: 0.7696 - f1: 0.1650 - val_loss: 0.0974 - val_tp: 1164.0000 - val_fp: 9085.0000 - val_tn: 33787.0000 - val_fn: 464.0000 - val_accuracy: 0.7854 - val_precision: 0.1136 - val_recall: 0.7150 - val_auc: 0.8344 - val_f1: 0.1948Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.9248 - tp: 1075.0000 - fp: 10050.0000 - tn: 32782.0000 - fn: 593.0000 - accuracy: 0.7608 - precision: 0.0966 - recall: 0.6445 - auc: 0.7775 - f1: 0.1677 - val_loss: 0.0725 - val_tp: 1123.0000 - val_fp: 7987.0000 - val_tn: 34885.0000 - val_fn: 505.0000 - val_accuracy: 0.8092 - val_precision: 0.1233 - val_recall: 0.6898 - val_auc: 0.8406 - val_f1: 0.2079Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.7453 - tp: 1049.0000 - fp: 9467.0000 - tn: 33393.0000 - fn: 591.0000 - accuracy: 0.7740 - precision: 0.0998 - recall: 0.6396 - auc: 0.7906 - f1: 0.1721 - val_loss: 0.0627 - val_tp: 1214.0000 - val_fp: 9676.0000 - val_tn: 33196.0000 - val_fn: 414.0000 - val_accuracy: 0.7733 - val_precision: 0.1115 - val_recall: 0.7457 - val_auc: 0.8445 - val_f1: 0.1928Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6748 - tp: 1040.0000 - fp: 9305.0000 - tn: 33593.0000 - fn: 562.0000 - accuracy: 0.7783 - precision: 0.1005 - recall: 0.6492 - auc: 0.7939 - f1: 0.1731 - val_loss: 0.0590 - val_tp: 1218.0000 - val_fp: 9346.0000 - val_tn: 33526.0000 - val_fn: 410.0000 - val_accuracy: 0.7808 - val_precision: 0.1153 - val_recall: 0.7482 - val_auc: 0.8481 - val_f1: 0.1988Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6614 - tp: 1165.0000 - fp: 9738.0000 - tn: 33056.0000 - fn: 541.0000 - accuracy: 0.7690 - precision: 0.1069 - recall: 0.6829 - auc: 0.8032 - f1: 0.1841 - val_loss: 0.0575 - val_tp: 1208.0000 - val_fp: 8956.0000 - val_tn: 33916.0000 - val_fn: 420.0000 - val_accuracy: 0.7893 - val_precision: 0.1189 - val_recall: 0.7420 - val_auc: 0.8495 - val_f1: 0.2038Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6502 - tp: 1045.0000 - fp: 9457.0000 - tn: 33436.0000 - fn: 562.0000 - accuracy: 0.7749 - precision: 0.0995 - recall: 0.6503 - auc: 0.7927 - f1: 0.1716 - val_loss: 0.0563 - val_tp: 1236.0000 - val_fp: 9558.0000 - val_tn: 33314.0000 - val_fn: 392.0000 - val_accuracy: 0.7764 - val_precision: 0.1145 - val_recall: 0.7592 - val_auc: 0.8506 - val_f1: 0.1980Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.6323 - tp: 1101.0000 - fp: 9512.0000 - tn: 33383.0000 - fn: 504.0000 - accuracy: 0.7749 - precision: 0.1037 - recall: 0.6860 - auc: 0.7999 - f1: 0.1795 - val_loss: 0.0560 - val_tp: 1170.0000 - val_fp: 7989.0000 - val_tn: 34883.0000 - val_fn: 458.0000 - val_accuracy: 0.8102 - val_precision: 0.1277 - val_recall: 0.7187 - val_auc: 0.8536 - val_f1: 0.2154Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6211 - tp: 1137.0000 - fp: 9350.0000 - tn: 33501.0000 - fn: 512.0000 - accuracy: 0.7784 - precision: 0.1084 - recall: 0.6895 - auc: 0.8150 - f1: 0.1867 - val_loss: 0.0553 - val_tp: 1216.0000 - val_fp: 8689.0000 - val_tn: 34183.0000 - val_fn: 412.0000 - val_accuracy: 0.7955 - val_precision: 0.1228 - val_recall: 0.7469 - val_auc: 0.8557 - val_f1: 0.2095Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 9s - loss: 0.6216 - tp: 1066.0000 - fp: 9013.0000 - tn: 33665.0000 - fn: 514.0000 - accuracy: 0.7847 - precision: 0.1058 - recall: 0.6747 - auc: 0.8050 - f1: 0.1807 - val_loss: 0.0545 - val_tp: 1234.0000 - val_fp: 8916.0000 - val_tn: 33956.0000 - val_fn: 394.0000 - val_accuracy: 0.7908 - val_precision: 0.1216 - val_recall: 0.7580 - val_auc: 0.8572 - val_f1: 0.2080Epoch 12/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.6104 - tp: 1163.0000 - fp: 9439.0000 - tn: 33406.0000 - fn: 492.0000 - accuracy: 0.7768 - precision: 0.1097 - recall: 0.7027 - auc: 0.8194 - f1: 0.1896 - val_loss: 0.0543 - val_tp: 1221.0000 - val_fp: 8425.0000 - val_tn: 34447.0000 - val_fn: 407.0000 - val_accuracy: 0.8015 - val_precision: 0.1266 - val_recall: 0.7500 - val_auc: 0.8588 - val_f1: 0.2152Epoch 13/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.6132 - tp: 1145.0000 - fp: 9276.0000 - tn: 33567.0000 - fn: 512.0000 - accuracy: 0.7800 - precision: 0.1099 - recall: 0.6910 - auc: 0.8170 - f1: 0.1895 - val_loss: 0.0541 - val_tp: 1195.0000 - val_fp: 7898.0000 - val_tn: 34974.0000 - val_fn: 433.0000 - val_accuracy: 0.8128 - val_precision: 0.1314 - val_recall: 0.7340 - val_auc: 0.8592 - val_f1: 0.2210Epoch 14/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.6081 - tp: 1111.0000 - fp: 9226.0000 - tn: 33652.0000 - fn: 511.0000 - accuracy: 0.7812 - precision: 0.1075 - recall: 0.6850 - auc: 0.8151 - f1: 0.1850 - val_loss: 0.0534 - val_tp: 1304.0000 - val_fp: 10529.0000 - val_tn: 32343.0000 - val_fn: 324.0000 - val_accuracy: 0.7561 - val_precision: 0.1102 - val_recall: 0.8010 - val_auc: 0.8603 - val_f1: 0.1926Epoch 15/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6067 - tp: 1136.0000 - fp: 9475.0000 - tn: 33398.0000 - fn: 491.0000 - accuracy: 0.7760 - precision: 0.1071 - recall: 0.6982 - auc: 0.8162 - f1: 0.1855 - val_loss: 0.0530 - val_tp: 1283.0000 - val_fp: 9528.0000 - val_tn: 33344.0000 - val_fn: 345.0000 - val_accuracy: 0.7781 - val_precision: 0.1187 - val_recall: 0.7881 - val_auc: 0.8610 - val_f1: 0.2050Epoch 16/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.5768 - tp: 1142.0000 - fp: 8959.0000 - tn: 33948.0000 - fn: 451.0000 - accuracy: 0.7885 - precision: 0.1131 - recall: 0.7169 - auc: 0.8325 - f1: 0.1946 - val_loss: 0.0528 - val_tp: 1228.0000 - val_fp: 8349.0000 - val_tn: 34523.0000 - val_fn: 400.0000 - val_accuracy: 0.8034 - val_precision: 0.1282 - val_recall: 0.7543 - val_auc: 0.8617 - val_f1: 0.2174Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.5978 - tp: 1126.0000 - fp: 9023.0000 - tn: 33848.0000 - fn: 503.0000 - accuracy: 0.7859 - precision: 0.1109 - recall: 0.6912 - auc: 0.8188 - f1: 0.1909 - val_loss: 0.0524 - val_tp: 1239.0000 - val_fp: 8554.0000 - val_tn: 34318.0000 - val_fn: 389.0000 - val_accuracy: 0.7990 - val_precision: 0.1265 - val_recall: 0.7611 - val_auc: 0.8650 - val_f1: 0.2150Epoch 18/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.5855 - tp: 1156.0000 - fp: 9143.0000 - tn: 33723.0000 - fn: 478.0000 - accuracy: 0.7838 - precision: 0.1122 - recall: 0.7075 - auc: 0.8280 - f1: 0.1922 - val_loss: 0.0522 - val_tp: 1262.0000 - val_fp: 9056.0000 - val_tn: 33816.0000 - val_fn: 366.0000 - val_accuracy: 0.7883 - val_precision: 0.1223 - val_recall: 0.7752 - val_auc: 0.8641 - val_f1: 0.2096Epoch 19/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.5915 - tp: 1220.0000 - fp: 9255.0000 - tn: 33563.0000 - fn: 462.0000 - accuracy: 0.7816 - precision: 0.1165 - recall: 0.7253 - auc: 0.8258 - f1: 0.2006 - val_loss: 0.0520 - val_tp: 1223.0000 - val_fp: 8246.0000 - val_tn: 34626.0000 - val_fn: 405.0000 - val_accuracy: 0.8056 - val_precision: 0.1292 - val_recall: 0.7512 - val_auc: 0.8663 - val_f1: 0.2185Epoch 20/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.5672 - tp: 1206.0000 - fp: 8805.0000 - tn: 34032.0000 - fn: 457.0000 - accuracy: 0.7919 - precision: 0.1205 - recall: 0.7252 - auc: 0.8440 - f1: 0.2062 - val_loss: 0.0521 - val_tp: 1349.0000 - val_fp: 11376.0000 - val_tn: 31496.0000 - val_fn: 279.0000 - val_accuracy: 0.7381 - val_precision: 0.1060 - val_recall: 0.8286 - val_auc: 0.8654 - val_f1: 0.1869\n",
      "Validation fraction incorrect: 0.44\n",
      "\n",
      "learning rate: 1.5e-02\n",
      "hidden layers: 7\n",
      "neurons per layer: 158\n",
      "dropout rate: 0.3\n",
      "l2 lambda: 4.5e-01\n",
      "class weight: 1.1, 11.89\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 158)               6478      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 158)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 159       \n",
      "=================================================================\n",
      "Total params: 182,491\n",
      "Trainable params: 182,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 54s - loss: 23.4607 - tp: 3.0000 - fp: 196.0000 - tn: 42689.0000 - fn: 1612.0000 - accuracy: 0.9594 - precision: 0.0151 - recall: 0.0019 - auc: 0.5026 - f1: 0.0014 - val_loss: 0.0912 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9133 - tp: 0.0000e+00 - fp: 5.0000 - tn: 42872.0000 - fn: 1623.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4972 - f1: 0.0000e+00 - val_loss: 0.0892 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9118 - tp: 1.0000 - fp: 1.0000 - tn: 42867.0000 - fn: 1631.0000 - accuracy: 0.9633 - precision: 0.5000 - recall: 6.1275e-04 - auc: 0.5014 - f1: 0.0012 - val_loss: 0.0893 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9213 - tp: 1.0000 - fp: 10.0000 - tn: 42822.0000 - fn: 1667.0000 - accuracy: 0.9623 - precision: 0.0909 - recall: 5.9952e-04 - auc: 0.5019 - f1: 9.7704e-04 - val_loss: 0.0890 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9113 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4923 - f1: 0.0000e+00 - val_loss: 0.0892 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.8979 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5002 - f1: 0.0000e+00 - val_loss: 0.0890 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9350 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4903 - f1: 0.0000e+00 - val_loss: 0.0899 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9047 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4787 - f1: 0.0000e+00 - val_loss: 0.0893 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.8957 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5036 - f1: 0.0000e+00 - val_loss: 0.0891 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9154 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4909 - f1: 0.0000e+00 - val_loss: 0.0894 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44258/449258 [=>............................] - ETA: 19s - loss: 0.8921 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4994 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 27s - loss: 0.8921 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4994 - f1: 0.0000e+00 - val_loss: 0.0891 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 2.2e-04\n",
      "hidden layers: 6\n",
      "neurons per layer: 134\n",
      "dropout rate: 0.04\n",
      "l2 lambda: 7.2e-01\n",
      "class weight: 9.4, 5.01\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 134)               5494      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 134)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 135       \n",
      "=================================================================\n",
      "Total params: 114,169\n",
      "Trainable params: 114,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 49s - loss: 481.9513 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5279 - f1: 0.0000e+00 - val_loss: 38.7805 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5151 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 322.5152 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5487 - f1: 0.0000e+00 - val_loss: 25.7737 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5107 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 212.8276 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5054 - f1: 0.0000e+00 - val_loss: 16.8554 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 138.0225 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5000 - f1: 0.0000e+00 - val_loss: 10.8161 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 87.7363 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5031 - f1: 0.0000e+00 - val_loss: 6.7979 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 22s - loss: 54.5874 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5041 - f1: 0.0000e+00 - val_loss: 4.1808 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 22s - loss: 33.2886 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4835 - f1: 0.0000e+00 - val_loss: 2.5174 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 19.8429 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5035 - f1: 0.0000e+00 - val_loss: 1.4885 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 11.6607 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4983 - f1: 0.0000e+00 - val_loss: 0.8706 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 6.8365 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4981 - f1: 0.0000e+00 - val_loss: 0.5112 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 43758/449258 [=>............................] - ETA: 15s - loss: 4.0403 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42197.0000 - fn: 1561.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5078 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 21s - loss: 4.0304 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5072 - f1: 0.0000e+00 - val_loss: 0.3092 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 9.3e-02\n",
      "hidden layers: 19\n",
      "neurons per layer: 126\n",
      "dropout rate: 0.01\n",
      "l2 lambda: 2.3e-02\n",
      "class weight: 5.3, 11.0\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 126)               5166      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 127       \n",
      "=================================================================\n",
      "Total params: 309,331\n",
      "Trainable params: 309,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:29 - loss: 4.3858 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5088 - f1: 0.0000e+00 - val_loss: 0.1429 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4411 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4876 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4433 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4994 - f1: 0.0000e+00 - val_loss: 0.1426 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4676 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4977 - f1: 0.0000e+00 - val_loss: 0.1428 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4484 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4983 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4244 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5035 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4915 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4869 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4276 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5014 - f1: 0.0000e+00 - val_loss: 0.1426 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4267 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4993 - f1: 0.0000e+00 - val_loss: 0.1428 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4556 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4930 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 38s - loss: 1.4162 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5003 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 52s - loss: 1.4162 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5003 - f1: 0.0000e+00 - val_loss: 0.1428 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.5e-05\n",
      "hidden layers: 29\n",
      "neurons per layer: 54\n",
      "dropout rate: 0.05\n",
      "l2 lambda: 6.2e-01\n",
      "class weight: 3.9, 19.75\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 54)                2214      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 54)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 55        \n",
      "=================================================================\n",
      "Total params: 88,399\n",
      "Trainable params: 88,399\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:21 - loss: 956.6409 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4939 - f1: 0.0000e+00 - val_loss: 93.9256 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 940.0966 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4977 - f1: 0.0000e+00 - val_loss: 92.2998 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 40s - loss: 923.8326 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4999 - f1: 0.0000e+00 - val_loss: 90.7012 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 40s - loss: 907.8732 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5070 - f1: 0.0000e+00 - val_loss: 89.1288 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 892.0892 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4964 - f1: 0.0000e+00 - val_loss: 87.5818 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 13500/449258 [..............................] - ETA: 30s - loss: 881.8188 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 13033.0000 - fn: 467.0000 - accuracy: 0.9654 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4949 - f1: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(\n",
    "    func=fitness,\n",
    "    dimensions=dimensions,\n",
    "    acq_func='EI', # Expected Improvement.\n",
    "    n_calls=20,\n",
    "    x0=default_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_names = [\n",
    "    'learning_rate',\n",
    "    'hidden_layers',\n",
    "    'neurons_per_layer',\n",
    "    'dropout_rate',\n",
    "    'l2_lambda',\n",
    "    'class_0_weight',\n",
    "    'class_1_weight'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_objective(result=search_result, dimension_names=dim_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = search_result.space\n",
    "winning_hyperparams = space.point_to_dict(search_result.x)\n",
    "winning_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.001, 2, 30, 0.5, 0.1, 0.5, 15 defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = winning_hyperparams['learning_rate'] / 10\n",
    "#learning_rate = 0.0001\n",
    "hidden_layers = winning_hyperparams['hidden_layers']\n",
    "units_per_layer = winning_hyperparams['neurons_per_layer']\n",
    "dropout_rate = winning_hyperparams['dropout_rate']\n",
    "l2_lambda = winning_hyperparams['l2_lambda']\n",
    "#l2_lambda = 0.005\n",
    "class_0_weight = winning_hyperparams['class_0_weight']\n",
    "#class_0_weight = 0.6\n",
    "class_1_weight = winning_hyperparams['class_1_weight']\n",
    "\n",
    "initial_bias = np.log([ignition_count/no_ignition_count])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "    \n",
    "class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 100000\n",
    "STEPS_PER_EPOCH = (len(training_data) * 1) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(validation_data) * 1) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.layers.Dense(\n",
    "    units_per_layer, \n",
    "    activation = 'relu', \n",
    "    input_dim = train_features.shape[-1],\n",
    "))\n",
    "\n",
    "# Add fully connected hidden layers\n",
    "for i in range(hidden_layers):\n",
    "    model.add(keras.layers.Dense(\n",
    "        units_per_layer,\n",
    "        bias_initializer=keras.initializers.VarianceScaling(\n",
    "            scale=1.0,\n",
    "            mode='fan_in', \n",
    "            distribution='normal', \n",
    "            seed=None\n",
    "        ),\n",
    "        kernel_regularizer=keras.regularizers.l2(l2_lambda),\n",
    "        activation = 'relu')\n",
    "    )\n",
    "\n",
    "# Add dropout layer\n",
    "model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Add output layer\n",
    "model.add(keras.layers.Dense(\n",
    "    1, \n",
    "    activation = 'sigmoid', \n",
    "    bias_initializer = output_bias\n",
    "))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    class_weight=class_weight,\n",
    "    workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_features, batch_size=len(train_features))\n",
    "test_predictions = model.predict(test_features, batch_size=len(test_features))\n",
    "\n",
    "results = model.evaluate(\n",
    "    test_features,\n",
    "    test_labels,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(name, ': ', value)\n",
    "\n",
    "plot_cm(test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\"Train\", train_labels, train_predictions, color=colors[0])\n",
    "plot_roc(\"Test\", test_labels, test_predictions, color=colors[0], linestyle='--')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
