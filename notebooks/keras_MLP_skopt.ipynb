{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "#from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_objective_2D #, plot_histogram\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dir_name(\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda,\n",
    "    class_0_weight,\n",
    "    class_1_weight\n",
    "):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./MLP_logs/hidden_layers{0}_neurons_per_layer{1}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(\n",
    "        learning_rate,\n",
    "        hidden_layers,\n",
    "        neurons_per_layer,\n",
    "        dropout_rate,\n",
    "        l2_lambda,\n",
    "        class_0_weight,\n",
    "        class_1_weight\n",
    "    )\n",
    "\n",
    "    return log_dir\n",
    "\n",
    "\n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'f1']\n",
    "    \n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(3,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        \n",
    "    if metric == 'loss':\n",
    "        plt.ylim([0, plt.ylim()[1]])\n",
    "        \n",
    "    elif metric == 'auc':\n",
    "        plt.ylim([0.8,1])\n",
    "        \n",
    "    else:\n",
    "        #plt.ylim([0,1])\n",
    "        plt.legend()\n",
    "        \n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    normalized_cm = np.empty([2, 2])\n",
    "    normalized_cm[0][0] = cm[0][0] / (cm[0][0] + cm[0][1])\n",
    "    normalized_cm[0][1] = cm[0][1] / (cm[0][0] + cm[0][1])\n",
    "    normalized_cm[1][0] = cm[1][0] / (cm[1][0] + cm[1][1])\n",
    "    normalized_cm[1][1] = cm[1][1] / (cm[1][0] + cm[1][1])\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(normalized_cm, annot=True)\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('No fire (True Negatives): ', cm[0][0])\n",
    "    print('False alarms (False Positives): ', cm[0][1])\n",
    "    print('Fires missed (False Negatives): ', cm[1][0])\n",
    "    print('Fires detected (True Positives): ', cm[1][1])\n",
    "    print('Total fires: ', np.sum(cm[1]))\n",
    "    \n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    #plt.xlim([-0.5,20])\n",
    "    #plt.ylim([80,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "def f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/stratified_training_data/1992-2015_training_data_added_features_n500000_ks_pval0.3.1.csv'\n",
    "validation_file = '../data/stratified_training_data/1992-2015_training_data_added_features_n500000_ks_pval0.3.2.csv'\n",
    "test_file = '../data/stratified_training_data/1992-2015_training_data_added_features_n500000_ks_pval0.3.3.csv'\n",
    "\n",
    "# Datatypes for dataframe loading\n",
    "dtypes = {\n",
    "    'lat': float,\n",
    "    'lon': float,\n",
    "    'weather_bin_year': int,\n",
    "    'weather_bin_month': int,\n",
    "    'weather_bin_day': int,\n",
    "    'air.2m': float,\n",
    "    'apcp': float,\n",
    "    'rhum.2m': float,\n",
    "    'dpt.2m': float,\n",
    "    'pres.sfc': float,\n",
    "    'uwnd.10m': float,\n",
    "    'vwnd.10m': float,\n",
    "    'veg': float,\n",
    "    'vis': float,\n",
    "    'ignition': float,\n",
    "    'mean.air.2m': float,\n",
    "    'mean.apcp': float,\n",
    "    'mean.rhum.2m': float,\n",
    "    'mean.dpt.2m': float,\n",
    "    'mean.pres.sfc': float,\n",
    "    'mean.uwnd.10m': float,\n",
    "    'mean.vwnd.10m': float,\n",
    "    'mean.veg': float,\n",
    "    'mean.vis': float,\n",
    "    'max.air.2m': float,\n",
    "    'max.apcp': float,\n",
    "    'max.rhum.2m': float,\n",
    "    'max.dpt.2m': float,\n",
    "    'max.pres.sfc': float,\n",
    "    'max.uwnd.10m': float,\n",
    "    'max.vwnd.10m': float,\n",
    "    'max.veg': float,\n",
    "    'max.vis': float,\n",
    "    'min.air.2m': float,\n",
    "    'min.apcp': float,\n",
    "    'min.rhum.2m': float,\n",
    "    'min.dpt.2m': float,\n",
    "    'min.pres.sfc': float,\n",
    "    'min.uwnd.10m': float,\n",
    "    'min.vwnd.10m': float,\n",
    "    'min.veg': float,\n",
    "    'min.vis': float,\n",
    "    'total_fires': float\n",
    "\n",
    "}\n",
    "\n",
    "# Features to use during training \n",
    "features = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'weather_bin_month',\n",
    "    'veg',\n",
    "    'ignition',\n",
    "    'mean.air.2m',\n",
    "    'mean.apcp',\n",
    "    'mean.rhum.2m',\n",
    "    'mean.dpt.2m',\n",
    "    'mean.pres.sfc',\n",
    "    'mean.uwnd.10m',\n",
    "    'mean.vwnd.10m',\n",
    "    'mean.vis',\n",
    "    'max.air.2m',\n",
    "    'max.apcp',\n",
    "    'max.rhum.2m',\n",
    "    'max.dpt.2m',\n",
    "    'max.pres.sfc',\n",
    "    'max.uwnd.10m',\n",
    "    'max.vwnd.10m',\n",
    "    'max.vis',\n",
    "    'min.air.2m',\n",
    "    'min.apcp',\n",
    "    'min.rhum.2m',\n",
    "    'min.dpt.2m',\n",
    "    'min.pres.sfc',\n",
    "    'min.uwnd.10m',\n",
    "    'min.vwnd.10m',\n",
    "    'min.vis',\n",
    "    'total_fires'\n",
    "]\n",
    "\n",
    "features_to_scale = [\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'veg',\n",
    "    'mean.air.2m',\n",
    "    'mean.apcp',\n",
    "    'mean.rhum.2m',\n",
    "    'mean.dpt.2m',\n",
    "    'mean.pres.sfc',\n",
    "    'mean.uwnd.10m',\n",
    "    'mean.vwnd.10m',\n",
    "    'mean.vis',\n",
    "    'max.air.2m',\n",
    "    'max.apcp',\n",
    "    'max.rhum.2m',\n",
    "    'max.dpt.2m',\n",
    "    'max.pres.sfc',\n",
    "    'max.uwnd.10m',\n",
    "    'max.vwnd.10m',\n",
    "    'max.vis',\n",
    "    'min.air.2m',\n",
    "    'min.apcp',\n",
    "    'min.rhum.2m',\n",
    "    'min.dpt.2m',\n",
    "    'min.pres.sfc',\n",
    "    'min.uwnd.10m',\n",
    "    'min.vwnd.10m',\n",
    "    'min.vis',\n",
    "    'total_fires'\n",
    "]\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'), \n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    f1\n",
    "]\n",
    "\n",
    "dim_learning_rate = Real(\n",
    "    low=1e-5, \n",
    "    high=1e-1, \n",
    "    prior='log-uniform',\n",
    "    name='learning_rate'\n",
    ")\n",
    "\n",
    "dim_hidden_layers = Integer(\n",
    "    low=2,\n",
    "    high=30, \n",
    "    name='hidden_layers'\n",
    ")\n",
    "\n",
    "dim_neurons_per_layer = Integer(\n",
    "    low=10, \n",
    "    high=200, \n",
    "    name='neurons_per_layer'\n",
    ")\n",
    "\n",
    "dim_dropout_rate = Real(\n",
    "    low=0.01, \n",
    "    high=0.5, \n",
    "    name='dropout_rate'\n",
    ")\n",
    "\n",
    "dim_l2_lambda = Real(\n",
    "    low=0.0001, \n",
    "    high=1, \n",
    "    name='l2_lambda'\n",
    ")\n",
    "\n",
    "dim_class_0_weight = Real(\n",
    "    low=0.1,\n",
    "    high=10,\n",
    "    name='class_0_weight'\n",
    ")\n",
    "\n",
    "dim_class_1_weight = Real(\n",
    "    low=5,\n",
    "    high=20,\n",
    "    name=\"class_1_weight\"\n",
    ")\n",
    "\n",
    "dimensions = [\n",
    "    dim_learning_rate,\n",
    "    dim_hidden_layers,\n",
    "    dim_neurons_per_layer,\n",
    "    dim_dropout_rate,\n",
    "    dim_l2_lambda,\n",
    "    dim_class_0_weight,\n",
    "    dim_class_1_weight\n",
    "]\n",
    "\n",
    "default_parameters = [0.001, 2, 30, 0.5, 0.1, 0.5, 15]\n",
    "\n",
    "path_best_model = '../trained_models/best_skopt_MLP.keras'\n",
    "best_fraction_incorrect = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file, dtype=dtypes)\n",
    "validation = pd.read_csv(validation_file, dtype=dtypes)\n",
    "test = pd.read_csv(test_file, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "training_data = data[features]\n",
    "validation_data = validation[features]\n",
    "test_data = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/siderealyear/anaconda3/envs/wildfire/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# One hot encode month\n",
    "\n",
    "column_names = [\n",
    "    'January',\n",
    "    'February',\n",
    "    'March',\n",
    "    'April',\n",
    "    'May',\n",
    "    'June',\n",
    "    'July',\n",
    "    'August',\n",
    "    'Septermber',\n",
    "    'October',\n",
    "    'November',\n",
    "    'December'\n",
    "]\n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Training data\n",
    "month = np.array(training_data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "training_data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "training_data = pd.concat([training_data, onehot_month_df], axis=1)\n",
    "\n",
    "# Validation data\n",
    "month = np.array(validation_data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "validation_data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "validation_data = pd.concat([validation_data, onehot_month_df], axis=1)\n",
    "\n",
    "# Test data data\n",
    "month = np.array(test_data['weather_bin_month']).reshape(-1, 1)\n",
    "onehot_month = onehot_encoder.fit_transform(month)\n",
    "\n",
    "test_data.drop('weather_bin_month', axis=1, inplace=True)\n",
    "onehot_month_df = pd.DataFrame(onehot_month, columns=column_names)\n",
    "test_data = pd.concat([test_data, onehot_month_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>veg</th>\n",
       "      <th>ignition</th>\n",
       "      <th>mean.air.2m</th>\n",
       "      <th>mean.apcp</th>\n",
       "      <th>mean.rhum.2m</th>\n",
       "      <th>mean.dpt.2m</th>\n",
       "      <th>mean.pres.sfc</th>\n",
       "      <th>mean.uwnd.10m</th>\n",
       "      <th>...</th>\n",
       "      <th>March</th>\n",
       "      <th>April</th>\n",
       "      <th>May</th>\n",
       "      <th>June</th>\n",
       "      <th>July</th>\n",
       "      <th>August</th>\n",
       "      <th>Septermber</th>\n",
       "      <th>October</th>\n",
       "      <th>November</th>\n",
       "      <th>December</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>34.18678</td>\n",
       "      <td>-118.8088</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.170681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.576653</td>\n",
       "      <td>279.735122</td>\n",
       "      <td>96650.870000</td>\n",
       "      <td>0.064065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37.03086</td>\n",
       "      <td>-119.0176</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.386004</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>33.640771</td>\n",
       "      <td>277.393281</td>\n",
       "      <td>79824.146797</td>\n",
       "      <td>-0.407725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37.49919</td>\n",
       "      <td>-119.8433</td>\n",
       "      <td>33.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.263178</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>60.458148</td>\n",
       "      <td>279.509951</td>\n",
       "      <td>89032.984219</td>\n",
       "      <td>-0.223786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>36.23206</td>\n",
       "      <td>-118.5007</td>\n",
       "      <td>46.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.102028</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>45.363947</td>\n",
       "      <td>256.344916</td>\n",
       "      <td>76906.623375</td>\n",
       "      <td>0.438386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>32.91990</td>\n",
       "      <td>-114.8649</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.117580</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>26.899587</td>\n",
       "      <td>269.320998</td>\n",
       "      <td>99661.673406</td>\n",
       "      <td>1.417608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       lon   veg  ignition  mean.air.2m  mean.apcp  mean.rhum.2m  \\\n",
       "0  34.18678 -118.8088  25.5       0.0   289.170681   0.000000     57.576653   \n",
       "1  37.03086 -119.0176  43.3       0.0   294.386004   0.000122     33.640771   \n",
       "2  37.49919 -119.8433  33.9       0.0   287.263178   0.139648     60.458148   \n",
       "3  36.23206 -118.5007  46.9       0.0   267.102028   0.000318     45.363947   \n",
       "4  32.91990 -114.8649   3.5       0.0   289.117580   0.000506     26.899587   \n",
       "\n",
       "   mean.dpt.2m  mean.pres.sfc  mean.uwnd.10m  ...  March  April  May  June  \\\n",
       "0   279.735122   96650.870000       0.064065  ...    0.0    0.0  0.0   0.0   \n",
       "1   277.393281   79824.146797      -0.407725  ...    0.0    0.0  0.0   0.0   \n",
       "2   279.509951   89032.984219      -0.223786  ...    0.0    1.0  0.0   0.0   \n",
       "3   256.344916   76906.623375       0.438386  ...    0.0    0.0  0.0   0.0   \n",
       "4   269.320998   99661.673406       1.417608  ...    0.0    0.0  0.0   0.0   \n",
       "\n",
       "   July  August  Septermber  October  November  December  \n",
       "0   0.0     0.0         0.0      0.0       0.0       0.0  \n",
       "1   0.0     1.0         0.0      0.0       0.0       0.0  \n",
       "2   0.0     0.0         0.0      0.0       0.0       0.0  \n",
       "3   0.0     0.0         0.0      0.0       0.0       1.0  \n",
       "4   0.0     0.0         0.0      0.0       1.0       0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 449258 entries, 0 to 449257\n",
      "Data columns (total 41 columns):\n",
      "lat              449258 non-null float64\n",
      "lon              449258 non-null float64\n",
      "veg              449258 non-null float64\n",
      "ignition         449258 non-null float64\n",
      "mean.air.2m      449258 non-null float64\n",
      "mean.apcp        449258 non-null float64\n",
      "mean.rhum.2m     449258 non-null float64\n",
      "mean.dpt.2m      449258 non-null float64\n",
      "mean.pres.sfc    449258 non-null float64\n",
      "mean.uwnd.10m    449258 non-null float64\n",
      "mean.vwnd.10m    449258 non-null float64\n",
      "mean.vis         449258 non-null float64\n",
      "max.air.2m       449258 non-null float64\n",
      "max.apcp         449258 non-null float64\n",
      "max.rhum.2m      449258 non-null float64\n",
      "max.dpt.2m       449258 non-null float64\n",
      "max.pres.sfc     449258 non-null float64\n",
      "max.uwnd.10m     449258 non-null float64\n",
      "max.vwnd.10m     449258 non-null float64\n",
      "max.vis          449258 non-null float64\n",
      "min.air.2m       449258 non-null float64\n",
      "min.apcp         449258 non-null float64\n",
      "min.rhum.2m      449258 non-null float64\n",
      "min.dpt.2m       449258 non-null float64\n",
      "min.pres.sfc     449258 non-null float64\n",
      "min.uwnd.10m     449258 non-null float64\n",
      "min.vwnd.10m     449258 non-null float64\n",
      "min.vis          449258 non-null float64\n",
      "total_fires      449258 non-null float64\n",
      "January          449258 non-null float64\n",
      "February         449258 non-null float64\n",
      "March            449258 non-null float64\n",
      "April            449258 non-null float64\n",
      "May              449258 non-null float64\n",
      "June             449258 non-null float64\n",
      "July             449258 non-null float64\n",
      "August           449258 non-null float64\n",
      "Septermber       449258 non-null float64\n",
      "October          449258 non-null float64\n",
      "November         449258 non-null float64\n",
      "December         449258 non-null float64\n",
      "dtypes: float64(41)\n",
      "memory usage: 140.5 MB\n"
     ]
    }
   ],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(training_data.pop('ignition'))\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(validation_data.pop('ignition'))\n",
    "test_labels = np.array(test_data.pop('ignition'))\n",
    "\n",
    "train_features = np.array(training_data)\n",
    "val_features = np.array(validation_data)\n",
    "test_features = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (449258,)\n",
      "Validation labels shape: (449257,)\n",
      "Test labels shape: (449258,)\n",
      "Training features shape: (449258, 40)\n",
      "Validation features shape: (449257, 40)\n",
      "Test features shape: (449258, 40)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "# weight_for_0 = (1 / no_ignition_count)*(total)/2.0 \n",
    "# weight_for_1 = (1 / ignition_count)*(total)/2.0\n",
    "\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "# print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "# print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "\n",
    "ignition_count = sum(train_labels)\n",
    "no_ignition_count = len(train_labels) - ignition_count\n",
    "\n",
    "initial_bias = np.log([ignition_count/no_ignition_count])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 500\n",
    "STEPS_PER_EPOCH = (len(train_features) * 0.1) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(val_features) * 0.1) // BATCH_SIZE\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    output_bias,\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda\n",
    "):    \n",
    "    # Define the keras model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Add input layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        neurons_per_layer, \n",
    "        activation = 'relu', \n",
    "        input_dim = train_features.shape[-1],\n",
    "    ))\n",
    "\n",
    "    # Add fully connected hidden layers\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(keras.layers.Dense(\n",
    "            neurons_per_layer,\n",
    "            bias_initializer=keras.initializers.VarianceScaling(\n",
    "                scale=1.0,\n",
    "                mode='fan_in', \n",
    "                distribution='normal', \n",
    "                seed=None\n",
    "            ),\n",
    "            kernel_regularizer=keras.regularizers.l2(l2_lambda),\n",
    "            activation = 'relu')\n",
    "        )\n",
    "    \n",
    "    # Add dropout layer\n",
    "    model.add(keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        1, \n",
    "        activation = 'sigmoid', \n",
    "        bias_initializer = output_bias\n",
    "    ))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(\n",
    "    learning_rate,\n",
    "    hidden_layers,\n",
    "    neurons_per_layer,\n",
    "    dropout_rate,\n",
    "    l2_lambda,\n",
    "    class_0_weight,\n",
    "    class_1_weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "    \n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('hidden layers:', hidden_layers)\n",
    "    print('neurons per layer:', neurons_per_layer)\n",
    "    print('dropout rate: {}'.format(np.round(dropout_rate,2)))\n",
    "    print('l2 lambda: {0:.1e}'.format(l2_lambda))\n",
    "    print('class weight: {}, {}'.format(np.round(class_weight[0],1), np.round(class_weight[1],2)))\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = make_model(\n",
    "        output_bias,\n",
    "        learning_rate = learning_rate,\n",
    "        hidden_layers = hidden_layers,\n",
    "        neurons_per_layer = neurons_per_layer,\n",
    "        dropout_rate = dropout_rate,\n",
    "        l2_lambda = l2_lambda\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    print()\n",
    "\n",
    "    # Dir-name for the TensorBoard log-files.\n",
    "    log_dir = log_dir_name(\n",
    "        learning_rate,\n",
    "        hidden_layers,\n",
    "        neurons_per_layer,\n",
    "        dropout_rate,\n",
    "        l2_lambda,\n",
    "        class_0_weight,\n",
    "        class_1_weight\n",
    "    )\n",
    "    \n",
    "    # Create a callback-function for Keras which will be\n",
    "    # run after each epoch has ended during training.\n",
    "    # This saves the log-files for TensorBoard.\n",
    "    # Note that there are complications when histogram_freq=1.\n",
    "    # It might give strange errors and it also does not properly\n",
    "    # support Keras data-generators for the validation-set.\n",
    "    callback_log = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False\n",
    "    )\n",
    "   \n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        callbacks = [early_stopping],\n",
    "        validation_data=(val_features, val_labels),\n",
    "        validation_steps=VALIDATION_STEPS,\n",
    "        class_weight=class_weight,\n",
    "        workers=8\n",
    "    )\n",
    "\n",
    "    # Get fraction incorrect on the validation-set\n",
    "    # after the last training-epoch.\n",
    "          \n",
    "    val_fp = history.history['val_fp'][-1]\n",
    "    val_fn = history.history['val_fn'][-1]\n",
    "    val_tp = history.history['val_tp'][-1]\n",
    "    val_tn = history.history['val_tn'][-1]\n",
    "          \n",
    "    fraction_incorrect = (val_fn /(val_fn + val_tp + K.epsilon())) + (val_fp / (val_fp + val_tn + K.epsilon()))\n",
    "    \n",
    "    print()\n",
    "    print(\"Validation fraction incorrect: {0:.2}\".format(fraction_incorrect))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_fraction_incorrect\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if fraction_incorrect < best_fraction_incorrect:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_fraction_incorrect = fraction_incorrect\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return fraction_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-03\n",
      "hidden layers: 2\n",
      "neurons per layer: 30\n",
      "dropout rate: 0.5\n",
      "l2 lambda: 1.0e-01\n",
      "class weight: 0.5, 15\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                1230      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 3,121\n",
      "Trainable params: 3,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 33s - loss: 5.2422 - tp: 288.0000 - fp: 4558.0000 - tn: 38327.0000 - fn: 1327.0000 - accuracy: 0.8678 - precision: 0.0594 - recall: 0.1783 - auc: 0.5364 - f1: 0.0565 - val_loss: 0.3267 - val_tp: 784.0000 - val_fp: 6998.0000 - val_tn: 35874.0000 - val_fn: 844.0000 - val_accuracy: 0.8238 - val_precision: 0.1007 - val_recall: 0.4816 - val_auc: 0.7426 - val_f1: 0.1659Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 2.4589 - tp: 866.0000 - fp: 9214.0000 - tn: 33663.0000 - fn: 757.0000 - accuracy: 0.7759 - precision: 0.0859 - recall: 0.5336 - auc: 0.7240 - f1: 0.1466 - val_loss: 0.1634 - val_tp: 1108.0000 - val_fp: 9402.0000 - val_tn: 33470.0000 - val_fn: 520.0000 - val_accuracy: 0.7770 - val_precision: 0.1054 - val_recall: 0.6806 - val_auc: 0.8139 - val_f1: 0.1815Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 1.3503 - tp: 984.0000 - fp: 9218.0000 - tn: 33650.0000 - fn: 648.0000 - accuracy: 0.7783 - precision: 0.0965 - recall: 0.6029 - auc: 0.7696 - f1: 0.1650 - val_loss: 0.0974 - val_tp: 1164.0000 - val_fp: 9085.0000 - val_tn: 33787.0000 - val_fn: 464.0000 - val_accuracy: 0.7854 - val_precision: 0.1136 - val_recall: 0.7150 - val_auc: 0.8344 - val_f1: 0.1948Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.9248 - tp: 1075.0000 - fp: 10050.0000 - tn: 32782.0000 - fn: 593.0000 - accuracy: 0.7608 - precision: 0.0966 - recall: 0.6445 - auc: 0.7775 - f1: 0.1677 - val_loss: 0.0725 - val_tp: 1123.0000 - val_fp: 7987.0000 - val_tn: 34885.0000 - val_fn: 505.0000 - val_accuracy: 0.8092 - val_precision: 0.1233 - val_recall: 0.6898 - val_auc: 0.8406 - val_f1: 0.2079Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.7453 - tp: 1049.0000 - fp: 9467.0000 - tn: 33393.0000 - fn: 591.0000 - accuracy: 0.7740 - precision: 0.0998 - recall: 0.6396 - auc: 0.7906 - f1: 0.1721 - val_loss: 0.0627 - val_tp: 1214.0000 - val_fp: 9676.0000 - val_tn: 33196.0000 - val_fn: 414.0000 - val_accuracy: 0.7733 - val_precision: 0.1115 - val_recall: 0.7457 - val_auc: 0.8445 - val_f1: 0.1928Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6748 - tp: 1040.0000 - fp: 9305.0000 - tn: 33593.0000 - fn: 562.0000 - accuracy: 0.7783 - precision: 0.1005 - recall: 0.6492 - auc: 0.7939 - f1: 0.1731 - val_loss: 0.0590 - val_tp: 1218.0000 - val_fp: 9346.0000 - val_tn: 33526.0000 - val_fn: 410.0000 - val_accuracy: 0.7808 - val_precision: 0.1153 - val_recall: 0.7482 - val_auc: 0.8481 - val_f1: 0.1988Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6614 - tp: 1165.0000 - fp: 9738.0000 - tn: 33056.0000 - fn: 541.0000 - accuracy: 0.7690 - precision: 0.1069 - recall: 0.6829 - auc: 0.8032 - f1: 0.1841 - val_loss: 0.0575 - val_tp: 1208.0000 - val_fp: 8956.0000 - val_tn: 33916.0000 - val_fn: 420.0000 - val_accuracy: 0.7893 - val_precision: 0.1189 - val_recall: 0.7420 - val_auc: 0.8495 - val_f1: 0.2038Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6502 - tp: 1045.0000 - fp: 9457.0000 - tn: 33436.0000 - fn: 562.0000 - accuracy: 0.7749 - precision: 0.0995 - recall: 0.6503 - auc: 0.7927 - f1: 0.1716 - val_loss: 0.0563 - val_tp: 1236.0000 - val_fp: 9558.0000 - val_tn: 33314.0000 - val_fn: 392.0000 - val_accuracy: 0.7764 - val_precision: 0.1145 - val_recall: 0.7592 - val_auc: 0.8506 - val_f1: 0.1980Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.6323 - tp: 1101.0000 - fp: 9512.0000 - tn: 33383.0000 - fn: 504.0000 - accuracy: 0.7749 - precision: 0.1037 - recall: 0.6860 - auc: 0.7999 - f1: 0.1795 - val_loss: 0.0560 - val_tp: 1170.0000 - val_fp: 7989.0000 - val_tn: 34883.0000 - val_fn: 458.0000 - val_accuracy: 0.8102 - val_precision: 0.1277 - val_recall: 0.7187 - val_auc: 0.8536 - val_f1: 0.2154Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6211 - tp: 1137.0000 - fp: 9350.0000 - tn: 33501.0000 - fn: 512.0000 - accuracy: 0.7784 - precision: 0.1084 - recall: 0.6895 - auc: 0.8150 - f1: 0.1867 - val_loss: 0.0553 - val_tp: 1216.0000 - val_fp: 8689.0000 - val_tn: 34183.0000 - val_fn: 412.0000 - val_accuracy: 0.7955 - val_precision: 0.1228 - val_recall: 0.7469 - val_auc: 0.8557 - val_f1: 0.2095Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 9s - loss: 0.6216 - tp: 1066.0000 - fp: 9013.0000 - tn: 33665.0000 - fn: 514.0000 - accuracy: 0.7847 - precision: 0.1058 - recall: 0.6747 - auc: 0.8050 - f1: 0.1807 - val_loss: 0.0545 - val_tp: 1234.0000 - val_fp: 8916.0000 - val_tn: 33956.0000 - val_fn: 394.0000 - val_accuracy: 0.7908 - val_precision: 0.1216 - val_recall: 0.7580 - val_auc: 0.8572 - val_f1: 0.2080Epoch 12/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.6104 - tp: 1163.0000 - fp: 9439.0000 - tn: 33406.0000 - fn: 492.0000 - accuracy: 0.7768 - precision: 0.1097 - recall: 0.7027 - auc: 0.8194 - f1: 0.1896 - val_loss: 0.0543 - val_tp: 1221.0000 - val_fp: 8425.0000 - val_tn: 34447.0000 - val_fn: 407.0000 - val_accuracy: 0.8015 - val_precision: 0.1266 - val_recall: 0.7500 - val_auc: 0.8588 - val_f1: 0.2152Epoch 13/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.6132 - tp: 1145.0000 - fp: 9276.0000 - tn: 33567.0000 - fn: 512.0000 - accuracy: 0.7800 - precision: 0.1099 - recall: 0.6910 - auc: 0.8170 - f1: 0.1895 - val_loss: 0.0541 - val_tp: 1195.0000 - val_fp: 7898.0000 - val_tn: 34974.0000 - val_fn: 433.0000 - val_accuracy: 0.8128 - val_precision: 0.1314 - val_recall: 0.7340 - val_auc: 0.8592 - val_f1: 0.2210Epoch 14/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.6081 - tp: 1111.0000 - fp: 9226.0000 - tn: 33652.0000 - fn: 511.0000 - accuracy: 0.7812 - precision: 0.1075 - recall: 0.6850 - auc: 0.8151 - f1: 0.1850 - val_loss: 0.0534 - val_tp: 1304.0000 - val_fp: 10529.0000 - val_tn: 32343.0000 - val_fn: 324.0000 - val_accuracy: 0.7561 - val_precision: 0.1102 - val_recall: 0.8010 - val_auc: 0.8603 - val_f1: 0.1926Epoch 15/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.6067 - tp: 1136.0000 - fp: 9475.0000 - tn: 33398.0000 - fn: 491.0000 - accuracy: 0.7760 - precision: 0.1071 - recall: 0.6982 - auc: 0.8162 - f1: 0.1855 - val_loss: 0.0530 - val_tp: 1283.0000 - val_fp: 9528.0000 - val_tn: 33344.0000 - val_fn: 345.0000 - val_accuracy: 0.7781 - val_precision: 0.1187 - val_recall: 0.7881 - val_auc: 0.8610 - val_f1: 0.2050Epoch 16/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.5768 - tp: 1142.0000 - fp: 8959.0000 - tn: 33948.0000 - fn: 451.0000 - accuracy: 0.7885 - precision: 0.1131 - recall: 0.7169 - auc: 0.8325 - f1: 0.1946 - val_loss: 0.0528 - val_tp: 1228.0000 - val_fp: 8349.0000 - val_tn: 34523.0000 - val_fn: 400.0000 - val_accuracy: 0.8034 - val_precision: 0.1282 - val_recall: 0.7543 - val_auc: 0.8617 - val_f1: 0.2174Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.5978 - tp: 1126.0000 - fp: 9023.0000 - tn: 33848.0000 - fn: 503.0000 - accuracy: 0.7859 - precision: 0.1109 - recall: 0.6912 - auc: 0.8188 - f1: 0.1909 - val_loss: 0.0524 - val_tp: 1239.0000 - val_fp: 8554.0000 - val_tn: 34318.0000 - val_fn: 389.0000 - val_accuracy: 0.7990 - val_precision: 0.1265 - val_recall: 0.7611 - val_auc: 0.8650 - val_f1: 0.2150Epoch 18/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.5855 - tp: 1156.0000 - fp: 9143.0000 - tn: 33723.0000 - fn: 478.0000 - accuracy: 0.7838 - precision: 0.1122 - recall: 0.7075 - auc: 0.8280 - f1: 0.1922 - val_loss: 0.0522 - val_tp: 1262.0000 - val_fp: 9056.0000 - val_tn: 33816.0000 - val_fn: 366.0000 - val_accuracy: 0.7883 - val_precision: 0.1223 - val_recall: 0.7752 - val_auc: 0.8641 - val_f1: 0.2096Epoch 19/20\n",
      " 44500/449258 [=>............................] - ETA: 10s - loss: 0.5915 - tp: 1220.0000 - fp: 9255.0000 - tn: 33563.0000 - fn: 462.0000 - accuracy: 0.7816 - precision: 0.1165 - recall: 0.7253 - auc: 0.8258 - f1: 0.2006 - val_loss: 0.0520 - val_tp: 1223.0000 - val_fp: 8246.0000 - val_tn: 34626.0000 - val_fn: 405.0000 - val_accuracy: 0.8056 - val_precision: 0.1292 - val_recall: 0.7512 - val_auc: 0.8663 - val_f1: 0.2185Epoch 20/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 0.5672 - tp: 1206.0000 - fp: 8805.0000 - tn: 34032.0000 - fn: 457.0000 - accuracy: 0.7919 - precision: 0.1205 - recall: 0.7252 - auc: 0.8440 - f1: 0.2062 - val_loss: 0.0521 - val_tp: 1349.0000 - val_fp: 11376.0000 - val_tn: 31496.0000 - val_fn: 279.0000 - val_accuracy: 0.7381 - val_precision: 0.1060 - val_recall: 0.8286 - val_auc: 0.8654 - val_f1: 0.1869\n",
      "Validation fraction incorrect: 0.44\n",
      "\n",
      "learning rate: 1.5e-02\n",
      "hidden layers: 7\n",
      "neurons per layer: 158\n",
      "dropout rate: 0.3\n",
      "l2 lambda: 4.5e-01\n",
      "class weight: 1.1, 11.89\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 158)               6478      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 158)               25122     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 158)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 159       \n",
      "=================================================================\n",
      "Total params: 182,491\n",
      "Trainable params: 182,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 54s - loss: 23.4607 - tp: 3.0000 - fp: 196.0000 - tn: 42689.0000 - fn: 1612.0000 - accuracy: 0.9594 - precision: 0.0151 - recall: 0.0019 - auc: 0.5026 - f1: 0.0014 - val_loss: 0.0912 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9133 - tp: 0.0000e+00 - fp: 5.0000 - tn: 42872.0000 - fn: 1623.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4972 - f1: 0.0000e+00 - val_loss: 0.0892 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9118 - tp: 1.0000 - fp: 1.0000 - tn: 42867.0000 - fn: 1631.0000 - accuracy: 0.9633 - precision: 0.5000 - recall: 6.1275e-04 - auc: 0.5014 - f1: 0.0012 - val_loss: 0.0893 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9213 - tp: 1.0000 - fp: 10.0000 - tn: 42822.0000 - fn: 1667.0000 - accuracy: 0.9623 - precision: 0.0909 - recall: 5.9952e-04 - auc: 0.5019 - f1: 9.7704e-04 - val_loss: 0.0890 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9113 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4923 - f1: 0.0000e+00 - val_loss: 0.0892 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.8979 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5002 - f1: 0.0000e+00 - val_loss: 0.0890 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9350 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4903 - f1: 0.0000e+00 - val_loss: 0.0899 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9047 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4787 - f1: 0.0000e+00 - val_loss: 0.0893 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.8957 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5036 - f1: 0.0000e+00 - val_loss: 0.0891 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.9154 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4909 - f1: 0.0000e+00 - val_loss: 0.0894 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44258/449258 [=>............................] - ETA: 19s - loss: 0.8921 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4994 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 27s - loss: 0.8921 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4994 - f1: 0.0000e+00 - val_loss: 0.0891 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 2.2e-04\n",
      "hidden layers: 6\n",
      "neurons per layer: 134\n",
      "dropout rate: 0.04\n",
      "l2 lambda: 7.2e-01\n",
      "class weight: 9.4, 5.01\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 134)               5494      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 134)               18090     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 134)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 135       \n",
      "=================================================================\n",
      "Total params: 114,169\n",
      "Trainable params: 114,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 49s - loss: 481.9513 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5279 - f1: 0.0000e+00 - val_loss: 38.7805 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5151 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 322.5152 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5487 - f1: 0.0000e+00 - val_loss: 25.7737 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5107 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 212.8276 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5054 - f1: 0.0000e+00 - val_loss: 16.8554 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 138.0225 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5000 - f1: 0.0000e+00 - val_loss: 10.8161 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 87.7363 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5031 - f1: 0.0000e+00 - val_loss: 6.7979 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 22s - loss: 54.5874 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5041 - f1: 0.0000e+00 - val_loss: 4.1808 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 22s - loss: 33.2886 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4835 - f1: 0.0000e+00 - val_loss: 2.5174 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 19.8429 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5035 - f1: 0.0000e+00 - val_loss: 1.4885 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 11.6607 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4983 - f1: 0.0000e+00 - val_loss: 0.8706 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 21s - loss: 6.8365 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4981 - f1: 0.0000e+00 - val_loss: 0.5112 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 43758/449258 [=>............................] - ETA: 15s - loss: 4.0403 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42197.0000 - fn: 1561.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5078 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 21s - loss: 4.0304 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5072 - f1: 0.0000e+00 - val_loss: 0.3092 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 9.3e-02\n",
      "hidden layers: 19\n",
      "neurons per layer: 126\n",
      "dropout rate: 0.01\n",
      "l2 lambda: 2.3e-02\n",
      "class weight: 5.3, 11.0\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 126)               5166      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 126)               16002     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 127       \n",
      "=================================================================\n",
      "Total params: 309,331\n",
      "Trainable params: 309,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:29 - loss: 4.3858 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5088 - f1: 0.0000e+00 - val_loss: 0.1429 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4411 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4876 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4433 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4994 - f1: 0.0000e+00 - val_loss: 0.1426 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4676 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4977 - f1: 0.0000e+00 - val_loss: 0.1428 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4484 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4983 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4244 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5035 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4915 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4869 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4276 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5014 - f1: 0.0000e+00 - val_loss: 0.1426 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4267 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4993 - f1: 0.0000e+00 - val_loss: 0.1428 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 52s - loss: 1.4556 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4930 - f1: 0.0000e+00 - val_loss: 0.1427 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 38s - loss: 1.4162 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5003 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 52s - loss: 1.4162 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5003 - f1: 0.0000e+00 - val_loss: 0.1428 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.5e-05\n",
      "hidden layers: 29\n",
      "neurons per layer: 54\n",
      "dropout rate: 0.05\n",
      "l2 lambda: 6.2e-01\n",
      "class weight: 3.9, 19.75\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 54)                2214      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 54)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 55        \n",
      "=================================================================\n",
      "Total params: 88,399\n",
      "Trainable params: 88,399\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:21 - loss: 956.6409 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4939 - f1: 0.0000e+00 - val_loss: 93.9256 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 940.0966 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4977 - f1: 0.0000e+00 - val_loss: 92.2998 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 40s - loss: 923.8326 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4999 - f1: 0.0000e+00 - val_loss: 90.7012 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 40s - loss: 907.8732 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5070 - f1: 0.0000e+00 - val_loss: 89.1288 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 892.0892 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4964 - f1: 0.0000e+00 - val_loss: 87.5818 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 876.5444 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4994 - f1: 0.0000e+00 - val_loss: 86.0598 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 861.4385 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4935 - f1: 0.0000e+00 - val_loss: 84.5618 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 846.3009 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5075 - f1: 0.0000e+00 - val_loss: 83.0876 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 40s - loss: 831.5306 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5007 - f1: 0.0000e+00 - val_loss: 81.6363 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 817.0495 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4926 - f1: 0.0000e+00 - val_loss: 80.2072 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 28s - loss: 802.6222 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5061 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 40s - loss: 802.6221 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5061 - f1: 0.0000e+00 - val_loss: 78.8002 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 7.4e-04\n",
      "hidden layers: 26\n",
      "neurons per layer: 139\n",
      "dropout rate: 0.23\n",
      "l2 lambda: 1.3e-02\n",
      "class weight: 9.4, 13.45\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 139)               5699      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 139)               19460     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 139)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 140       \n",
      "=================================================================\n",
      "Total params: 511,799\n",
      "Trainable params: 511,799\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:53 - loss: 27.7421 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5064 - f1: 0.0000e+00 - val_loss: 1.2642 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 1:12 - loss: 7.2073 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4975 - f1: 0.0000e+00 - val_loss: 0.3781 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 1:12 - loss: 2.7709 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5050 - f1: 0.0000e+00 - val_loss: 0.2162 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 1:12 - loss: 2.0728 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4970 - f1: 0.0000e+00 - val_loss: 0.1945 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 1:12 - loss: 1.9600 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5005 - f1: 0.0000e+00 - val_loss: 0.1924 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 1:13 - loss: 1.9187 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4986 - f1: 0.0000e+00 - val_loss: 0.1922 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 1:12 - loss: 2.0103 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4952 - f1: 0.0000e+00 - val_loss: 0.1923 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 1:12 - loss: 1.9224 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4950 - f1: 0.0000e+00 - val_loss: 0.1921 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 1:12 - loss: 1.9199 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5003 - f1: 0.0000e+00 - val_loss: 0.1922 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 1:13 - loss: 1.9598 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4908 - f1: 0.0000e+00 - val_loss: 0.1921 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 53s - loss: 1.9059 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4931 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 1:13 - loss: 1.9059 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4931 - f1: 0.0000e+00 - val_loss: 0.1923 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 3.5e-04\n",
      "hidden layers: 2\n",
      "neurons per layer: 54\n",
      "dropout rate: 0.13\n",
      "l2 lambda: 6.8e-01\n",
      "class weight: 6.1, 17.5\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 54)                2214      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 54)                2970      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 54)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 55        \n",
      "=================================================================\n",
      "Total params: 8,209\n",
      "Trainable params: 8,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 32s - loss: 63.0712 - tp: 0.0000e+00 - fp: 1.0000 - tn: 42884.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5966 - f1: 0.0000e+00 - val_loss: 5.0839 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7245 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 42.4223 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7429 - f1: 0.0000e+00 - val_loss: 3.4094 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7674 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 28.3897 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7664 - f1: 0.0000e+00 - val_loss: 2.2729 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7875 - val_f1: 0.0000e+00Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 8s - loss: 18.9180 - tp: 3.0000 - fp: 1.0000 - tn: 42831.0000 - fn: 1665.0000 - accuracy: 0.9626 - precision: 0.7500 - recall: 0.0018 - auc: 0.7843 - f1: 0.0036 - val_loss: 1.5102 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7972 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 12.5725 - tp: 5.0000 - fp: 3.0000 - tn: 42857.0000 - fn: 1635.0000 - accuracy: 0.9632 - precision: 0.6250 - recall: 0.0030 - auc: 0.7958 - f1: 0.0062 - val_loss: 1.0077 - val_tp: 3.0000 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1625.0000 - val_accuracy: 0.9635 - val_precision: 1.0000 - val_recall: 0.0018 - val_auc: 0.8038 - val_f1: 0.0042Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 8.4162 - tp: 5.0000 - fp: 4.0000 - tn: 42894.0000 - fn: 1597.0000 - accuracy: 0.9640 - precision: 0.5556 - recall: 0.0031 - auc: 0.8037 - f1: 0.0057 - val_loss: 0.6836 - val_tp: 14.0000 - val_fp: 3.0000 - val_tn: 42869.0000 - val_fn: 1614.0000 - val_accuracy: 0.9637 - val_precision: 0.8235 - val_recall: 0.0086 - val_auc: 0.8097 - val_f1: 0.0169Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 5.8357 - tp: 36.0000 - fp: 18.0000 - tn: 42776.0000 - fn: 1670.0000 - accuracy: 0.9621 - precision: 0.6667 - recall: 0.0211 - auc: 0.8172 - f1: 0.0403 - val_loss: 0.4794 - val_tp: 35.0000 - val_fp: 24.0000 - val_tn: 42848.0000 - val_fn: 1593.0000 - val_accuracy: 0.9637 - val_precision: 0.5932 - val_recall: 0.0215 - val_auc: 0.8141 - val_f1: 0.0395Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 4.1488 - tp: 24.0000 - fp: 23.0000 - tn: 42870.0000 - fn: 1583.0000 - accuracy: 0.9639 - precision: 0.5106 - recall: 0.0149 - auc: 0.8140 - f1: 0.0273 - val_loss: 0.3538 - val_tp: 30.0000 - val_fp: 18.0000 - val_tn: 42854.0000 - val_fn: 1598.0000 - val_accuracy: 0.9637 - val_precision: 0.6250 - val_recall: 0.0184 - val_auc: 0.8181 - val_f1: 0.0336Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 3.1503 - tp: 49.0000 - fp: 30.0000 - tn: 42865.0000 - fn: 1556.0000 - accuracy: 0.9644 - precision: 0.6203 - recall: 0.0305 - auc: 0.8147 - f1: 0.0558 - val_loss: 0.2790 - val_tp: 33.0000 - val_fp: 24.0000 - val_tn: 42848.0000 - val_fn: 1595.0000 - val_accuracy: 0.9636 - val_precision: 0.5789 - val_recall: 0.0203 - val_auc: 0.8217 - val_f1: 0.0379Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 2.5695 - tp: 56.0000 - fp: 66.0000 - tn: 42785.0000 - fn: 1593.0000 - accuracy: 0.9627 - precision: 0.4590 - recall: 0.0340 - auc: 0.8317 - f1: 0.0573 - val_loss: 0.2353 - val_tp: 54.0000 - val_fp: 46.0000 - val_tn: 42826.0000 - val_fn: 1574.0000 - val_accuracy: 0.9636 - val_precision: 0.5400 - val_recall: 0.0332 - val_auc: 0.8257 - val_f1: 0.0589Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 8s - loss: 2.2292 - tp: 48.0000 - fp: 70.0000 - tn: 42608.0000 - fn: 1532.0000 - accuracy: 0.9638 - precision: 0.4068 - recall: 0.0304 - auc: 0.8175 - f1: 0.0533 - val_loss: 0.2106 - val_tp: 45.0000 - val_fp: 37.0000 - val_tn: 42835.0000 - val_fn: 1583.0000 - val_accuracy: 0.9636 - val_precision: 0.5488 - val_recall: 0.0276 - val_auc: 0.8290 - val_f1: 0.0493Epoch 12/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 2.0633 - tp: 71.0000 - fp: 96.0000 - tn: 42749.0000 - fn: 1584.0000 - accuracy: 0.9622 - precision: 0.4251 - recall: 0.0429 - auc: 0.8331 - f1: 0.0758 - val_loss: 0.1962 - val_tp: 80.0000 - val_fp: 93.0000 - val_tn: 42779.0000 - val_fn: 1548.0000 - val_accuracy: 0.9631 - val_precision: 0.4624 - val_recall: 0.0491 - val_auc: 0.8320 - val_f1: 0.0840Epoch 13/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.9481 - tp: 113.0000 - fp: 168.0000 - tn: 42675.0000 - fn: 1544.0000 - accuracy: 0.9615 - precision: 0.4021 - recall: 0.0682 - auc: 0.8362 - f1: 0.1140 - val_loss: 0.1891 - val_tp: 61.0000 - val_fp: 56.0000 - val_tn: 42816.0000 - val_fn: 1567.0000 - val_accuracy: 0.9635 - val_precision: 0.5214 - val_recall: 0.0375 - val_auc: 0.8354 - val_f1: 0.0650Epoch 14/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.8804 - tp: 102.0000 - fp: 127.0000 - tn: 42751.0000 - fn: 1520.0000 - accuracy: 0.9630 - precision: 0.4454 - recall: 0.0629 - auc: 0.8298 - f1: 0.1045 - val_loss: 0.1836 - val_tp: 106.0000 - val_fp: 151.0000 - val_tn: 42721.0000 - val_fn: 1522.0000 - val_accuracy: 0.9624 - val_precision: 0.4125 - val_recall: 0.0651 - val_auc: 0.8372 - val_f1: 0.1069Epoch 15/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.8636 - tp: 97.0000 - fp: 167.0000 - tn: 42706.0000 - fn: 1530.0000 - accuracy: 0.9619 - precision: 0.3674 - recall: 0.0596 - auc: 0.8315 - f1: 0.0981 - val_loss: 0.1808 - val_tp: 99.0000 - val_fp: 125.0000 - val_tn: 42747.0000 - val_fn: 1529.0000 - val_accuracy: 0.9628 - val_precision: 0.4420 - val_recall: 0.0608 - val_auc: 0.8382 - val_f1: 0.1012Epoch 16/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.7725 - tp: 125.0000 - fp: 206.0000 - tn: 42701.0000 - fn: 1468.0000 - accuracy: 0.9624 - precision: 0.3776 - recall: 0.0785 - auc: 0.8451 - f1: 0.1241 - val_loss: 0.1792 - val_tp: 88.0000 - val_fp: 104.0000 - val_tn: 42768.0000 - val_fn: 1540.0000 - val_accuracy: 0.9631 - val_precision: 0.4583 - val_recall: 0.0541 - val_auc: 0.8395 - val_f1: 0.0920Epoch 17/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.8243 - tp: 122.0000 - fp: 195.0000 - tn: 42676.0000 - fn: 1507.0000 - accuracy: 0.9618 - precision: 0.3849 - recall: 0.0749 - auc: 0.8293 - f1: 0.1180 - val_loss: 0.1772 - val_tp: 115.0000 - val_fp: 162.0000 - val_tn: 42710.0000 - val_fn: 1513.0000 - val_accuracy: 0.9624 - val_precision: 0.4152 - val_recall: 0.0706 - val_auc: 0.8436 - val_f1: 0.1141Epoch 18/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.7941 - tp: 142.0000 - fp: 231.0000 - tn: 42635.0000 - fn: 1492.0000 - accuracy: 0.9613 - precision: 0.3807 - recall: 0.0869 - auc: 0.8403 - f1: 0.1341 - val_loss: 0.1762 - val_tp: 126.0000 - val_fp: 192.0000 - val_tn: 42680.0000 - val_fn: 1502.0000 - val_accuracy: 0.9619 - val_precision: 0.3962 - val_recall: 0.0774 - val_auc: 0.8436 - val_f1: 0.1228Epoch 19/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.8144 - tp: 192.0000 - fp: 282.0000 - tn: 42536.0000 - fn: 1490.0000 - accuracy: 0.9602 - precision: 0.4051 - recall: 0.1141 - auc: 0.8370 - f1: 0.1722 - val_loss: 0.1751 - val_tp: 132.0000 - val_fp: 199.0000 - val_tn: 42673.0000 - val_fn: 1496.0000 - val_accuracy: 0.9619 - val_precision: 0.3988 - val_recall: 0.0811 - val_auc: 0.8459 - val_f1: 0.1269Epoch 20/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.7464 - tp: 219.0000 - fp: 349.0000 - tn: 42488.0000 - fn: 1444.0000 - accuracy: 0.9597 - precision: 0.3856 - recall: 0.1317 - auc: 0.8572 - f1: 0.1928 - val_loss: 0.1744 - val_tp: 229.0000 - val_fp: 407.0000 - val_tn: 42465.0000 - val_fn: 1399.0000 - val_accuracy: 0.9594 - val_precision: 0.3601 - val_recall: 0.1407 - val_auc: 0.8473 - val_f1: 0.1914\n",
      "Validation fraction incorrect: 0.87\n",
      "\n",
      "learning rate: 4.9e-05\n",
      "hidden layers: 13\n",
      "neurons per layer: 45\n",
      "dropout rate: 0.38\n",
      "l2 lambda: 4.3e-01\n",
      "class weight: 2.2, 13.52\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 45)                1845      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 46        \n",
      "=================================================================\n",
      "Total params: 28,801\n",
      "Trainable params: 28,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 57s - loss: 243.6222 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4903 - f1: 0.0000e+00 - val_loss: 23.5153 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 231.5065 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5032 - f1: 0.0000e+00 - val_loss: 22.3446 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5012 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 219.9760 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5042 - f1: 0.0000e+00 - val_loss: 21.2294 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 24s - loss: 209.0057 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5165 - f1: 0.0000e+00 - val_loss: 20.1661 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 198.5003 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5022 - f1: 0.0000e+00 - val_loss: 19.1521 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5008 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 188.4721 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4911 - f1: 0.0000e+00 - val_loss: 18.1847 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 24s - loss: 179.0020 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5145 - f1: 0.0000e+00 - val_loss: 17.2611 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 169.8150 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4977 - f1: 0.0000e+00 - val_loss: 16.3793 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 24s - loss: 161.1130 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4958 - f1: 0.0000e+00 - val_loss: 15.5373 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 152.8311 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5135 - f1: 0.0000e+00 - val_loss: 14.7331 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 24s - loss: 144.8355 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4987 - f1: 0.0000e+00 - val_loss: 13.9649 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 12/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 137.3193 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42845.0000 - fn: 1655.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5020 - f1: 0.0000e+00 - val_loss: 13.2311 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5254 - val_f1: 0.0000e+00Epoch 13/20\n",
      " 44500/449258 [=>............................] - ETA: 24s - loss: 130.0811 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42843.0000 - fn: 1657.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4911 - f1: 0.0000e+00 - val_loss: 12.5302 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 14/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 123.1365 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42878.0000 - fn: 1622.0000 - accuracy: 0.9636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5013 - f1: 0.0000e+00 - val_loss: 11.8609 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 15/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 116.5333 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42873.0000 - fn: 1627.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4991 - f1: 0.0000e+00 - val_loss: 11.2217 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 16/20\n",
      " 44500/449258 [=>............................] - ETA: 24s - loss: 110.2069 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42907.0000 - fn: 1593.0000 - accuracy: 0.9642 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4885 - f1: 0.0000e+00 - val_loss: 10.6114 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 17/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 104.2069 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42871.0000 - fn: 1629.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4960 - f1: 0.0000e+00 - val_loss: 10.0287 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 23s - loss: 98.4646 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42866.0000 - fn: 1634.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4913 - f1: 0.0000e+00 - val_loss: 9.4726 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 19/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 93.0097 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42818.0000 - fn: 1682.0000 - accuracy: 0.9622 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4954 - f1: 0.0000e+00 - val_loss: 8.9418 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 20/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 87.7604 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42837.0000 - fn: 1663.0000 - accuracy: 0.9626 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4933 - f1: 0.0000e+00 - val_loss: 8.4355 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.3e-05\n",
      "hidden layers: 26\n",
      "neurons per layer: 95\n",
      "dropout rate: 0.2\n",
      "l2 lambda: 9.3e-01\n",
      "class weight: 7.3, 9.9\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 95)                3895      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 95)                9120      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 96        \n",
      "=================================================================\n",
      "Total params: 241,111\n",
      "Trainable params: 241,111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:31 - loss: 2261.6439 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4986 - f1: 0.0000e+00 - val_loss: 221.7548 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 49s - loss: 2216.6642 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5019 - f1: 0.0000e+00 - val_loss: 217.3428 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 49s - loss: 2172.5526 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4989 - f1: 0.0000e+00 - val_loss: 213.0151 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 49s - loss: 2129.2972 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5025 - f1: 0.0000e+00 - val_loss: 208.7691 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 49s - loss: 2086.8058 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5121 - f1: 0.0000e+00 - val_loss: 204.6020 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 48s - loss: 2045.0954 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4979 - f1: 0.0000e+00 - val_loss: 200.5115 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 50s - loss: 2004.2451 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4902 - f1: 0.0000e+00 - val_loss: 196.4954 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 50s - loss: 1963.9924 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5024 - f1: 0.0000e+00 - val_loss: 192.5515 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 47s - loss: 1924.5283 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4981 - f1: 0.0000e+00 - val_loss: 188.6780 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 47s - loss: 1885.7955 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5061 - f1: 0.0000e+00 - val_loss: 184.8730 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 34s - loss: 1847.5920 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4935 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 47s - loss: 1847.5915 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4935 - f1: 0.0000e+00 - val_loss: 181.1347 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.9e-03\n",
      "hidden layers: 17\n",
      "neurons per layer: 193\n",
      "dropout rate: 0.42\n",
      "l2 lambda: 7.5e-01\n",
      "class weight: 5.4, 13.8\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 193)               7913      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 193)               37442     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 193)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 194       \n",
      "=================================================================\n",
      "Total params: 644,621\n",
      "Trainable params: 644,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:32 - loss: 504.9078 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5057 - f1: 0.0000e+00 - val_loss: 0.4121 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 58s - loss: 1.9421 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4976 - f1: 0.0000e+00 - val_loss: 0.1695 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 59s - loss: 1.7171 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5015 - f1: 0.0000e+00 - val_loss: 0.1695 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 59s - loss: 1.7440 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4999 - f1: 0.0000e+00 - val_loss: 0.1695 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 1:03 - loss: 1.7211 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5077 - f1: 0.0000e+00 - val_loss: 0.1695 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 1:09 - loss: 1.6946 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5008 - f1: 0.0000e+00 - val_loss: 0.1695 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 1:44 - loss: 1.7739 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4875 - f1: 0.0000e+00 - val_loss: 0.1696 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 1:42 - loss: 1.6985 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4934 - f1: 0.0000e+00 - val_loss: 0.1695 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 1:12 - loss: 1.6962 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5003 - f1: 0.0000e+00 - val_loss: 0.1695 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 1:18 - loss: 1.7294 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4967 - f1: 0.0000e+00 - val_loss: 0.1695 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 1:01 - loss: 1.6835 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5040 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 1:25 - loss: 1.6835 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5040 - f1: 0.0000e+00 - val_loss: 0.1697 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 7.3e-02\n",
      "hidden layers: 19\n",
      "neurons per layer: 62\n",
      "dropout rate: 0.16\n",
      "l2 lambda: 1.7e-01\n",
      "class weight: 0.3, 11.35\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 62)                2542      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 62)                3906      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 62)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 63        \n",
      "=================================================================\n",
      "Total params: 76,819\n",
      "Trainable params: 76,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:13 - loss: 8.3235 - tp: 1334.0000 - fp: 35175.0000 - tn: 7710.0000 - fn: 281.0000 - accuracy: 0.2032 - precision: 0.0365 - recall: 0.8260 - auc: 0.4997 - f1: 0.0666 - val_loss: 0.0441 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 27s - loss: 0.4567 - tp: 1377.0000 - fp: 36570.0000 - tn: 6307.0000 - fn: 246.0000 - accuracy: 0.1727 - precision: 0.0363 - recall: 0.8484 - auc: 0.5018 - f1: 0.0686 - val_loss: 0.0437 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 29s - loss: 0.4464 - tp: 1485.0000 - fp: 38843.0000 - tn: 4025.0000 - fn: 147.0000 - accuracy: 0.1238 - precision: 0.0368 - recall: 0.9099 - auc: 0.5086 - f1: 0.0708 - val_loss: 0.0437 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 32s - loss: 0.4458 - tp: 1645.0000 - fp: 42249.0000 - tn: 583.0000 - fn: 23.0000 - accuracy: 0.0501 - precision: 0.0375 - recall: 0.9862 - auc: 0.4986 - f1: 0.0721 - val_loss: 0.0432 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 34s - loss: 0.4425 - tp: 1629.0000 - fp: 42610.0000 - tn: 250.0000 - fn: 11.0000 - accuracy: 0.0422 - precision: 0.0368 - recall: 0.9933 - auc: 0.4841 - f1: 0.0710 - val_loss: 0.0433 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 36s - loss: 0.4359 - tp: 1600.0000 - fp: 42817.0000 - tn: 81.0000 - fn: 2.0000 - accuracy: 0.0378 - precision: 0.0360 - recall: 0.9988 - auc: 0.4934 - f1: 0.0694 - val_loss: 0.0433 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 36s - loss: 0.4470 - tp: 1706.0000 - fp: 42793.0000 - tn: 1.0000 - fn: 0.0000e+00 - accuracy: 0.0384 - precision: 0.0383 - recall: 1.0000 - auc: 0.5015 - f1: 0.0737 - val_loss: 0.0432 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 36s - loss: 0.4349 - tp: 1607.0000 - fp: 42893.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0361 - precision: 0.0361 - recall: 1.0000 - auc: 0.4922 - f1: 0.0696 - val_loss: 0.0432 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 34s - loss: 0.4339 - tp: 1605.0000 - fp: 42895.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0361 - precision: 0.0361 - recall: 1.0000 - auc: 0.5009 - f1: 0.0695 - val_loss: 0.0432 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 37s - loss: 0.4390 - tp: 1649.0000 - fp: 42851.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0371 - precision: 0.0371 - recall: 1.0000 - auc: 0.4926 - f1: 0.0713 - val_loss: 0.0432 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 26s - loss: 0.4320 - tp: 1580.0000 - fp: 42678.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0357 - precision: 0.0357 - recall: 1.0000 - auc: 0.4955 - f1: 0.0687Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 34s - loss: 0.4320 - tp: 1580.0000 - fp: 42678.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0357 - precision: 0.0357 - recall: 1.0000 - auc: 0.4955 - f1: 0.0687 - val_loss: 0.0432 - val_tp: 1628.0000 - val_fp: 42872.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.0366 - val_precision: 0.0366 - val_recall: 1.0000 - val_auc: 0.5000 - val_f1: 0.0705Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.0e-05\n",
      "hidden layers: 2\n",
      "neurons per layer: 200\n",
      "dropout rate: 0.5\n",
      "l2 lambda: 1.0e+00\n",
      "class weight: 10.0, 5.0\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               8200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 88,801\n",
      "Trainable params: 88,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 49s - loss: 397.5642 - tp: 0.0000e+00 - fp: 1.0000 - tn: 42884.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5505 - f1: 0.0000e+00 - val_loss: 38.9462 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6058 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 388.9935 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5687 - f1: 0.0000e+00 - val_loss: 38.1065 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6250 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 380.6103 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5588 - f1: 0.0000e+00 - val_loss: 37.2841 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6447 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 372.4018 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5870 - f1: 0.0000e+00 - val_loss: 36.4787 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6599 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 364.3387 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6022 - f1: 0.0000e+00 - val_loss: 35.6895 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6741 - val_f1: 0.0000e+00Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 24s - loss: 356.4392 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5968 - f1: 0.0000e+00 - val_loss: 34.9160 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6848 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 348.7400 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6283 - f1: 0.0000e+00 - val_loss: 34.1578 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6991 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 22s - loss: 341.1256 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6250 - f1: 0.0000e+00 - val_loss: 33.4144 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7087 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 24s - loss: 333.6901 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6346 - f1: 0.0000e+00 - val_loss: 32.6854 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7164 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 326.4148 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6514 - f1: 0.0000e+00 - val_loss: 31.9704 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7254 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 23s - loss: 319.2276 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6525 - f1: 0.0000e+00 - val_loss: 31.2690 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7330 - val_f1: 0.0000e+00Epoch 12/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 312.2504 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42845.0000 - fn: 1655.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6684 - f1: 0.0000e+00 - val_loss: 30.5808 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7377 - val_f1: 0.0000e+00Epoch 13/20\n",
      " 44500/449258 [=>............................] - ETA: 24s - loss: 305.3705 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42843.0000 - fn: 1657.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6718 - f1: 0.0000e+00 - val_loss: 29.9056 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7413 - val_f1: 0.0000e+00Epoch 14/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 298.6034 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42878.0000 - fn: 1622.0000 - accuracy: 0.9636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6703 - f1: 0.0000e+00 - val_loss: 29.2429 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7466 - val_f1: 0.0000e+00Epoch 15/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 291.9781 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42873.0000 - fn: 1627.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6714 - f1: 0.0000e+00 - val_loss: 28.5926 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7490 - val_f1: 0.0000e+00Epoch 16/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 285.4573 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42907.0000 - fn: 1593.0000 - accuracy: 0.9642 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6890 - f1: 0.0000e+00 - val_loss: 27.9543 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7537 - val_f1: 0.0000e+00Epoch 17/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 279.0884 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42871.0000 - fn: 1629.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6838 - f1: 0.0000e+00 - val_loss: 27.3277 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7577 - val_f1: 0.0000e+00Epoch 18/20\n",
      " 44500/449258 [=>............................] - ETA: 23s - loss: 272.8195 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42866.0000 - fn: 1634.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6972 - f1: 0.0000e+00 - val_loss: 26.7126 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7588 - val_f1: 0.0000e+00Epoch 19/20\n",
      " 44500/449258 [=>............................] - ETA: 24s - loss: 266.6846 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42818.0000 - fn: 1682.0000 - accuracy: 0.9622 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6981 - f1: 0.0000e+00 - val_loss: 26.1088 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7624 - val_f1: 0.0000e+00Epoch 20/20\n",
      " 44500/449258 [=>............................] - ETA: 24s - loss: 260.6334 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42837.0000 - fn: 1663.0000 - accuracy: 0.9626 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7181 - f1: 0.0000e+00 - val_loss: 25.5159 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7653 - val_f1: 0.0000e+00\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.4e-04\n",
      "hidden layers: 2\n",
      "neurons per layer: 10\n",
      "dropout rate: 0.5\n",
      "l2 lambda: 1.0e-04\n",
      "class weight: 1.1, 16.57\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                410       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 641\n",
      "Trainable params: 641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 1:02 - loss: 2.3052 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5359 - f1: 0.0000e+00 - val_loss: 0.2256 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5488 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 6s - loss: 2.2240 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5469 - f1: 0.0000e+00 - val_loss: 0.2178 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5650 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 6s - loss: 2.1736 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5593 - f1: 0.0000e+00 - val_loss: 0.2124 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5832 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 6s - loss: 2.1696 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5670 - f1: 0.0000e+00 - val_loss: 0.2085 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5965 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 6s - loss: 2.0986 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5811 - f1: 0.0000e+00 - val_loss: 0.2055 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5938 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 6s - loss: 2.0263 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5772 - f1: 0.0000e+00 - val_loss: 0.2031 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5805 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 6s - loss: 2.1301 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5811 - f1: 0.0000e+00 - val_loss: 0.2009 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5914 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 5s - loss: 1.9924 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5782 - f1: 0.0000e+00 - val_loss: 0.1992 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6325 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 6s - loss: 1.9744 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6049 - f1: 0.0000e+00 - val_loss: 0.1976 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6279 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 5s - loss: 2.0076 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6031 - f1: 0.0000e+00 - val_loss: 0.1958 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6072 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 11s - loss: 1.9167 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5875 - f1: 0.0000e+00 - val_loss: 0.1935 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6015 - val_f1: 0.0000e+00Epoch 12/20\n",
      " 44500/449258 [=>............................] - ETA: 15s - loss: 1.9674 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42845.0000 - fn: 1655.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5880 - f1: 0.0000e+00 - val_loss: 0.1904 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6234 - val_f1: 0.0000e+00Epoch 13/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.9338 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42843.0000 - fn: 1657.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5784 - f1: 0.0000e+00 - val_loss: 0.1865 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6437 - val_f1: 0.0000e+00Epoch 14/20\n",
      " 44500/449258 [=>............................] - ETA: 6s - loss: 1.8587 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42878.0000 - fn: 1622.0000 - accuracy: 0.9636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5566 - f1: 0.0000e+00 - val_loss: 0.1819 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6776 - val_f1: 0.0000e+00Epoch 15/20\n",
      " 44500/449258 [=>............................] - ETA: 13s - loss: 1.8147 - tp: 0.0000e+00 - fp: 1.0000 - tn: 42872.0000 - fn: 1627.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5584 - f1: 0.0000e+00 - val_loss: 0.1760 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7072 - val_f1: 0.0000e+00Epoch 16/20\n",
      " 44500/449258 [=>............................] - ETA: 5s - loss: 1.7090 - tp: 0.0000e+00 - fp: 4.0000 - tn: 42903.0000 - fn: 1593.0000 - accuracy: 0.9641 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5696 - f1: 0.0000e+00 - val_loss: 0.1681 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7292 - val_f1: 0.0000e+00Epoch 17/20\n",
      " 44500/449258 [=>............................] - ETA: 5s - loss: 1.6657 - tp: 0.0000e+00 - fp: 5.0000 - tn: 42866.0000 - fn: 1629.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5706 - f1: 0.0000e+00 - val_loss: 0.1582 - val_tp: 0.0000e+00 - val_fp: 1.0000 - val_tn: 42871.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7460 - val_f1: 0.0000e+00Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.5708 - tp: 1.0000 - fp: 13.0000 - tn: 42853.0000 - fn: 1633.0000 - accuracy: 0.9630 - precision: 0.0714 - recall: 6.1200e-04 - auc: 0.5770 - f1: 0.0012 - val_loss: 0.1460 - val_tp: 0.0000e+00 - val_fp: 1.0000 - val_tn: 42871.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7551 - val_f1: 0.0000e+00Epoch 19/20\n",
      " 44500/449258 [=>............................] - ETA: 7s - loss: 1.5293 - tp: 69.0000 - fp: 188.0000 - tn: 42630.0000 - fn: 1613.0000 - accuracy: 0.9595 - precision: 0.2685 - recall: 0.0410 - auc: 0.5815 - f1: 0.0660 - val_loss: 0.1342 - val_tp: 0.0000e+00 - val_fp: 2.0000 - val_tn: 42870.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7598 - val_f1: 0.0000e+00Epoch 20/20\n",
      " 44500/449258 [=>............................] - ETA: 7s - loss: 1.4531 - tp: 237.0000 - fp: 1233.0000 - tn: 41604.0000 - fn: 1426.0000 - accuracy: 0.9402 - precision: 0.1612 - recall: 0.1425 - auc: 0.5928 - f1: 0.1465 - val_loss: 0.1257 - val_tp: 0.0000e+00 - val_fp: 2.0000 - val_tn: 42870.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7655 - val_f1: 0.0000e+00\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 2.9e-04\n",
      "hidden layers: 2\n",
      "neurons per layer: 28\n",
      "dropout rate: 0.37\n",
      "l2 lambda: 1.1e-01\n",
      "class weight: 0.5, 15.0\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 28)                1148      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 2,801\n",
      "Trainable params: 2,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:37 - loss: 7.3484 - tp: 6.0000 - fp: 52.0000 - tn: 42833.0000 - fn: 1609.0000 - accuracy: 0.9627 - precision: 0.1034 - recall: 0.0037 - auc: 0.5715 - f1: 0.0071 - val_loss: 0.6381 - val_tp: 9.0000 - val_fp: 8.0000 - val_tn: 42864.0000 - val_fn: 1619.0000 - val_accuracy: 0.9634 - val_precision: 0.5294 - val_recall: 0.0055 - val_auc: 0.7190 - val_f1: 0.0099Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 14s - loss: 5.7120 - tp: 323.0000 - fp: 2044.0000 - tn: 40833.0000 - fn: 1300.0000 - accuracy: 0.9249 - precision: 0.1365 - recall: 0.1990 - auc: 0.6847 - f1: 0.1510 - val_loss: 0.4938 - val_tp: 632.0000 - val_fp: 4103.0000 - val_tn: 38769.0000 - val_fn: 996.0000 - val_accuracy: 0.8854 - val_precision: 0.1335 - val_recall: 0.3882 - val_auc: 0.7743 - val_f1: 0.1968Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 13s - loss: 4.5690 - tp: 747.0000 - fp: 7012.0000 - tn: 35856.0000 - fn: 885.0000 - accuracy: 0.8225 - precision: 0.0963 - recall: 0.4577 - auc: 0.7279 - f1: 0.1574 - val_loss: 0.3986 - val_tp: 901.0000 - val_fp: 7052.0000 - val_tn: 35820.0000 - val_fn: 727.0000 - val_accuracy: 0.8252 - val_precision: 0.1133 - val_recall: 0.5534 - val_auc: 0.7928 - val_f1: 0.1872Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 13s - loss: 3.7212 - tp: 908.0000 - fp: 8443.0000 - tn: 34389.0000 - fn: 760.0000 - accuracy: 0.7932 - precision: 0.0971 - recall: 0.5444 - auc: 0.7499 - f1: 0.1637 - val_loss: 0.3243 - val_tp: 1020.0000 - val_fp: 8292.0000 - val_tn: 34580.0000 - val_fn: 608.0000 - val_accuracy: 0.8000 - val_precision: 0.1095 - val_recall: 0.6265 - val_auc: 0.8034 - val_f1: 0.1857Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 12s - loss: 3.0425 - tp: 933.0000 - fp: 8775.0000 - tn: 34085.0000 - fn: 707.0000 - accuracy: 0.7869 - precision: 0.0961 - recall: 0.5689 - auc: 0.7592 - f1: 0.1640 - val_loss: 0.2654 - val_tp: 1064.0000 - val_fp: 8531.0000 - val_tn: 34341.0000 - val_fn: 564.0000 - val_accuracy: 0.7956 - val_precision: 0.1109 - val_recall: 0.6536 - val_auc: 0.8113 - val_f1: 0.1887Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 13s - loss: 2.5050 - tp: 935.0000 - fp: 9071.0000 - tn: 33827.0000 - fn: 667.0000 - accuracy: 0.7812 - precision: 0.0934 - recall: 0.5836 - auc: 0.7583 - f1: 0.1598 - val_loss: 0.2183 - val_tp: 1108.0000 - val_fp: 8898.0000 - val_tn: 33974.0000 - val_fn: 520.0000 - val_accuracy: 0.7884 - val_precision: 0.1107 - val_recall: 0.6806 - val_auc: 0.8171 - val_f1: 0.1897Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 12s - loss: 2.0800 - tp: 1077.0000 - fp: 9524.0000 - tn: 33270.0000 - fn: 629.0000 - accuracy: 0.7718 - precision: 0.1016 - recall: 0.6313 - auc: 0.7798 - f1: 0.1740 - val_loss: 0.1810 - val_tp: 1169.0000 - val_fp: 9901.0000 - val_tn: 32971.0000 - val_fn: 459.0000 - val_accuracy: 0.7672 - val_precision: 0.1056 - val_recall: 0.7181 - val_auc: 0.8209 - val_f1: 0.1831Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 11s - loss: 1.7235 - tp: 1024.0000 - fp: 9727.0000 - tn: 33166.0000 - fn: 583.0000 - accuracy: 0.7683 - precision: 0.0952 - recall: 0.6372 - auc: 0.7816 - f1: 0.1645 - val_loss: 0.1516 - val_tp: 1179.0000 - val_fp: 10019.0000 - val_tn: 32853.0000 - val_fn: 449.0000 - val_accuracy: 0.7648 - val_precision: 0.1053 - val_recall: 0.7242 - val_auc: 0.8235 - val_f1: 0.1828Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 1.4480 - tp: 1077.0000 - fp: 9991.0000 - tn: 32904.0000 - fn: 528.0000 - accuracy: 0.7636 - precision: 0.0973 - recall: 0.6710 - auc: 0.7896 - f1: 0.1689 - val_loss: 0.1283 - val_tp: 1167.0000 - val_fp: 9475.0000 - val_tn: 33397.0000 - val_fn: 461.0000 - val_accuracy: 0.7767 - val_precision: 0.1097 - val_recall: 0.7168 - val_auc: 0.8269 - val_f1: 0.1890Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 9s - loss: 1.2401 - tp: 1135.0000 - fp: 10274.0000 - tn: 32577.0000 - fn: 514.0000 - accuracy: 0.7576 - precision: 0.0995 - recall: 0.6883 - auc: 0.7963 - f1: 0.1730 - val_loss: 0.1101 - val_tp: 1205.0000 - val_fp: 10121.0000 - val_tn: 32751.0000 - val_fn: 423.0000 - val_accuracy: 0.7631 - val_precision: 0.1064 - val_recall: 0.7402 - val_auc: 0.8297 - val_f1: 0.1849Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 8s - loss: 1.0885 - tp: 1043.0000 - fp: 10359.0000 - tn: 32319.0000 - fn: 537.0000 - accuracy: 0.7538 - precision: 0.0915 - recall: 0.6601 - auc: 0.7806 - f1: 0.1586 - val_loss: 0.0963 - val_tp: 1186.0000 - val_fp: 9501.0000 - val_tn: 33371.0000 - val_fn: 442.0000 - val_accuracy: 0.7766 - val_precision: 0.1110 - val_recall: 0.7285 - val_auc: 0.8326 - val_f1: 0.1913Epoch 12/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 0.9651 - tp: 1144.0000 - fp: 10740.0000 - tn: 32105.0000 - fn: 511.0000 - accuracy: 0.7472 - precision: 0.0963 - recall: 0.6912 - auc: 0.7939 - f1: 0.1685 - val_loss: 0.0856 - val_tp: 1256.0000 - val_fp: 11427.0000 - val_tn: 31445.0000 - val_fn: 372.0000 - val_accuracy: 0.7349 - val_precision: 0.0990 - val_recall: 0.7715 - val_auc: 0.8335 - val_f1: 0.1746Epoch 13/20\n",
      " 44500/449258 [=>............................] - ETA: 8s - loss: 0.8712 - tp: 1174.0000 - fp: 10975.0000 - tn: 31868.0000 - fn: 483.0000 - accuracy: 0.7425 - precision: 0.0966 - recall: 0.7085 - auc: 0.7949 - f1: 0.1694 - val_loss: 0.0777 - val_tp: 1220.0000 - val_fp: 10105.0000 - val_tn: 32767.0000 - val_fn: 408.0000 - val_accuracy: 0.7638 - val_precision: 0.1077 - val_recall: 0.7494 - val_auc: 0.8351 - val_f1: 0.1873Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 8s - loss: 0.7955 - tp: 1116.0000 - fp: 10734.0000 - tn: 32144.0000 - fn: 506.0000 - accuracy: 0.7474 - precision: 0.0942 - recall: 0.6880 - auc: 0.7947 - f1: 0.1647 - val_loss: 0.0716 - val_tp: 1255.0000 - val_fp: 10885.0000 - val_tn: 31987.0000 - val_fn: 373.0000 - val_accuracy: 0.7470 - val_precision: 0.1034 - val_recall: 0.7709 - val_auc: 0.8374 - val_f1: 0.1813Epoch 15/20\n",
      " 44500/449258 [=>............................] - ETA: 14s - loss: 0.7486 - tp: 1143.0000 - fp: 11197.0000 - tn: 31676.0000 - fn: 484.0000 - accuracy: 0.7375 - precision: 0.0926 - recall: 0.7025 - auc: 0.7919 - f1: 0.1632 - val_loss: 0.0672 - val_tp: 1246.0000 - val_fp: 10542.0000 - val_tn: 32330.0000 - val_fn: 382.0000 - val_accuracy: 0.7545 - val_precision: 0.1057 - val_recall: 0.7654 - val_auc: 0.8386 - val_f1: 0.1847Epoch 16/20\n",
      " 44500/449258 [=>............................] - ETA: 15s - loss: 0.6807 - tp: 1160.0000 - fp: 10623.0000 - tn: 32284.0000 - fn: 433.0000 - accuracy: 0.7516 - precision: 0.0984 - recall: 0.7282 - auc: 0.8121 - f1: 0.1727 - val_loss: 0.0639 - val_tp: 1221.0000 - val_fp: 9939.0000 - val_tn: 32933.0000 - val_fn: 407.0000 - val_accuracy: 0.7675 - val_precision: 0.1094 - val_recall: 0.7500 - val_auc: 0.8392 - val_f1: 0.1898Epoch 17/20\n",
      " 44500/449258 [=>............................] - ETA: 13s - loss: 0.6795 - tp: 1129.0000 - fp: 10959.0000 - tn: 31912.0000 - fn: 500.0000 - accuracy: 0.7425 - precision: 0.0934 - recall: 0.6931 - auc: 0.7929 - f1: 0.1648 - val_loss: 0.0613 - val_tp: 1260.0000 - val_fp: 10684.0000 - val_tn: 32188.0000 - val_fn: 368.0000 - val_accuracy: 0.7516 - val_precision: 0.1055 - val_recall: 0.7740 - val_auc: 0.8410 - val_f1: 0.1848Epoch 18/20\n",
      " 44500/449258 [=>............................] - ETA: 7s - loss: 0.6563 - tp: 1176.0000 - fp: 11150.0000 - tn: 31716.0000 - fn: 458.0000 - accuracy: 0.7391 - precision: 0.0954 - recall: 0.7197 - auc: 0.7957 - f1: 0.1672 - val_loss: 0.0595 - val_tp: 1263.0000 - val_fp: 10784.0000 - val_tn: 32088.0000 - val_fn: 365.0000 - val_accuracy: 0.7495 - val_precision: 0.1048 - val_recall: 0.7758 - val_auc: 0.8418 - val_f1: 0.1837Epoch 19/20\n",
      " 44500/449258 [=>............................] - ETA: 13s - loss: 0.6468 - tp: 1217.0000 - fp: 11342.0000 - tn: 31476.0000 - fn: 465.0000 - accuracy: 0.7347 - precision: 0.0969 - recall: 0.7235 - auc: 0.7988 - f1: 0.1706 - val_loss: 0.0581 - val_tp: 1270.0000 - val_fp: 10839.0000 - val_tn: 32033.0000 - val_fn: 358.0000 - val_accuracy: 0.7484 - val_precision: 0.1049 - val_recall: 0.7801 - val_auc: 0.8435 - val_f1: 0.1839Epoch 20/20\n",
      " 44500/449258 [=>............................] - ETA: 15s - loss: 0.6066 - tp: 1239.0000 - fp: 10825.0000 - tn: 32012.0000 - fn: 424.0000 - accuracy: 0.7472 - precision: 0.1027 - recall: 0.7450 - auc: 0.8226 - f1: 0.1797 - val_loss: 0.0572 - val_tp: 1284.0000 - val_fp: 11510.0000 - val_tn: 31362.0000 - val_fn: 344.0000 - val_accuracy: 0.7336 - val_precision: 0.1004 - val_recall: 0.7887 - val_auc: 0.8442 - val_f1: 0.1771\n",
      "Validation fraction incorrect: 0.48\n",
      "\n",
      "learning rate: 6.4e-02\n",
      "hidden layers: 16\n",
      "neurons per layer: 67\n",
      "dropout rate: 0.35\n",
      "l2 lambda: 8.0e-01\n",
      "class weight: 2.7, 15.26\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 67)                2747      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 67)                4556      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 67)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 68        \n",
      "=================================================================\n",
      "Total params: 75,711\n",
      "Trainable params: 75,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:33 - loss: 35.6827 - tp: 0.0000e+00 - fp: 1.0000 - tn: 42884.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4966 - f1: 0.0000e+00 - val_loss: 0.1497 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 38s - loss: 1.4980 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4935 - f1: 0.0000e+00 - val_loss: 0.1483 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 35s - loss: 1.4924 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4997 - f1: 0.0000e+00 - val_loss: 0.1472 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 38s - loss: 1.5078 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4930 - f1: 0.0000e+00 - val_loss: 0.1469 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 40s - loss: 1.4886 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4962 - f1: 0.0000e+00 - val_loss: 0.1469 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 41s - loss: 1.4674 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4996 - f1: 0.0000e+00 - val_loss: 0.1468 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 37s - loss: 1.5279 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4881 - f1: 0.0000e+00 - val_loss: 0.1467 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 41s - loss: 1.4700 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4925 - f1: 0.0000e+00 - val_loss: 0.1467 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 40s - loss: 1.4685 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4962 - f1: 0.0000e+00 - val_loss: 0.1467 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 1.4950 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4914 - f1: 0.0000e+00 - val_loss: 0.1468 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 25s - loss: 1.4588 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4961 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 38s - loss: 1.4588 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4961 - f1: 0.0000e+00 - val_loss: 0.1468 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 6.7e-04\n",
      "hidden layers: 30\n",
      "neurons per layer: 163\n",
      "dropout rate: 0.17\n",
      "l2 lambda: 8.9e-02\n",
      "class weight: 9.3, 11.28\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 163)               6683      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 163)               26732     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 163)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 164       \n",
      "=================================================================\n",
      "Total params: 808,807\n",
      "Trainable params: 808,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 3:17 - loss: 236.9852 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4980 - f1: 0.0000e+00 - val_loss: 10.0993 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 1:49 - loss: 51.0753 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5027 - f1: 0.0000e+00 - val_loss: 1.9483 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 2:15 - loss: 9.7102 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4987 - f1: 0.0000e+00 - val_loss: 0.4053 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 2:01 - loss: 2.6886 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5053 - f1: 0.0000e+00 - val_loss: 0.1908 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 1:37 - loss: 1.7884 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4976 - f1: 0.0000e+00 - val_loss: 0.1694 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 1:42 - loss: 1.6806 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4954 - f1: 0.0000e+00 - val_loss: 0.1679 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 1:45 - loss: 1.7561 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4951 - f1: 0.0000e+00 - val_loss: 0.1680 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 1:47 - loss: 1.6798 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4897 - f1: 0.0000e+00 - val_loss: 0.1679 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 1:36 - loss: 1.6773 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4977 - f1: 0.0000e+00 - val_loss: 0.1679 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 1:32 - loss: 1.7120 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4970 - f1: 0.0000e+00 - val_loss: 0.1679 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 1:20 - loss: 1.6635 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5057 - f1: 0.0000e+00Restoring model weights from the end of the best epoch.\n",
      " 44258/449258 [=>............................] - ETA: 1:54 - loss: 1.6635 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5057 - f1: 0.0000e+00 - val_loss: 0.1679 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 00011: early stopping\n",
      "\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 1.1e-05\n",
      "hidden layers: 8\n",
      "neurons per layer: 192\n",
      "dropout rate: 0.21\n",
      "l2 lambda: 7.9e-01\n",
      "class weight: 0.6, 9.18\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 192)               7872      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 193       \n",
      "=================================================================\n",
      "Total params: 304,513\n",
      "Trainable params: 304,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:22 - loss: 1206.4907 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4822 - f1: 0.0000e+00 - val_loss: 118.1124 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4498 - val_f1: 0.0000e+00Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 45s - loss: 1178.8610 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4898 - f1: 0.0000e+00 - val_loss: 115.4061 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5002 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 45s - loss: 1151.8437 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4923 - f1: 0.0000e+00 - val_loss: 112.7593 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4991 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 45s - loss: 1125.4336 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4999 - f1: 0.0000e+00 - val_loss: 110.1698 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4800 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 46s - loss: 1099.5508 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5042 - f1: 0.0000e+00 - val_loss: 107.6357 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5237 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 36s - loss: 1074.2130 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5010 - f1: 0.0000e+00 - val_loss: 105.1552 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5002 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 31s - loss: 1049.4953 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4940 - f1: 0.0000e+00 - val_loss: 102.7264 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4965 - val_f1: 0.0000e+00Epoch 8/20\n",
      " 44500/449258 [=>............................] - ETA: 41s - loss: 1025.1650 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42893.0000 - fn: 1607.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5057 - f1: 0.0000e+00 - val_loss: 100.3479 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5596 - val_f1: 0.0000e+00Epoch 9/20\n",
      " 44500/449258 [=>............................] - ETA: 45s - loss: 1001.3977 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42895.0000 - fn: 1605.0000 - accuracy: 0.9639 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5035 - f1: 0.0000e+00 - val_loss: 98.0183 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5133 - val_f1: 0.0000e+00Epoch 10/20\n",
      " 44500/449258 [=>............................] - ETA: 48s - loss: 978.1417 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42851.0000 - fn: 1649.0000 - accuracy: 0.9629 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5185 - f1: 0.0000e+00 - val_loss: 95.7361 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5009 - val_f1: 0.0000e+00Epoch 11/20\n",
      " 44258/449258 [=>............................] - ETA: 48s - loss: 955.2461 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42678.0000 - fn: 1580.0000 - accuracy: 0.9643 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5172 - f1: 0.0000e+00 - val_loss: 93.5000 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4916 - val_f1: 0.0000e+00Epoch 12/20\n",
      " 44500/449258 [=>............................] - ETA: 47s - loss: 932.9871 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42845.0000 - fn: 1655.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5112 - f1: 0.0000e+00 - val_loss: 91.3088 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5493 - val_f1: 0.0000e+00Epoch 13/20\n",
      " 44500/449258 [=>............................] - ETA: 47s - loss: 911.0854 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42843.0000 - fn: 1657.0000 - accuracy: 0.9628 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5120 - f1: 0.0000e+00 - val_loss: 89.1613 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5048 - val_f1: 0.0000e+00Epoch 14/20\n",
      " 44500/449258 [=>............................] - ETA: 48s - loss: 889.5974 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42878.0000 - fn: 1622.0000 - accuracy: 0.9636 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5115 - f1: 0.0000e+00 - val_loss: 87.0564 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5010 - val_f1: 0.0000e+00Epoch 15/20\n",
      " 44500/449258 [=>............................] - ETA: 47s - loss: 868.5575 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42873.0000 - fn: 1627.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5143 - f1: 0.0000e+00 - val_loss: 84.9929 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6039 - val_f1: 0.0000e+00Epoch 16/20\n",
      " 44500/449258 [=>............................] - ETA: 47s - loss: 847.9095 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42907.0000 - fn: 1593.0000 - accuracy: 0.9642 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5187 - f1: 0.0000e+00 - val_loss: 82.9701 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5084 - val_f1: 0.0000e+00Epoch 17/20\n",
      " 44500/449258 [=>............................] - ETA: 47s - loss: 827.7061 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42871.0000 - fn: 1629.0000 - accuracy: 0.9634 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5168 - f1: 0.0000e+00 - val_loss: 80.9868 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5019 - val_f1: 0.0000e+00Epoch 18/20\n",
      " 44500/449258 [=>............................] - ETA: 39s - loss: 807.8803 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42866.0000 - fn: 1634.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5176 - f1: 0.0000e+00 - val_loss: 79.0421 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5185 - val_f1: 0.0000e+00Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44500/449258 [=>............................] - ETA: 31s - loss: 788.4637 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42818.0000 - fn: 1682.0000 - accuracy: 0.9622 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5150 - f1: 0.0000e+00 - val_loss: 77.1353 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5107 - val_f1: 0.0000e+00Epoch 20/20\n",
      " 44500/449258 [=>............................] - ETA: 55s - loss: 769.3868 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42837.0000 - fn: 1663.0000 - accuracy: 0.9626 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5157 - f1: 0.0000e+00 - val_loss: 75.2654 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5017 - val_f1: 0.0000e+00\n",
      "Validation fraction incorrect: 1.0\n",
      "\n",
      "learning rate: 8.6e-02\n",
      "hidden layers: 18\n",
      "neurons per layer: 31\n",
      "dropout rate: 0.39\n",
      "l2 lambda: 5.7e-01\n",
      "class weight: 6.0, 18.15\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 31)                1271      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 19,159\n",
      "Trainable params: 19,159\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Train on 449258 samples, validate on 449257 samples\n",
      "Epoch 1/20\n",
      " 44500/449258 [=>............................] - ETA: 1:39 - loss: 14.5844 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42885.0000 - fn: 1615.0000 - accuracy: 0.9637 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4944 - f1: 0.0000e+00 - val_loss: 0.2129 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 2/20\n",
      " 44500/449258 [=>............................] - ETA: 45s - loss: 2.1383 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42877.0000 - fn: 1623.0000 - accuracy: 0.9635 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4895 - f1: 0.0000e+00 - val_loss: 0.2120 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 3/20\n",
      " 44500/449258 [=>............................] - ETA: 44s - loss: 2.1438 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42868.0000 - fn: 1632.0000 - accuracy: 0.9633 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4975 - f1: 0.0000e+00 - val_loss: 0.2118 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 4/20\n",
      " 44500/449258 [=>............................] - ETA: 45s - loss: 2.1782 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42832.0000 - fn: 1668.0000 - accuracy: 0.9625 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4960 - f1: 0.0000e+00 - val_loss: 0.2122 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 5/20\n",
      " 44500/449258 [=>............................] - ETA: 45s - loss: 2.1507 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42860.0000 - fn: 1640.0000 - accuracy: 0.9631 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4970 - f1: 0.0000e+00 - val_loss: 0.2120 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 6/20\n",
      " 44500/449258 [=>............................] - ETA: 44s - loss: 2.1164 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42898.0000 - fn: 1602.0000 - accuracy: 0.9640 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4990 - f1: 0.0000e+00 - val_loss: 0.2119 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 42872.0000 - val_fn: 1628.0000 - val_accuracy: 0.9634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000 - val_f1: 0.0000e+00Epoch 7/20\n",
      " 44500/449258 [=>............................] - ETA: 29s - loss: 2.2123 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 42794.0000 - fn: 1706.0000 - accuracy: 0.9617 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4869 - f1: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(\n",
    "    func=fitness,\n",
    "    dimensions=dimensions,\n",
    "    acq_func='EI', # Expected Improvement.\n",
    "    n_calls=20,\n",
    "    x0=default_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_names = [\n",
    "    'learning_rate',\n",
    "    'hidden_layers',\n",
    "    'neurons_per_layer',\n",
    "    'dropout_rate',\n",
    "    'l2_lambda',\n",
    "    'class_0_weight',\n",
    "    'class_1_weight'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_objective(result=search_result, dimension_names=dim_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = search_result.space\n",
    "winning_hyperparams = space.point_to_dict(search_result.x)\n",
    "winning_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.001, 2, 30, 0.5, 0.1, 0.5, 15 defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = winning_hyperparams['learning_rate'] / 10\n",
    "#learning_rate = 0.0001\n",
    "hidden_layers = winning_hyperparams['hidden_layers']\n",
    "units_per_layer = winning_hyperparams['neurons_per_layer']\n",
    "dropout_rate = winning_hyperparams['dropout_rate']\n",
    "l2_lambda = winning_hyperparams['l2_lambda']\n",
    "#l2_lambda = 0.005\n",
    "class_0_weight = winning_hyperparams['class_0_weight']\n",
    "#class_0_weight = 0.6\n",
    "class_1_weight = winning_hyperparams['class_1_weight']\n",
    "\n",
    "initial_bias = np.log([ignition_count/no_ignition_count])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "    \n",
    "class_weight = {0: class_0_weight, 1: class_1_weight}\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 100000\n",
    "STEPS_PER_EPOCH = (len(training_data) * 1) // BATCH_SIZE\n",
    "VALIDATION_STEPS = (len(validation_data) * 1) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keras model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.layers.Dense(\n",
    "    units_per_layer, \n",
    "    activation = 'relu', \n",
    "    input_dim = train_features.shape[-1],\n",
    "))\n",
    "\n",
    "# Add fully connected hidden layers\n",
    "for i in range(hidden_layers):\n",
    "    model.add(keras.layers.Dense(\n",
    "        units_per_layer,\n",
    "        bias_initializer=keras.initializers.VarianceScaling(\n",
    "            scale=1.0,\n",
    "            mode='fan_in', \n",
    "            distribution='normal', \n",
    "            seed=None\n",
    "        ),\n",
    "        kernel_regularizer=keras.regularizers.l2(l2_lambda),\n",
    "        activation = 'relu')\n",
    "    )\n",
    "\n",
    "# Add dropout layer\n",
    "model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Add output layer\n",
    "model.add(keras.layers.Dense(\n",
    "    1, \n",
    "    activation = 'sigmoid', \n",
    "    bias_initializer = output_bias\n",
    "))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    class_weight=class_weight,\n",
    "    workers=8\n",
    ")\n",
    "\n",
    "model.save('../trained_models/best_MLP.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_features, batch_size=len(train_features))\n",
    "test_predictions = model.predict(test_features, batch_size=len(test_features))\n",
    "\n",
    "results = model.evaluate(\n",
    "    test_features,\n",
    "    test_labels,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(name, ': ', value)\n",
    "\n",
    "plot_cm(test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\"Train\", train_labels, train_predictions, color=colors[0])\n",
    "plot_roc(\"Test\", test_labels, test_predictions, color=colors[0], linestyle='--')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
